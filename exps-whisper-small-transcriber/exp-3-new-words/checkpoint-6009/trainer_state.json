{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 6009,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0004992511233150275,
      "grad_norm": 6.120997428894043,
      "learning_rate": 0.0004999950074887668,
      "loss": 2.6211,
      "step": 1
    },
    {
      "epoch": 0.000998502246630055,
      "grad_norm": 2.630180835723877,
      "learning_rate": 0.0004999900149775337,
      "loss": 2.0141,
      "step": 2
    },
    {
      "epoch": 0.0014977533699450823,
      "grad_norm": 2.5270438194274902,
      "learning_rate": 0.0004999850224663005,
      "loss": 1.6169,
      "step": 3
    },
    {
      "epoch": 0.00199700449326011,
      "grad_norm": 1.5933995246887207,
      "learning_rate": 0.0004999800299550674,
      "loss": 1.4171,
      "step": 4
    },
    {
      "epoch": 0.0024962556165751375,
      "grad_norm": 2.3549134731292725,
      "learning_rate": 0.0004999750374438342,
      "loss": 1.2698,
      "step": 5
    },
    {
      "epoch": 0.0029955067398901645,
      "grad_norm": 35.72513961791992,
      "learning_rate": 0.0004999700449326011,
      "loss": 1.0777,
      "step": 6
    },
    {
      "epoch": 0.003494757863205192,
      "grad_norm": 1.6968032121658325,
      "learning_rate": 0.0004999650524213679,
      "loss": 0.9514,
      "step": 7
    },
    {
      "epoch": 0.00399400898652022,
      "grad_norm": 2.2044880390167236,
      "learning_rate": 0.0004999600599101348,
      "loss": 1.0022,
      "step": 8
    },
    {
      "epoch": 0.0044932601098352475,
      "grad_norm": 1.6846178770065308,
      "learning_rate": 0.0004999550673989016,
      "loss": 0.9283,
      "step": 9
    },
    {
      "epoch": 0.004992511233150275,
      "grad_norm": 1.5415821075439453,
      "learning_rate": 0.0004999500748876685,
      "loss": 0.8281,
      "step": 10
    },
    {
      "epoch": 0.0054917623564653024,
      "grad_norm": 1.4449172019958496,
      "learning_rate": 0.0004999450823764353,
      "loss": 0.8858,
      "step": 11
    },
    {
      "epoch": 0.005991013479780329,
      "grad_norm": 1.0873680114746094,
      "learning_rate": 0.0004999400898652022,
      "loss": 0.7825,
      "step": 12
    },
    {
      "epoch": 0.006490264603095357,
      "grad_norm": 0.9987495541572571,
      "learning_rate": 0.000499935097353969,
      "loss": 0.7659,
      "step": 13
    },
    {
      "epoch": 0.006989515726410384,
      "grad_norm": 1.2618849277496338,
      "learning_rate": 0.0004999301048427359,
      "loss": 0.8149,
      "step": 14
    },
    {
      "epoch": 0.0074887668497254116,
      "grad_norm": 1.6624404191970825,
      "learning_rate": 0.0004999251123315027,
      "loss": 0.6954,
      "step": 15
    },
    {
      "epoch": 0.00798801797304044,
      "grad_norm": 1.2449898719787598,
      "learning_rate": 0.0004999201198202695,
      "loss": 0.786,
      "step": 16
    },
    {
      "epoch": 0.008487269096355467,
      "grad_norm": 1.1384518146514893,
      "learning_rate": 0.0004999151273090364,
      "loss": 0.7129,
      "step": 17
    },
    {
      "epoch": 0.008986520219670495,
      "grad_norm": 0.7827017903327942,
      "learning_rate": 0.0004999101347978032,
      "loss": 0.6101,
      "step": 18
    },
    {
      "epoch": 0.009485771342985522,
      "grad_norm": 1.3521363735198975,
      "learning_rate": 0.0004999051422865701,
      "loss": 0.6477,
      "step": 19
    },
    {
      "epoch": 0.00998502246630055,
      "grad_norm": 0.8460620641708374,
      "learning_rate": 0.0004999001497753369,
      "loss": 0.5661,
      "step": 20
    },
    {
      "epoch": 0.010484273589615577,
      "grad_norm": 1.1970947980880737,
      "learning_rate": 0.0004998951572641038,
      "loss": 0.661,
      "step": 21
    },
    {
      "epoch": 0.010983524712930605,
      "grad_norm": 0.8060115575790405,
      "learning_rate": 0.0004998901647528706,
      "loss": 0.5244,
      "step": 22
    },
    {
      "epoch": 0.011482775836245632,
      "grad_norm": 0.8036531805992126,
      "learning_rate": 0.0004998851722416376,
      "loss": 0.6145,
      "step": 23
    },
    {
      "epoch": 0.011982026959560658,
      "grad_norm": 1.0255440473556519,
      "learning_rate": 0.0004998801797304044,
      "loss": 0.6094,
      "step": 24
    },
    {
      "epoch": 0.012481278082875686,
      "grad_norm": 0.8487549424171448,
      "learning_rate": 0.0004998751872191713,
      "loss": 0.5798,
      "step": 25
    },
    {
      "epoch": 0.012980529206190713,
      "grad_norm": 0.7948604822158813,
      "learning_rate": 0.0004998701947079381,
      "loss": 0.5006,
      "step": 26
    },
    {
      "epoch": 0.01347978032950574,
      "grad_norm": 0.7997277975082397,
      "learning_rate": 0.000499865202196705,
      "loss": 0.6143,
      "step": 27
    },
    {
      "epoch": 0.013979031452820768,
      "grad_norm": 0.9981871843338013,
      "learning_rate": 0.0004998602096854718,
      "loss": 0.5208,
      "step": 28
    },
    {
      "epoch": 0.014478282576135796,
      "grad_norm": 0.7989357709884644,
      "learning_rate": 0.0004998552171742387,
      "loss": 0.5331,
      "step": 29
    },
    {
      "epoch": 0.014977533699450823,
      "grad_norm": 0.8680317401885986,
      "learning_rate": 0.0004998502246630055,
      "loss": 0.534,
      "step": 30
    },
    {
      "epoch": 0.01547678482276585,
      "grad_norm": 1.013829231262207,
      "learning_rate": 0.0004998452321517724,
      "loss": 0.5583,
      "step": 31
    },
    {
      "epoch": 0.01597603594608088,
      "grad_norm": 0.766355574131012,
      "learning_rate": 0.0004998402396405392,
      "loss": 0.4436,
      "step": 32
    },
    {
      "epoch": 0.016475287069395907,
      "grad_norm": 1.294235348701477,
      "learning_rate": 0.0004998352471293061,
      "loss": 0.5473,
      "step": 33
    },
    {
      "epoch": 0.016974538192710935,
      "grad_norm": 0.8136950731277466,
      "learning_rate": 0.0004998302546180729,
      "loss": 0.4622,
      "step": 34
    },
    {
      "epoch": 0.017473789316025962,
      "grad_norm": 0.9064289331436157,
      "learning_rate": 0.0004998252621068398,
      "loss": 0.5541,
      "step": 35
    },
    {
      "epoch": 0.01797304043934099,
      "grad_norm": 0.8769462704658508,
      "learning_rate": 0.0004998202695956066,
      "loss": 0.5015,
      "step": 36
    },
    {
      "epoch": 0.018472291562656017,
      "grad_norm": 0.9368168711662292,
      "learning_rate": 0.0004998152770843735,
      "loss": 0.5271,
      "step": 37
    },
    {
      "epoch": 0.018971542685971045,
      "grad_norm": 0.8138905167579651,
      "learning_rate": 0.0004998102845731403,
      "loss": 0.4881,
      "step": 38
    },
    {
      "epoch": 0.019470793809286072,
      "grad_norm": 0.8409281969070435,
      "learning_rate": 0.0004998052920619072,
      "loss": 0.4618,
      "step": 39
    },
    {
      "epoch": 0.0199700449326011,
      "grad_norm": 0.7671636343002319,
      "learning_rate": 0.000499800299550674,
      "loss": 0.4704,
      "step": 40
    },
    {
      "epoch": 0.020469296055916127,
      "grad_norm": 0.8780698776245117,
      "learning_rate": 0.0004997953070394409,
      "loss": 0.3762,
      "step": 41
    },
    {
      "epoch": 0.020968547179231155,
      "grad_norm": 0.7687978744506836,
      "learning_rate": 0.0004997903145282077,
      "loss": 0.4217,
      "step": 42
    },
    {
      "epoch": 0.021467798302546182,
      "grad_norm": 0.7107002139091492,
      "learning_rate": 0.0004997853220169746,
      "loss": 0.4673,
      "step": 43
    },
    {
      "epoch": 0.02196704942586121,
      "grad_norm": 0.7417888045310974,
      "learning_rate": 0.0004997803295057414,
      "loss": 0.5055,
      "step": 44
    },
    {
      "epoch": 0.022466300549176237,
      "grad_norm": 0.6941593885421753,
      "learning_rate": 0.0004997753369945082,
      "loss": 0.4131,
      "step": 45
    },
    {
      "epoch": 0.022965551672491265,
      "grad_norm": 0.7113185524940491,
      "learning_rate": 0.0004997703444832751,
      "loss": 0.3817,
      "step": 46
    },
    {
      "epoch": 0.023464802795806292,
      "grad_norm": 0.6967732310295105,
      "learning_rate": 0.0004997653519720419,
      "loss": 0.3828,
      "step": 47
    },
    {
      "epoch": 0.023964053919121316,
      "grad_norm": 0.6536065936088562,
      "learning_rate": 0.0004997603594608088,
      "loss": 0.4104,
      "step": 48
    },
    {
      "epoch": 0.024463305042436344,
      "grad_norm": 0.6956614255905151,
      "learning_rate": 0.0004997553669495756,
      "loss": 0.4172,
      "step": 49
    },
    {
      "epoch": 0.02496255616575137,
      "grad_norm": 0.8547549843788147,
      "learning_rate": 0.0004997503744383425,
      "loss": 0.3971,
      "step": 50
    },
    {
      "epoch": 0.0254618072890664,
      "grad_norm": 0.6046010255813599,
      "learning_rate": 0.0004997453819271093,
      "loss": 0.3713,
      "step": 51
    },
    {
      "epoch": 0.025961058412381426,
      "grad_norm": 0.8792543411254883,
      "learning_rate": 0.0004997403894158762,
      "loss": 0.361,
      "step": 52
    },
    {
      "epoch": 0.026460309535696454,
      "grad_norm": 0.8429045677185059,
      "learning_rate": 0.000499735396904643,
      "loss": 0.3681,
      "step": 53
    },
    {
      "epoch": 0.02695956065901148,
      "grad_norm": 0.7167637348175049,
      "learning_rate": 0.0004997304043934099,
      "loss": 0.3298,
      "step": 54
    },
    {
      "epoch": 0.02745881178232651,
      "grad_norm": 0.6307560801506042,
      "learning_rate": 0.0004997254118821767,
      "loss": 0.3104,
      "step": 55
    },
    {
      "epoch": 0.027958062905641536,
      "grad_norm": 1.1028634309768677,
      "learning_rate": 0.0004997204193709436,
      "loss": 0.3565,
      "step": 56
    },
    {
      "epoch": 0.028457314028956564,
      "grad_norm": 0.5154388546943665,
      "learning_rate": 0.0004997154268597104,
      "loss": 0.2591,
      "step": 57
    },
    {
      "epoch": 0.02895656515227159,
      "grad_norm": 0.8021361827850342,
      "learning_rate": 0.0004997104343484773,
      "loss": 0.2855,
      "step": 58
    },
    {
      "epoch": 0.02945581627558662,
      "grad_norm": 3.079922914505005,
      "learning_rate": 0.0004997054418372441,
      "loss": 0.3142,
      "step": 59
    },
    {
      "epoch": 0.029955067398901646,
      "grad_norm": 1.0959041118621826,
      "learning_rate": 0.000499700449326011,
      "loss": 0.2332,
      "step": 60
    },
    {
      "epoch": 0.030454318522216674,
      "grad_norm": 0.7110378742218018,
      "learning_rate": 0.0004996954568147778,
      "loss": 0.2715,
      "step": 61
    },
    {
      "epoch": 0.0309535696455317,
      "grad_norm": 0.6792394518852234,
      "learning_rate": 0.0004996904643035447,
      "loss": 0.1975,
      "step": 62
    },
    {
      "epoch": 0.03145282076884673,
      "grad_norm": 0.7084535956382751,
      "learning_rate": 0.0004996854717923115,
      "loss": 0.1536,
      "step": 63
    },
    {
      "epoch": 0.03195207189216176,
      "grad_norm": 0.7073140740394592,
      "learning_rate": 0.0004996804792810784,
      "loss": 0.1724,
      "step": 64
    },
    {
      "epoch": 0.032451323015476784,
      "grad_norm": 0.9047018885612488,
      "learning_rate": 0.0004996754867698452,
      "loss": 0.1746,
      "step": 65
    },
    {
      "epoch": 0.032950574138791815,
      "grad_norm": 0.9069072604179382,
      "learning_rate": 0.0004996704942586121,
      "loss": 0.1824,
      "step": 66
    },
    {
      "epoch": 0.03344982526210684,
      "grad_norm": 0.4651952385902405,
      "learning_rate": 0.0004996655017473789,
      "loss": 0.116,
      "step": 67
    },
    {
      "epoch": 0.03394907638542187,
      "grad_norm": 0.839250385761261,
      "learning_rate": 0.0004996605092361458,
      "loss": 0.1445,
      "step": 68
    },
    {
      "epoch": 0.034448327508736894,
      "grad_norm": 1.0006686449050903,
      "learning_rate": 0.0004996555167249126,
      "loss": 0.2468,
      "step": 69
    },
    {
      "epoch": 0.034947578632051925,
      "grad_norm": 0.6691070795059204,
      "learning_rate": 0.0004996505242136795,
      "loss": 0.1423,
      "step": 70
    },
    {
      "epoch": 0.03544682975536695,
      "grad_norm": 0.6922507882118225,
      "learning_rate": 0.0004996455317024463,
      "loss": 0.1072,
      "step": 71
    },
    {
      "epoch": 0.03594608087868198,
      "grad_norm": 0.609680712223053,
      "learning_rate": 0.0004996405391912133,
      "loss": 0.1177,
      "step": 72
    },
    {
      "epoch": 0.036445332001997004,
      "grad_norm": 0.5559775829315186,
      "learning_rate": 0.00049963554667998,
      "loss": 0.1204,
      "step": 73
    },
    {
      "epoch": 0.036944583125312035,
      "grad_norm": 0.6379209756851196,
      "learning_rate": 0.0004996305541687468,
      "loss": 0.0992,
      "step": 74
    },
    {
      "epoch": 0.03744383424862706,
      "grad_norm": 0.6482429504394531,
      "learning_rate": 0.0004996255616575138,
      "loss": 0.1079,
      "step": 75
    },
    {
      "epoch": 0.03794308537194209,
      "grad_norm": 0.5704926252365112,
      "learning_rate": 0.0004996205691462806,
      "loss": 0.1084,
      "step": 76
    },
    {
      "epoch": 0.038442336495257114,
      "grad_norm": 0.6105742454528809,
      "learning_rate": 0.0004996155766350475,
      "loss": 0.0686,
      "step": 77
    },
    {
      "epoch": 0.038941587618572145,
      "grad_norm": 0.4196445941925049,
      "learning_rate": 0.0004996105841238143,
      "loss": 0.0966,
      "step": 78
    },
    {
      "epoch": 0.03944083874188717,
      "grad_norm": 0.6628545522689819,
      "learning_rate": 0.0004996055916125812,
      "loss": 0.1271,
      "step": 79
    },
    {
      "epoch": 0.0399400898652022,
      "grad_norm": 0.9187802672386169,
      "learning_rate": 0.000499600599101348,
      "loss": 0.0937,
      "step": 80
    },
    {
      "epoch": 0.040439340988517224,
      "grad_norm": 0.6537569165229797,
      "learning_rate": 0.0004995956065901149,
      "loss": 0.115,
      "step": 81
    },
    {
      "epoch": 0.040938592111832255,
      "grad_norm": 0.7351019382476807,
      "learning_rate": 0.0004995906140788817,
      "loss": 0.1166,
      "step": 82
    },
    {
      "epoch": 0.04143784323514728,
      "grad_norm": 0.8125127553939819,
      "learning_rate": 0.0004995856215676486,
      "loss": 0.1151,
      "step": 83
    },
    {
      "epoch": 0.04193709435846231,
      "grad_norm": 0.5291076898574829,
      "learning_rate": 0.0004995806290564154,
      "loss": 0.0746,
      "step": 84
    },
    {
      "epoch": 0.042436345481777334,
      "grad_norm": 0.5329294204711914,
      "learning_rate": 0.0004995756365451823,
      "loss": 0.0785,
      "step": 85
    },
    {
      "epoch": 0.042935596605092365,
      "grad_norm": 0.596165120601654,
      "learning_rate": 0.0004995706440339491,
      "loss": 0.1187,
      "step": 86
    },
    {
      "epoch": 0.04343484772840739,
      "grad_norm": 0.8381733894348145,
      "learning_rate": 0.000499565651522716,
      "loss": 0.1174,
      "step": 87
    },
    {
      "epoch": 0.04393409885172242,
      "grad_norm": 0.5694604516029358,
      "learning_rate": 0.0004995606590114828,
      "loss": 0.0702,
      "step": 88
    },
    {
      "epoch": 0.044433349975037444,
      "grad_norm": 0.6831681728363037,
      "learning_rate": 0.0004995556665002497,
      "loss": 0.0888,
      "step": 89
    },
    {
      "epoch": 0.044932601098352475,
      "grad_norm": 0.658631443977356,
      "learning_rate": 0.0004995506739890165,
      "loss": 0.1783,
      "step": 90
    },
    {
      "epoch": 0.0454318522216675,
      "grad_norm": 0.5111440420150757,
      "learning_rate": 0.0004995456814777834,
      "loss": 0.1116,
      "step": 91
    },
    {
      "epoch": 0.04593110334498253,
      "grad_norm": 0.464771032333374,
      "learning_rate": 0.0004995406889665502,
      "loss": 0.092,
      "step": 92
    },
    {
      "epoch": 0.046430354468297554,
      "grad_norm": 0.5935910940170288,
      "learning_rate": 0.0004995356964553171,
      "loss": 0.128,
      "step": 93
    },
    {
      "epoch": 0.046929605591612585,
      "grad_norm": 0.3598115146160126,
      "learning_rate": 0.0004995307039440839,
      "loss": 0.0787,
      "step": 94
    },
    {
      "epoch": 0.04742885671492761,
      "grad_norm": 0.47406744956970215,
      "learning_rate": 0.0004995257114328508,
      "loss": 0.0933,
      "step": 95
    },
    {
      "epoch": 0.04792810783824263,
      "grad_norm": 0.45718419551849365,
      "learning_rate": 0.0004995207189216176,
      "loss": 0.0681,
      "step": 96
    },
    {
      "epoch": 0.048427358961557664,
      "grad_norm": 0.7016404271125793,
      "learning_rate": 0.0004995157264103845,
      "loss": 0.1121,
      "step": 97
    },
    {
      "epoch": 0.04892661008487269,
      "grad_norm": 0.49441760778427124,
      "learning_rate": 0.0004995107338991513,
      "loss": 0.1008,
      "step": 98
    },
    {
      "epoch": 0.04942586120818772,
      "grad_norm": 0.6295199394226074,
      "learning_rate": 0.0004995057413879182,
      "loss": 0.0699,
      "step": 99
    },
    {
      "epoch": 0.04992511233150274,
      "grad_norm": 0.561440646648407,
      "learning_rate": 0.000499500748876685,
      "loss": 0.0887,
      "step": 100
    },
    {
      "epoch": 0.050424363454817774,
      "grad_norm": 0.43650612235069275,
      "learning_rate": 0.0004994957563654519,
      "loss": 0.0804,
      "step": 101
    },
    {
      "epoch": 0.0509236145781328,
      "grad_norm": 0.5211852788925171,
      "learning_rate": 0.0004994907638542187,
      "loss": 0.0771,
      "step": 102
    },
    {
      "epoch": 0.05142286570144783,
      "grad_norm": 0.4409448206424713,
      "learning_rate": 0.0004994857713429855,
      "loss": 0.0638,
      "step": 103
    },
    {
      "epoch": 0.05192211682476285,
      "grad_norm": 0.45993682742118835,
      "learning_rate": 0.0004994807788317524,
      "loss": 0.073,
      "step": 104
    },
    {
      "epoch": 0.052421367948077884,
      "grad_norm": 0.4390294849872589,
      "learning_rate": 0.0004994757863205192,
      "loss": 0.0778,
      "step": 105
    },
    {
      "epoch": 0.05292061907139291,
      "grad_norm": 0.4068378508090973,
      "learning_rate": 0.0004994707938092861,
      "loss": 0.0651,
      "step": 106
    },
    {
      "epoch": 0.05341987019470794,
      "grad_norm": 0.41393619775772095,
      "learning_rate": 0.0004994658012980529,
      "loss": 0.0498,
      "step": 107
    },
    {
      "epoch": 0.05391912131802296,
      "grad_norm": 0.6075630187988281,
      "learning_rate": 0.0004994608087868198,
      "loss": 0.1034,
      "step": 108
    },
    {
      "epoch": 0.054418372441337994,
      "grad_norm": 0.6843851208686829,
      "learning_rate": 0.0004994558162755866,
      "loss": 0.1201,
      "step": 109
    },
    {
      "epoch": 0.05491762356465302,
      "grad_norm": 0.49053776264190674,
      "learning_rate": 0.0004994508237643535,
      "loss": 0.0848,
      "step": 110
    },
    {
      "epoch": 0.05541687468796805,
      "grad_norm": 0.4114893674850464,
      "learning_rate": 0.0004994458312531203,
      "loss": 0.084,
      "step": 111
    },
    {
      "epoch": 0.05591612581128307,
      "grad_norm": 0.5255985260009766,
      "learning_rate": 0.0004994408387418872,
      "loss": 0.0723,
      "step": 112
    },
    {
      "epoch": 0.056415376934598103,
      "grad_norm": 0.5525289177894592,
      "learning_rate": 0.000499435846230654,
      "loss": 0.0853,
      "step": 113
    },
    {
      "epoch": 0.05691462805791313,
      "grad_norm": 0.400870680809021,
      "learning_rate": 0.0004994308537194208,
      "loss": 0.0827,
      "step": 114
    },
    {
      "epoch": 0.05741387918122816,
      "grad_norm": 0.6181483268737793,
      "learning_rate": 0.0004994258612081877,
      "loss": 0.1156,
      "step": 115
    },
    {
      "epoch": 0.05791313030454318,
      "grad_norm": 0.3882978856563568,
      "learning_rate": 0.0004994208686969545,
      "loss": 0.0688,
      "step": 116
    },
    {
      "epoch": 0.05841238142785821,
      "grad_norm": 0.8127439022064209,
      "learning_rate": 0.0004994158761857214,
      "loss": 0.0837,
      "step": 117
    },
    {
      "epoch": 0.05891163255117324,
      "grad_norm": 0.5814099907875061,
      "learning_rate": 0.0004994108836744882,
      "loss": 0.0777,
      "step": 118
    },
    {
      "epoch": 0.05941088367448827,
      "grad_norm": 0.4323510229587555,
      "learning_rate": 0.0004994058911632551,
      "loss": 0.0561,
      "step": 119
    },
    {
      "epoch": 0.05991013479780329,
      "grad_norm": 0.6518946886062622,
      "learning_rate": 0.0004994008986520219,
      "loss": 0.0977,
      "step": 120
    },
    {
      "epoch": 0.06040938592111832,
      "grad_norm": 0.4889104664325714,
      "learning_rate": 0.0004993959061407888,
      "loss": 0.0646,
      "step": 121
    },
    {
      "epoch": 0.06090863704443335,
      "grad_norm": 0.30036044120788574,
      "learning_rate": 0.0004993909136295556,
      "loss": 0.041,
      "step": 122
    },
    {
      "epoch": 0.06140788816774838,
      "grad_norm": 0.8271141648292542,
      "learning_rate": 0.0004993859211183225,
      "loss": 0.0806,
      "step": 123
    },
    {
      "epoch": 0.0619071392910634,
      "grad_norm": 0.4115693271160126,
      "learning_rate": 0.0004993809286070893,
      "loss": 0.0725,
      "step": 124
    },
    {
      "epoch": 0.06240639041437843,
      "grad_norm": 0.4068520665168762,
      "learning_rate": 0.0004993759360958562,
      "loss": 0.0643,
      "step": 125
    },
    {
      "epoch": 0.06290564153769346,
      "grad_norm": 0.34701067209243774,
      "learning_rate": 0.000499370943584623,
      "loss": 0.0488,
      "step": 126
    },
    {
      "epoch": 0.06340489266100849,
      "grad_norm": 0.5528767704963684,
      "learning_rate": 0.00049936595107339,
      "loss": 0.057,
      "step": 127
    },
    {
      "epoch": 0.06390414378432352,
      "grad_norm": 0.4356134533882141,
      "learning_rate": 0.0004993609585621567,
      "loss": 0.061,
      "step": 128
    },
    {
      "epoch": 0.06440339490763854,
      "grad_norm": 0.7329128384590149,
      "learning_rate": 0.0004993559660509237,
      "loss": 0.0644,
      "step": 129
    },
    {
      "epoch": 0.06490264603095357,
      "grad_norm": 0.42670685052871704,
      "learning_rate": 0.0004993509735396905,
      "loss": 0.0701,
      "step": 130
    },
    {
      "epoch": 0.0654018971542686,
      "grad_norm": 0.4719182252883911,
      "learning_rate": 0.0004993459810284573,
      "loss": 0.0889,
      "step": 131
    },
    {
      "epoch": 0.06590114827758363,
      "grad_norm": 0.4741695523262024,
      "learning_rate": 0.0004993409885172242,
      "loss": 0.0782,
      "step": 132
    },
    {
      "epoch": 0.06640039940089865,
      "grad_norm": 0.4992305338382721,
      "learning_rate": 0.000499335996005991,
      "loss": 0.0625,
      "step": 133
    },
    {
      "epoch": 0.06689965052421368,
      "grad_norm": 0.40540388226509094,
      "learning_rate": 0.0004993310034947579,
      "loss": 0.0673,
      "step": 134
    },
    {
      "epoch": 0.06739890164752871,
      "grad_norm": 0.32872122526168823,
      "learning_rate": 0.0004993260109835247,
      "loss": 0.0485,
      "step": 135
    },
    {
      "epoch": 0.06789815277084374,
      "grad_norm": 0.4448872208595276,
      "learning_rate": 0.0004993210184722916,
      "loss": 0.0552,
      "step": 136
    },
    {
      "epoch": 0.06839740389415876,
      "grad_norm": 0.5688396096229553,
      "learning_rate": 0.0004993160259610584,
      "loss": 0.0808,
      "step": 137
    },
    {
      "epoch": 0.06889665501747379,
      "grad_norm": 0.4370996057987213,
      "learning_rate": 0.0004993110334498253,
      "loss": 0.0535,
      "step": 138
    },
    {
      "epoch": 0.06939590614078882,
      "grad_norm": 0.47869744896888733,
      "learning_rate": 0.0004993060409385921,
      "loss": 0.0769,
      "step": 139
    },
    {
      "epoch": 0.06989515726410385,
      "grad_norm": 0.5602365732192993,
      "learning_rate": 0.000499301048427359,
      "loss": 0.0817,
      "step": 140
    },
    {
      "epoch": 0.07039440838741887,
      "grad_norm": 0.3635822534561157,
      "learning_rate": 0.0004992960559161258,
      "loss": 0.0491,
      "step": 141
    },
    {
      "epoch": 0.0708936595107339,
      "grad_norm": 0.5172790884971619,
      "learning_rate": 0.0004992910634048927,
      "loss": 0.0905,
      "step": 142
    },
    {
      "epoch": 0.07139291063404893,
      "grad_norm": 0.31211963295936584,
      "learning_rate": 0.0004992860708936595,
      "loss": 0.0428,
      "step": 143
    },
    {
      "epoch": 0.07189216175736396,
      "grad_norm": 0.5288603901863098,
      "learning_rate": 0.0004992810783824264,
      "loss": 0.0725,
      "step": 144
    },
    {
      "epoch": 0.07239141288067898,
      "grad_norm": 0.6762768626213074,
      "learning_rate": 0.0004992760858711932,
      "loss": 0.0801,
      "step": 145
    },
    {
      "epoch": 0.07289066400399401,
      "grad_norm": 0.5485990047454834,
      "learning_rate": 0.0004992710933599601,
      "loss": 0.0577,
      "step": 146
    },
    {
      "epoch": 0.07338991512730904,
      "grad_norm": 0.8061070442199707,
      "learning_rate": 0.0004992661008487269,
      "loss": 0.0697,
      "step": 147
    },
    {
      "epoch": 0.07388916625062407,
      "grad_norm": 0.6838955283164978,
      "learning_rate": 0.0004992611083374938,
      "loss": 0.0711,
      "step": 148
    },
    {
      "epoch": 0.07438841737393909,
      "grad_norm": 0.4998021423816681,
      "learning_rate": 0.0004992561158262606,
      "loss": 0.057,
      "step": 149
    },
    {
      "epoch": 0.07488766849725412,
      "grad_norm": 0.5430911779403687,
      "learning_rate": 0.0004992511233150275,
      "loss": 0.0805,
      "step": 150
    },
    {
      "epoch": 0.07538691962056915,
      "grad_norm": 0.5087220668792725,
      "learning_rate": 0.0004992461308037943,
      "loss": 0.0607,
      "step": 151
    },
    {
      "epoch": 0.07588617074388418,
      "grad_norm": 0.627007007598877,
      "learning_rate": 0.0004992411382925612,
      "loss": 0.0794,
      "step": 152
    },
    {
      "epoch": 0.0763854218671992,
      "grad_norm": 0.39466843008995056,
      "learning_rate": 0.000499236145781328,
      "loss": 0.0595,
      "step": 153
    },
    {
      "epoch": 0.07688467299051423,
      "grad_norm": 0.678068995475769,
      "learning_rate": 0.0004992311532700949,
      "loss": 0.061,
      "step": 154
    },
    {
      "epoch": 0.07738392411382926,
      "grad_norm": 0.6309887766838074,
      "learning_rate": 0.0004992261607588617,
      "loss": 0.0926,
      "step": 155
    },
    {
      "epoch": 0.07788317523714429,
      "grad_norm": 0.8153858184814453,
      "learning_rate": 0.0004992211682476286,
      "loss": 0.0784,
      "step": 156
    },
    {
      "epoch": 0.0783824263604593,
      "grad_norm": 0.44099679589271545,
      "learning_rate": 0.0004992161757363954,
      "loss": 0.0602,
      "step": 157
    },
    {
      "epoch": 0.07888167748377434,
      "grad_norm": 0.4738486409187317,
      "learning_rate": 0.0004992111832251623,
      "loss": 0.0642,
      "step": 158
    },
    {
      "epoch": 0.07938092860708937,
      "grad_norm": 0.4119270145893097,
      "learning_rate": 0.0004992061907139291,
      "loss": 0.034,
      "step": 159
    },
    {
      "epoch": 0.0798801797304044,
      "grad_norm": 0.9652016162872314,
      "learning_rate": 0.000499201198202696,
      "loss": 0.0862,
      "step": 160
    },
    {
      "epoch": 0.08037943085371942,
      "grad_norm": 0.34416669607162476,
      "learning_rate": 0.0004991962056914628,
      "loss": 0.061,
      "step": 161
    },
    {
      "epoch": 0.08087868197703445,
      "grad_norm": 0.5231978893280029,
      "learning_rate": 0.0004991912131802296,
      "loss": 0.0447,
      "step": 162
    },
    {
      "epoch": 0.08137793310034948,
      "grad_norm": 0.6497066020965576,
      "learning_rate": 0.0004991862206689965,
      "loss": 0.0508,
      "step": 163
    },
    {
      "epoch": 0.08187718422366451,
      "grad_norm": 0.5477699637413025,
      "learning_rate": 0.0004991812281577633,
      "loss": 0.0515,
      "step": 164
    },
    {
      "epoch": 0.08237643534697953,
      "grad_norm": 0.5381770133972168,
      "learning_rate": 0.0004991762356465302,
      "loss": 0.0725,
      "step": 165
    },
    {
      "epoch": 0.08287568647029456,
      "grad_norm": 0.34602561593055725,
      "learning_rate": 0.000499171243135297,
      "loss": 0.0423,
      "step": 166
    },
    {
      "epoch": 0.08337493759360959,
      "grad_norm": 0.31395819783210754,
      "learning_rate": 0.0004991662506240639,
      "loss": 0.0345,
      "step": 167
    },
    {
      "epoch": 0.08387418871692462,
      "grad_norm": 0.6040289998054504,
      "learning_rate": 0.0004991612581128307,
      "loss": 0.0442,
      "step": 168
    },
    {
      "epoch": 0.08437343984023964,
      "grad_norm": 0.5824993252754211,
      "learning_rate": 0.0004991562656015976,
      "loss": 0.0756,
      "step": 169
    },
    {
      "epoch": 0.08487269096355467,
      "grad_norm": 0.5916781425476074,
      "learning_rate": 0.0004991512730903644,
      "loss": 0.0518,
      "step": 170
    },
    {
      "epoch": 0.0853719420868697,
      "grad_norm": 0.588089644908905,
      "learning_rate": 0.0004991462805791313,
      "loss": 0.0686,
      "step": 171
    },
    {
      "epoch": 0.08587119321018473,
      "grad_norm": 0.3051721453666687,
      "learning_rate": 0.0004991412880678981,
      "loss": 0.0339,
      "step": 172
    },
    {
      "epoch": 0.08637044433349975,
      "grad_norm": 0.5076770782470703,
      "learning_rate": 0.000499136295556665,
      "loss": 0.0695,
      "step": 173
    },
    {
      "epoch": 0.08686969545681478,
      "grad_norm": 0.5903242230415344,
      "learning_rate": 0.0004991313030454318,
      "loss": 0.061,
      "step": 174
    },
    {
      "epoch": 0.08736894658012981,
      "grad_norm": 0.9310416579246521,
      "learning_rate": 0.0004991263105341987,
      "loss": 0.0447,
      "step": 175
    },
    {
      "epoch": 0.08786819770344484,
      "grad_norm": 0.4704227149486542,
      "learning_rate": 0.0004991213180229655,
      "loss": 0.0381,
      "step": 176
    },
    {
      "epoch": 0.08836744882675986,
      "grad_norm": 0.42777127027511597,
      "learning_rate": 0.0004991163255117324,
      "loss": 0.0425,
      "step": 177
    },
    {
      "epoch": 0.08886669995007489,
      "grad_norm": 0.5216723084449768,
      "learning_rate": 0.0004991113330004992,
      "loss": 0.0441,
      "step": 178
    },
    {
      "epoch": 0.08936595107338992,
      "grad_norm": 0.6728938221931458,
      "learning_rate": 0.0004991063404892662,
      "loss": 0.0829,
      "step": 179
    },
    {
      "epoch": 0.08986520219670495,
      "grad_norm": 0.9190219640731812,
      "learning_rate": 0.000499101347978033,
      "loss": 0.1008,
      "step": 180
    },
    {
      "epoch": 0.09036445332001997,
      "grad_norm": 0.28325724601745605,
      "learning_rate": 0.0004990963554667999,
      "loss": 0.0322,
      "step": 181
    },
    {
      "epoch": 0.090863704443335,
      "grad_norm": 0.3120146691799164,
      "learning_rate": 0.0004990913629555667,
      "loss": 0.0409,
      "step": 182
    },
    {
      "epoch": 0.09136295556665003,
      "grad_norm": 0.7516278028488159,
      "learning_rate": 0.0004990863704443336,
      "loss": 0.0601,
      "step": 183
    },
    {
      "epoch": 0.09186220668996506,
      "grad_norm": 0.366938978433609,
      "learning_rate": 0.0004990813779331004,
      "loss": 0.0414,
      "step": 184
    },
    {
      "epoch": 0.09236145781328008,
      "grad_norm": 0.4744446277618408,
      "learning_rate": 0.0004990763854218673,
      "loss": 0.0458,
      "step": 185
    },
    {
      "epoch": 0.09286070893659511,
      "grad_norm": 0.36783793568611145,
      "learning_rate": 0.0004990713929106341,
      "loss": 0.0439,
      "step": 186
    },
    {
      "epoch": 0.09335996005991014,
      "grad_norm": 0.2575206756591797,
      "learning_rate": 0.000499066400399401,
      "loss": 0.0258,
      "step": 187
    },
    {
      "epoch": 0.09385921118322517,
      "grad_norm": 0.3123023509979248,
      "learning_rate": 0.0004990614078881678,
      "loss": 0.0234,
      "step": 188
    },
    {
      "epoch": 0.09435846230654019,
      "grad_norm": 1.3767681121826172,
      "learning_rate": 0.0004990564153769347,
      "loss": 0.1069,
      "step": 189
    },
    {
      "epoch": 0.09485771342985522,
      "grad_norm": 0.44338470697402954,
      "learning_rate": 0.0004990514228657015,
      "loss": 0.0296,
      "step": 190
    },
    {
      "epoch": 0.09535696455317025,
      "grad_norm": 0.6581529378890991,
      "learning_rate": 0.0004990464303544683,
      "loss": 0.0749,
      "step": 191
    },
    {
      "epoch": 0.09585621567648527,
      "grad_norm": 0.3075734078884125,
      "learning_rate": 0.0004990414378432352,
      "loss": 0.0461,
      "step": 192
    },
    {
      "epoch": 0.0963554667998003,
      "grad_norm": 0.4488879144191742,
      "learning_rate": 0.000499036445332002,
      "loss": 0.0353,
      "step": 193
    },
    {
      "epoch": 0.09685471792311533,
      "grad_norm": 0.29134172201156616,
      "learning_rate": 0.0004990314528207689,
      "loss": 0.0273,
      "step": 194
    },
    {
      "epoch": 0.09735396904643036,
      "grad_norm": 0.6712952852249146,
      "learning_rate": 0.0004990264603095357,
      "loss": 0.04,
      "step": 195
    },
    {
      "epoch": 0.09785322016974538,
      "grad_norm": 0.40146198868751526,
      "learning_rate": 0.0004990214677983026,
      "loss": 0.0396,
      "step": 196
    },
    {
      "epoch": 0.0983524712930604,
      "grad_norm": 0.4488627016544342,
      "learning_rate": 0.0004990164752870694,
      "loss": 0.0653,
      "step": 197
    },
    {
      "epoch": 0.09885172241637544,
      "grad_norm": 0.3536049723625183,
      "learning_rate": 0.0004990114827758363,
      "loss": 0.0617,
      "step": 198
    },
    {
      "epoch": 0.09935097353969047,
      "grad_norm": 0.41112220287323,
      "learning_rate": 0.0004990064902646031,
      "loss": 0.0445,
      "step": 199
    },
    {
      "epoch": 0.09985022466300549,
      "grad_norm": 0.522229790687561,
      "learning_rate": 0.00049900149775337,
      "loss": 0.0578,
      "step": 200
    },
    {
      "epoch": 0.10034947578632052,
      "grad_norm": 1.2831556797027588,
      "learning_rate": 0.0004989965052421368,
      "loss": 0.125,
      "step": 201
    },
    {
      "epoch": 0.10084872690963555,
      "grad_norm": 0.4847091734409332,
      "learning_rate": 0.0004989915127309037,
      "loss": 0.0441,
      "step": 202
    },
    {
      "epoch": 0.10134797803295058,
      "grad_norm": 0.5305832624435425,
      "learning_rate": 0.0004989865202196705,
      "loss": 0.0782,
      "step": 203
    },
    {
      "epoch": 0.1018472291562656,
      "grad_norm": 0.3707484304904938,
      "learning_rate": 0.0004989815277084374,
      "loss": 0.0292,
      "step": 204
    },
    {
      "epoch": 0.10234648027958063,
      "grad_norm": 0.50972580909729,
      "learning_rate": 0.0004989765351972042,
      "loss": 0.0456,
      "step": 205
    },
    {
      "epoch": 0.10284573140289566,
      "grad_norm": 0.6489745378494263,
      "learning_rate": 0.0004989715426859711,
      "loss": 0.0693,
      "step": 206
    },
    {
      "epoch": 0.10334498252621069,
      "grad_norm": 0.6855041980743408,
      "learning_rate": 0.0004989665501747379,
      "loss": 0.0669,
      "step": 207
    },
    {
      "epoch": 0.1038442336495257,
      "grad_norm": 0.39287516474723816,
      "learning_rate": 0.0004989615576635048,
      "loss": 0.0438,
      "step": 208
    },
    {
      "epoch": 0.10434348477284074,
      "grad_norm": 0.9805415868759155,
      "learning_rate": 0.0004989565651522716,
      "loss": 0.0643,
      "step": 209
    },
    {
      "epoch": 0.10484273589615577,
      "grad_norm": 0.4949904680252075,
      "learning_rate": 0.0004989515726410385,
      "loss": 0.0527,
      "step": 210
    },
    {
      "epoch": 0.1053419870194708,
      "grad_norm": 0.6515039205551147,
      "learning_rate": 0.0004989465801298053,
      "loss": 0.1025,
      "step": 211
    },
    {
      "epoch": 0.10584123814278582,
      "grad_norm": 0.42340487241744995,
      "learning_rate": 0.0004989415876185722,
      "loss": 0.0373,
      "step": 212
    },
    {
      "epoch": 0.10634048926610085,
      "grad_norm": 0.373735249042511,
      "learning_rate": 0.000498936595107339,
      "loss": 0.0519,
      "step": 213
    },
    {
      "epoch": 0.10683974038941588,
      "grad_norm": 0.4218568503856659,
      "learning_rate": 0.0004989316025961059,
      "loss": 0.0412,
      "step": 214
    },
    {
      "epoch": 0.10733899151273091,
      "grad_norm": 0.31363698840141296,
      "learning_rate": 0.0004989266100848727,
      "loss": 0.0228,
      "step": 215
    },
    {
      "epoch": 0.10783824263604593,
      "grad_norm": 0.48808351159095764,
      "learning_rate": 0.0004989216175736396,
      "loss": 0.0614,
      "step": 216
    },
    {
      "epoch": 0.10833749375936096,
      "grad_norm": 0.36343246698379517,
      "learning_rate": 0.0004989166250624064,
      "loss": 0.049,
      "step": 217
    },
    {
      "epoch": 0.10883674488267599,
      "grad_norm": 0.33909380435943604,
      "learning_rate": 0.0004989116325511733,
      "loss": 0.0358,
      "step": 218
    },
    {
      "epoch": 0.10933599600599102,
      "grad_norm": 0.5053661465644836,
      "learning_rate": 0.0004989066400399401,
      "loss": 0.0415,
      "step": 219
    },
    {
      "epoch": 0.10983524712930604,
      "grad_norm": 0.3160415589809418,
      "learning_rate": 0.0004989016475287069,
      "loss": 0.0392,
      "step": 220
    },
    {
      "epoch": 0.11033449825262107,
      "grad_norm": 0.46065253019332886,
      "learning_rate": 0.0004988966550174738,
      "loss": 0.0463,
      "step": 221
    },
    {
      "epoch": 0.1108337493759361,
      "grad_norm": 0.4846228063106537,
      "learning_rate": 0.0004988916625062406,
      "loss": 0.0573,
      "step": 222
    },
    {
      "epoch": 0.11133300049925113,
      "grad_norm": 0.47671830654144287,
      "learning_rate": 0.0004988866699950075,
      "loss": 0.042,
      "step": 223
    },
    {
      "epoch": 0.11183225162256615,
      "grad_norm": 0.4746737480163574,
      "learning_rate": 0.0004988816774837743,
      "loss": 0.0499,
      "step": 224
    },
    {
      "epoch": 0.11233150274588118,
      "grad_norm": 0.41873642802238464,
      "learning_rate": 0.0004988766849725412,
      "loss": 0.057,
      "step": 225
    },
    {
      "epoch": 0.11283075386919621,
      "grad_norm": 0.2916344702243805,
      "learning_rate": 0.000498871692461308,
      "loss": 0.0308,
      "step": 226
    },
    {
      "epoch": 0.11333000499251124,
      "grad_norm": 0.319148451089859,
      "learning_rate": 0.0004988666999500748,
      "loss": 0.0309,
      "step": 227
    },
    {
      "epoch": 0.11382925611582626,
      "grad_norm": 0.3844365179538727,
      "learning_rate": 0.0004988617074388417,
      "loss": 0.0427,
      "step": 228
    },
    {
      "epoch": 0.11432850723914129,
      "grad_norm": 0.47918564081192017,
      "learning_rate": 0.0004988567149276085,
      "loss": 0.0277,
      "step": 229
    },
    {
      "epoch": 0.11482775836245632,
      "grad_norm": 0.7554214000701904,
      "learning_rate": 0.0004988517224163754,
      "loss": 0.074,
      "step": 230
    },
    {
      "epoch": 0.11532700948577135,
      "grad_norm": 0.6750468015670776,
      "learning_rate": 0.0004988467299051422,
      "loss": 0.1005,
      "step": 231
    },
    {
      "epoch": 0.11582626060908637,
      "grad_norm": 0.20112168788909912,
      "learning_rate": 0.0004988417373939091,
      "loss": 0.0213,
      "step": 232
    },
    {
      "epoch": 0.1163255117324014,
      "grad_norm": 0.37146392464637756,
      "learning_rate": 0.0004988367448826759,
      "loss": 0.0454,
      "step": 233
    },
    {
      "epoch": 0.11682476285571643,
      "grad_norm": 0.4073951542377472,
      "learning_rate": 0.0004988317523714429,
      "loss": 0.0258,
      "step": 234
    },
    {
      "epoch": 0.11732401397903146,
      "grad_norm": 0.861153781414032,
      "learning_rate": 0.0004988267598602096,
      "loss": 0.0859,
      "step": 235
    },
    {
      "epoch": 0.11782326510234647,
      "grad_norm": 1.0701745748519897,
      "learning_rate": 0.0004988217673489766,
      "loss": 0.0647,
      "step": 236
    },
    {
      "epoch": 0.1183225162256615,
      "grad_norm": 0.7375985383987427,
      "learning_rate": 0.0004988167748377434,
      "loss": 0.0752,
      "step": 237
    },
    {
      "epoch": 0.11882176734897654,
      "grad_norm": 0.32063233852386475,
      "learning_rate": 0.0004988117823265103,
      "loss": 0.0272,
      "step": 238
    },
    {
      "epoch": 0.11932101847229157,
      "grad_norm": 0.37593525648117065,
      "learning_rate": 0.0004988067898152771,
      "loss": 0.0336,
      "step": 239
    },
    {
      "epoch": 0.11982026959560658,
      "grad_norm": 0.39979735016822815,
      "learning_rate": 0.000498801797304044,
      "loss": 0.0383,
      "step": 240
    },
    {
      "epoch": 0.12031952071892162,
      "grad_norm": 0.36698681116104126,
      "learning_rate": 0.0004987968047928108,
      "loss": 0.0432,
      "step": 241
    },
    {
      "epoch": 0.12081877184223665,
      "grad_norm": 0.5590863227844238,
      "learning_rate": 0.0004987918122815777,
      "loss": 0.043,
      "step": 242
    },
    {
      "epoch": 0.12131802296555168,
      "grad_norm": 0.402714341878891,
      "learning_rate": 0.0004987868197703445,
      "loss": 0.0486,
      "step": 243
    },
    {
      "epoch": 0.1218172740888667,
      "grad_norm": 0.45480459928512573,
      "learning_rate": 0.0004987818272591114,
      "loss": 0.0558,
      "step": 244
    },
    {
      "epoch": 0.12231652521218173,
      "grad_norm": 0.3243517279624939,
      "learning_rate": 0.0004987768347478782,
      "loss": 0.0348,
      "step": 245
    },
    {
      "epoch": 0.12281577633549676,
      "grad_norm": 0.5499271154403687,
      "learning_rate": 0.0004987718422366451,
      "loss": 0.0301,
      "step": 246
    },
    {
      "epoch": 0.12331502745881179,
      "grad_norm": 0.8209841251373291,
      "learning_rate": 0.0004987668497254119,
      "loss": 0.0313,
      "step": 247
    },
    {
      "epoch": 0.1238142785821268,
      "grad_norm": 0.5699890851974487,
      "learning_rate": 0.0004987618572141787,
      "loss": 0.0534,
      "step": 248
    },
    {
      "epoch": 0.12431352970544184,
      "grad_norm": 0.3753235936164856,
      "learning_rate": 0.0004987568647029456,
      "loss": 0.0433,
      "step": 249
    },
    {
      "epoch": 0.12481278082875687,
      "grad_norm": 0.4087829887866974,
      "learning_rate": 0.0004987518721917124,
      "loss": 0.0497,
      "step": 250
    },
    {
      "epoch": 0.12531203195207188,
      "grad_norm": 0.3652738928794861,
      "learning_rate": 0.0004987468796804793,
      "loss": 0.0359,
      "step": 251
    },
    {
      "epoch": 0.12581128307538691,
      "grad_norm": 0.1655699461698532,
      "learning_rate": 0.0004987418871692461,
      "loss": 0.0159,
      "step": 252
    },
    {
      "epoch": 0.12631053419870195,
      "grad_norm": 0.4442887306213379,
      "learning_rate": 0.000498736894658013,
      "loss": 0.0313,
      "step": 253
    },
    {
      "epoch": 0.12680978532201698,
      "grad_norm": 0.5559415221214294,
      "learning_rate": 0.0004987319021467798,
      "loss": 0.0471,
      "step": 254
    },
    {
      "epoch": 0.127309036445332,
      "grad_norm": 0.5709772109985352,
      "learning_rate": 0.0004987269096355467,
      "loss": 0.0803,
      "step": 255
    },
    {
      "epoch": 0.12780828756864704,
      "grad_norm": 0.3861192762851715,
      "learning_rate": 0.0004987219171243135,
      "loss": 0.0356,
      "step": 256
    },
    {
      "epoch": 0.12830753869196207,
      "grad_norm": 0.185808464884758,
      "learning_rate": 0.0004987169246130804,
      "loss": 0.0207,
      "step": 257
    },
    {
      "epoch": 0.12880678981527707,
      "grad_norm": 0.5240080952644348,
      "learning_rate": 0.0004987119321018472,
      "loss": 0.0388,
      "step": 258
    },
    {
      "epoch": 0.1293060409385921,
      "grad_norm": 0.7408198118209839,
      "learning_rate": 0.0004987069395906141,
      "loss": 0.0643,
      "step": 259
    },
    {
      "epoch": 0.12980529206190713,
      "grad_norm": 0.3359752893447876,
      "learning_rate": 0.0004987019470793809,
      "loss": 0.0269,
      "step": 260
    },
    {
      "epoch": 0.13030454318522217,
      "grad_norm": 0.3178306519985199,
      "learning_rate": 0.0004986969545681478,
      "loss": 0.0379,
      "step": 261
    },
    {
      "epoch": 0.1308037943085372,
      "grad_norm": 0.45020902156829834,
      "learning_rate": 0.0004986919620569146,
      "loss": 0.0383,
      "step": 262
    },
    {
      "epoch": 0.13130304543185223,
      "grad_norm": 0.7180114984512329,
      "learning_rate": 0.0004986869695456815,
      "loss": 0.0183,
      "step": 263
    },
    {
      "epoch": 0.13180229655516726,
      "grad_norm": 0.35159242153167725,
      "learning_rate": 0.0004986819770344483,
      "loss": 0.0258,
      "step": 264
    },
    {
      "epoch": 0.1323015476784823,
      "grad_norm": 0.21686071157455444,
      "learning_rate": 0.0004986769845232152,
      "loss": 0.0177,
      "step": 265
    },
    {
      "epoch": 0.1328007988017973,
      "grad_norm": 0.5597072839736938,
      "learning_rate": 0.000498671992011982,
      "loss": 0.0391,
      "step": 266
    },
    {
      "epoch": 0.13330004992511232,
      "grad_norm": 0.4299474358558655,
      "learning_rate": 0.0004986669995007489,
      "loss": 0.0235,
      "step": 267
    },
    {
      "epoch": 0.13379930104842735,
      "grad_norm": 1.4108444452285767,
      "learning_rate": 0.0004986620069895157,
      "loss": 0.0717,
      "step": 268
    },
    {
      "epoch": 0.13429855217174239,
      "grad_norm": 0.34522315859794617,
      "learning_rate": 0.0004986570144782826,
      "loss": 0.0251,
      "step": 269
    },
    {
      "epoch": 0.13479780329505742,
      "grad_norm": 0.23870478570461273,
      "learning_rate": 0.0004986520219670494,
      "loss": 0.0195,
      "step": 270
    },
    {
      "epoch": 0.13529705441837245,
      "grad_norm": 0.27193084359169006,
      "learning_rate": 0.0004986470294558163,
      "loss": 0.0239,
      "step": 271
    },
    {
      "epoch": 0.13579630554168748,
      "grad_norm": 0.4924977719783783,
      "learning_rate": 0.0004986420369445831,
      "loss": 0.034,
      "step": 272
    },
    {
      "epoch": 0.1362955566650025,
      "grad_norm": 0.2416546642780304,
      "learning_rate": 0.00049863704443335,
      "loss": 0.0212,
      "step": 273
    },
    {
      "epoch": 0.1367948077883175,
      "grad_norm": 0.9173612594604492,
      "learning_rate": 0.0004986320519221168,
      "loss": 0.0375,
      "step": 274
    },
    {
      "epoch": 0.13729405891163254,
      "grad_norm": 0.46124374866485596,
      "learning_rate": 0.0004986270594108837,
      "loss": 0.0515,
      "step": 275
    },
    {
      "epoch": 0.13779331003494757,
      "grad_norm": 0.4448823928833008,
      "learning_rate": 0.0004986220668996505,
      "loss": 0.0477,
      "step": 276
    },
    {
      "epoch": 0.1382925611582626,
      "grad_norm": 0.6188457608222961,
      "learning_rate": 0.0004986170743884173,
      "loss": 0.0338,
      "step": 277
    },
    {
      "epoch": 0.13879181228157764,
      "grad_norm": 0.33191201090812683,
      "learning_rate": 0.0004986120818771842,
      "loss": 0.0225,
      "step": 278
    },
    {
      "epoch": 0.13929106340489267,
      "grad_norm": 0.376253217458725,
      "learning_rate": 0.000498607089365951,
      "loss": 0.0251,
      "step": 279
    },
    {
      "epoch": 0.1397903145282077,
      "grad_norm": 0.4169706404209137,
      "learning_rate": 0.0004986020968547179,
      "loss": 0.0243,
      "step": 280
    },
    {
      "epoch": 0.14028956565152273,
      "grad_norm": 0.44929370284080505,
      "learning_rate": 0.0004985971043434847,
      "loss": 0.0409,
      "step": 281
    },
    {
      "epoch": 0.14078881677483773,
      "grad_norm": 0.4689862132072449,
      "learning_rate": 0.0004985921118322516,
      "loss": 0.0257,
      "step": 282
    },
    {
      "epoch": 0.14128806789815276,
      "grad_norm": 0.4170236885547638,
      "learning_rate": 0.0004985871193210184,
      "loss": 0.0214,
      "step": 283
    },
    {
      "epoch": 0.1417873190214678,
      "grad_norm": 0.4183659553527832,
      "learning_rate": 0.0004985821268097853,
      "loss": 0.0204,
      "step": 284
    },
    {
      "epoch": 0.14228657014478283,
      "grad_norm": 0.32576289772987366,
      "learning_rate": 0.0004985771342985521,
      "loss": 0.0265,
      "step": 285
    },
    {
      "epoch": 0.14278582126809786,
      "grad_norm": 0.37601980566978455,
      "learning_rate": 0.000498572141787319,
      "loss": 0.0305,
      "step": 286
    },
    {
      "epoch": 0.1432850723914129,
      "grad_norm": 0.35216057300567627,
      "learning_rate": 0.0004985671492760858,
      "loss": 0.0312,
      "step": 287
    },
    {
      "epoch": 0.14378432351472792,
      "grad_norm": 0.4912785589694977,
      "learning_rate": 0.0004985621567648528,
      "loss": 0.0963,
      "step": 288
    },
    {
      "epoch": 0.14428357463804292,
      "grad_norm": 0.17178508639335632,
      "learning_rate": 0.0004985571642536196,
      "loss": 0.0144,
      "step": 289
    },
    {
      "epoch": 0.14478282576135795,
      "grad_norm": 0.37604671716690063,
      "learning_rate": 0.0004985521717423865,
      "loss": 0.0242,
      "step": 290
    },
    {
      "epoch": 0.14528207688467298,
      "grad_norm": 0.6709932088851929,
      "learning_rate": 0.0004985471792311533,
      "loss": 0.0301,
      "step": 291
    },
    {
      "epoch": 0.14578132800798801,
      "grad_norm": 0.27363646030426025,
      "learning_rate": 0.0004985421867199202,
      "loss": 0.0156,
      "step": 292
    },
    {
      "epoch": 0.14628057913130305,
      "grad_norm": 0.48186641931533813,
      "learning_rate": 0.000498537194208687,
      "loss": 0.0284,
      "step": 293
    },
    {
      "epoch": 0.14677983025461808,
      "grad_norm": 0.41893094778060913,
      "learning_rate": 0.0004985322016974539,
      "loss": 0.0304,
      "step": 294
    },
    {
      "epoch": 0.1472790813779331,
      "grad_norm": 0.37836819887161255,
      "learning_rate": 0.0004985272091862207,
      "loss": 0.0274,
      "step": 295
    },
    {
      "epoch": 0.14777833250124814,
      "grad_norm": 0.8989354372024536,
      "learning_rate": 0.0004985222166749876,
      "loss": 0.1237,
      "step": 296
    },
    {
      "epoch": 0.14827758362456314,
      "grad_norm": 0.2789827585220337,
      "learning_rate": 0.0004985172241637544,
      "loss": 0.0219,
      "step": 297
    },
    {
      "epoch": 0.14877683474787817,
      "grad_norm": 0.30818894505500793,
      "learning_rate": 0.0004985122316525213,
      "loss": 0.0283,
      "step": 298
    },
    {
      "epoch": 0.1492760858711932,
      "grad_norm": 0.3221130967140198,
      "learning_rate": 0.0004985072391412881,
      "loss": 0.0134,
      "step": 299
    },
    {
      "epoch": 0.14977533699450823,
      "grad_norm": 0.39362409710884094,
      "learning_rate": 0.000498502246630055,
      "loss": 0.0158,
      "step": 300
    },
    {
      "epoch": 0.15027458811782327,
      "grad_norm": 0.2790830731391907,
      "learning_rate": 0.0004984972541188218,
      "loss": 0.0235,
      "step": 301
    },
    {
      "epoch": 0.1507738392411383,
      "grad_norm": 0.32635563611984253,
      "learning_rate": 0.0004984922616075887,
      "loss": 0.0222,
      "step": 302
    },
    {
      "epoch": 0.15127309036445333,
      "grad_norm": 0.5302700400352478,
      "learning_rate": 0.0004984872690963555,
      "loss": 0.0536,
      "step": 303
    },
    {
      "epoch": 0.15177234148776836,
      "grad_norm": 0.32442981004714966,
      "learning_rate": 0.0004984822765851224,
      "loss": 0.0233,
      "step": 304
    },
    {
      "epoch": 0.15227159261108336,
      "grad_norm": 0.37993043661117554,
      "learning_rate": 0.0004984772840738892,
      "loss": 0.0249,
      "step": 305
    },
    {
      "epoch": 0.1527708437343984,
      "grad_norm": 0.2561033070087433,
      "learning_rate": 0.000498472291562656,
      "loss": 0.0194,
      "step": 306
    },
    {
      "epoch": 0.15327009485771342,
      "grad_norm": 0.2945704460144043,
      "learning_rate": 0.0004984672990514229,
      "loss": 0.0178,
      "step": 307
    },
    {
      "epoch": 0.15376934598102845,
      "grad_norm": 0.2627381384372711,
      "learning_rate": 0.0004984623065401897,
      "loss": 0.0188,
      "step": 308
    },
    {
      "epoch": 0.15426859710434349,
      "grad_norm": 0.38039085268974304,
      "learning_rate": 0.0004984573140289566,
      "loss": 0.0299,
      "step": 309
    },
    {
      "epoch": 0.15476784822765852,
      "grad_norm": 0.3577764928340912,
      "learning_rate": 0.0004984523215177234,
      "loss": 0.021,
      "step": 310
    },
    {
      "epoch": 0.15526709935097355,
      "grad_norm": 0.30739372968673706,
      "learning_rate": 0.0004984473290064903,
      "loss": 0.0215,
      "step": 311
    },
    {
      "epoch": 0.15576635047428858,
      "grad_norm": 0.21939809620380402,
      "learning_rate": 0.0004984423364952571,
      "loss": 0.0159,
      "step": 312
    },
    {
      "epoch": 0.15626560159760358,
      "grad_norm": 0.4903862476348877,
      "learning_rate": 0.000498437343984024,
      "loss": 0.0237,
      "step": 313
    },
    {
      "epoch": 0.1567648527209186,
      "grad_norm": 0.28614962100982666,
      "learning_rate": 0.0004984323514727908,
      "loss": 0.0189,
      "step": 314
    },
    {
      "epoch": 0.15726410384423364,
      "grad_norm": 0.14950640499591827,
      "learning_rate": 0.0004984273589615577,
      "loss": 0.0109,
      "step": 315
    },
    {
      "epoch": 0.15776335496754867,
      "grad_norm": 0.24882857501506805,
      "learning_rate": 0.0004984223664503245,
      "loss": 0.0175,
      "step": 316
    },
    {
      "epoch": 0.1582626060908637,
      "grad_norm": 0.3428606688976288,
      "learning_rate": 0.0004984173739390914,
      "loss": 0.0216,
      "step": 317
    },
    {
      "epoch": 0.15876185721417874,
      "grad_norm": 0.30291441082954407,
      "learning_rate": 0.0004984123814278582,
      "loss": 0.0164,
      "step": 318
    },
    {
      "epoch": 0.15926110833749377,
      "grad_norm": 0.4015312194824219,
      "learning_rate": 0.0004984073889166251,
      "loss": 0.0292,
      "step": 319
    },
    {
      "epoch": 0.1597603594608088,
      "grad_norm": 0.2209319919347763,
      "learning_rate": 0.0004984023964053919,
      "loss": 0.0114,
      "step": 320
    },
    {
      "epoch": 0.1602596105841238,
      "grad_norm": 0.36144769191741943,
      "learning_rate": 0.0004983974038941588,
      "loss": 0.0463,
      "step": 321
    },
    {
      "epoch": 0.16075886170743883,
      "grad_norm": 0.25136563181877136,
      "learning_rate": 0.0004983924113829256,
      "loss": 0.0138,
      "step": 322
    },
    {
      "epoch": 0.16125811283075386,
      "grad_norm": 0.47101014852523804,
      "learning_rate": 0.0004983874188716925,
      "loss": 0.0177,
      "step": 323
    },
    {
      "epoch": 0.1617573639540689,
      "grad_norm": 0.18626423180103302,
      "learning_rate": 0.0004983824263604593,
      "loss": 0.0104,
      "step": 324
    },
    {
      "epoch": 0.16225661507738393,
      "grad_norm": 0.8882743120193481,
      "learning_rate": 0.0004983774338492262,
      "loss": 0.0648,
      "step": 325
    },
    {
      "epoch": 0.16275586620069896,
      "grad_norm": 0.19926276803016663,
      "learning_rate": 0.000498372441337993,
      "loss": 0.0202,
      "step": 326
    },
    {
      "epoch": 0.163255117324014,
      "grad_norm": 0.3190975785255432,
      "learning_rate": 0.0004983674488267599,
      "loss": 0.0258,
      "step": 327
    },
    {
      "epoch": 0.16375436844732902,
      "grad_norm": 0.1713508814573288,
      "learning_rate": 0.0004983624563155267,
      "loss": 0.0089,
      "step": 328
    },
    {
      "epoch": 0.16425361957064402,
      "grad_norm": 0.16267497837543488,
      "learning_rate": 0.0004983574638042936,
      "loss": 0.0093,
      "step": 329
    },
    {
      "epoch": 0.16475287069395905,
      "grad_norm": 0.2712850570678711,
      "learning_rate": 0.0004983524712930604,
      "loss": 0.0095,
      "step": 330
    },
    {
      "epoch": 0.16525212181727408,
      "grad_norm": 0.8863838911056519,
      "learning_rate": 0.0004983474787818273,
      "loss": 0.0223,
      "step": 331
    },
    {
      "epoch": 0.16575137294058911,
      "grad_norm": 0.3371226489543915,
      "learning_rate": 0.0004983424862705941,
      "loss": 0.0134,
      "step": 332
    },
    {
      "epoch": 0.16625062406390415,
      "grad_norm": 0.25202974677085876,
      "learning_rate": 0.000498337493759361,
      "loss": 0.0156,
      "step": 333
    },
    {
      "epoch": 0.16674987518721918,
      "grad_norm": 0.311005175113678,
      "learning_rate": 0.0004983325012481278,
      "loss": 0.0216,
      "step": 334
    },
    {
      "epoch": 0.1672491263105342,
      "grad_norm": 0.17022891342639923,
      "learning_rate": 0.0004983275087368946,
      "loss": 0.0114,
      "step": 335
    },
    {
      "epoch": 0.16774837743384924,
      "grad_norm": 0.3386506736278534,
      "learning_rate": 0.0004983225162256615,
      "loss": 0.0289,
      "step": 336
    },
    {
      "epoch": 0.16824762855716424,
      "grad_norm": 0.45013150572776794,
      "learning_rate": 0.0004983175237144283,
      "loss": 0.0414,
      "step": 337
    },
    {
      "epoch": 0.16874687968047927,
      "grad_norm": 0.28446608781814575,
      "learning_rate": 0.0004983125312031953,
      "loss": 0.0185,
      "step": 338
    },
    {
      "epoch": 0.1692461308037943,
      "grad_norm": 0.6510182619094849,
      "learning_rate": 0.000498307538691962,
      "loss": 0.0526,
      "step": 339
    },
    {
      "epoch": 0.16974538192710933,
      "grad_norm": 0.1628807783126831,
      "learning_rate": 0.0004983025461807288,
      "loss": 0.0112,
      "step": 340
    },
    {
      "epoch": 0.17024463305042437,
      "grad_norm": 0.4252367317676544,
      "learning_rate": 0.0004982975536694958,
      "loss": 0.0158,
      "step": 341
    },
    {
      "epoch": 0.1707438841737394,
      "grad_norm": 0.16717536747455597,
      "learning_rate": 0.0004982925611582625,
      "loss": 0.0114,
      "step": 342
    },
    {
      "epoch": 0.17124313529705443,
      "grad_norm": 0.36811602115631104,
      "learning_rate": 0.0004982875686470295,
      "loss": 0.0244,
      "step": 343
    },
    {
      "epoch": 0.17174238642036946,
      "grad_norm": 0.3334439694881439,
      "learning_rate": 0.0004982825761357963,
      "loss": 0.0201,
      "step": 344
    },
    {
      "epoch": 0.17224163754368446,
      "grad_norm": 0.37023085355758667,
      "learning_rate": 0.0004982775836245632,
      "loss": 0.034,
      "step": 345
    },
    {
      "epoch": 0.1727408886669995,
      "grad_norm": 0.21570153534412384,
      "learning_rate": 0.00049827259111333,
      "loss": 0.0254,
      "step": 346
    },
    {
      "epoch": 0.17324013979031452,
      "grad_norm": 0.31631386280059814,
      "learning_rate": 0.0004982675986020969,
      "loss": 0.0197,
      "step": 347
    },
    {
      "epoch": 0.17373939091362955,
      "grad_norm": 0.1977965235710144,
      "learning_rate": 0.0004982626060908637,
      "loss": 0.0105,
      "step": 348
    },
    {
      "epoch": 0.17423864203694459,
      "grad_norm": 0.21342088282108307,
      "learning_rate": 0.0004982576135796306,
      "loss": 0.0179,
      "step": 349
    },
    {
      "epoch": 0.17473789316025962,
      "grad_norm": 0.21684925258159637,
      "learning_rate": 0.0004982526210683974,
      "loss": 0.0116,
      "step": 350
    },
    {
      "epoch": 0.17523714428357465,
      "grad_norm": 0.8456213474273682,
      "learning_rate": 0.0004982476285571643,
      "loss": 0.0373,
      "step": 351
    },
    {
      "epoch": 0.17573639540688968,
      "grad_norm": 0.10263587534427643,
      "learning_rate": 0.0004982426360459311,
      "loss": 0.0089,
      "step": 352
    },
    {
      "epoch": 0.17623564653020468,
      "grad_norm": 0.24039848148822784,
      "learning_rate": 0.000498237643534698,
      "loss": 0.0186,
      "step": 353
    },
    {
      "epoch": 0.1767348976535197,
      "grad_norm": 3.203631639480591,
      "learning_rate": 0.0004982326510234648,
      "loss": 0.0485,
      "step": 354
    },
    {
      "epoch": 0.17723414877683474,
      "grad_norm": 0.4660643935203552,
      "learning_rate": 0.0004982276585122317,
      "loss": 0.0188,
      "step": 355
    },
    {
      "epoch": 0.17773339990014977,
      "grad_norm": 0.6368995904922485,
      "learning_rate": 0.0004982226660009985,
      "loss": 0.0166,
      "step": 356
    },
    {
      "epoch": 0.1782326510234648,
      "grad_norm": 0.11281245946884155,
      "learning_rate": 0.0004982176734897654,
      "loss": 0.0078,
      "step": 357
    },
    {
      "epoch": 0.17873190214677984,
      "grad_norm": 0.1602088212966919,
      "learning_rate": 0.0004982126809785322,
      "loss": 0.0108,
      "step": 358
    },
    {
      "epoch": 0.17923115327009487,
      "grad_norm": 0.3384755849838257,
      "learning_rate": 0.0004982076884672991,
      "loss": 0.0228,
      "step": 359
    },
    {
      "epoch": 0.1797304043934099,
      "grad_norm": 0.23840783536434174,
      "learning_rate": 0.0004982026959560659,
      "loss": 0.0103,
      "step": 360
    },
    {
      "epoch": 0.1802296555167249,
      "grad_norm": 0.8238934874534607,
      "learning_rate": 0.0004981977034448328,
      "loss": 0.0613,
      "step": 361
    },
    {
      "epoch": 0.18072890664003993,
      "grad_norm": 0.35607999563217163,
      "learning_rate": 0.0004981927109335996,
      "loss": 0.0295,
      "step": 362
    },
    {
      "epoch": 0.18122815776335496,
      "grad_norm": 0.5152838826179504,
      "learning_rate": 0.0004981877184223665,
      "loss": 0.021,
      "step": 363
    },
    {
      "epoch": 0.18172740888667,
      "grad_norm": 0.5046747326850891,
      "learning_rate": 0.0004981827259111333,
      "loss": 0.0272,
      "step": 364
    },
    {
      "epoch": 0.18222666000998503,
      "grad_norm": 0.6206425428390503,
      "learning_rate": 0.0004981777333999001,
      "loss": 0.0212,
      "step": 365
    },
    {
      "epoch": 0.18272591113330006,
      "grad_norm": 0.3966296911239624,
      "learning_rate": 0.000498172740888667,
      "loss": 0.0187,
      "step": 366
    },
    {
      "epoch": 0.1832251622566151,
      "grad_norm": 0.40282878279685974,
      "learning_rate": 0.0004981677483774338,
      "loss": 0.0221,
      "step": 367
    },
    {
      "epoch": 0.18372441337993012,
      "grad_norm": 0.162016823887825,
      "learning_rate": 0.0004981627558662007,
      "loss": 0.0097,
      "step": 368
    },
    {
      "epoch": 0.18422366450324512,
      "grad_norm": 0.2090069204568863,
      "learning_rate": 0.0004981577633549675,
      "loss": 0.0105,
      "step": 369
    },
    {
      "epoch": 0.18472291562656015,
      "grad_norm": 0.2476317584514618,
      "learning_rate": 0.0004981527708437344,
      "loss": 0.0305,
      "step": 370
    },
    {
      "epoch": 0.18522216674987518,
      "grad_norm": 0.3463817238807678,
      "learning_rate": 0.0004981477783325012,
      "loss": 0.0176,
      "step": 371
    },
    {
      "epoch": 0.18572141787319021,
      "grad_norm": 0.4815678298473358,
      "learning_rate": 0.0004981427858212681,
      "loss": 0.0267,
      "step": 372
    },
    {
      "epoch": 0.18622066899650525,
      "grad_norm": 1.1537944078445435,
      "learning_rate": 0.0004981377933100349,
      "loss": 0.0315,
      "step": 373
    },
    {
      "epoch": 0.18671992011982028,
      "grad_norm": 0.5663283467292786,
      "learning_rate": 0.0004981328007988018,
      "loss": 0.0227,
      "step": 374
    },
    {
      "epoch": 0.1872191712431353,
      "grad_norm": 0.0830884799361229,
      "learning_rate": 0.0004981278082875686,
      "loss": 0.007,
      "step": 375
    },
    {
      "epoch": 0.18771842236645034,
      "grad_norm": 0.3620222210884094,
      "learning_rate": 0.0004981228157763355,
      "loss": 0.0179,
      "step": 376
    },
    {
      "epoch": 0.18821767348976534,
      "grad_norm": 0.7457088232040405,
      "learning_rate": 0.0004981178232651023,
      "loss": 0.0269,
      "step": 377
    },
    {
      "epoch": 0.18871692461308037,
      "grad_norm": 0.7377716302871704,
      "learning_rate": 0.0004981128307538692,
      "loss": 0.0209,
      "step": 378
    },
    {
      "epoch": 0.1892161757363954,
      "grad_norm": 0.6173860430717468,
      "learning_rate": 0.000498107838242636,
      "loss": 0.0306,
      "step": 379
    },
    {
      "epoch": 0.18971542685971043,
      "grad_norm": 0.2786791920661926,
      "learning_rate": 0.0004981028457314029,
      "loss": 0.0144,
      "step": 380
    },
    {
      "epoch": 0.19021467798302547,
      "grad_norm": 0.5465955138206482,
      "learning_rate": 0.0004980978532201697,
      "loss": 0.0162,
      "step": 381
    },
    {
      "epoch": 0.1907139291063405,
      "grad_norm": 0.2980177700519562,
      "learning_rate": 0.0004980928607089366,
      "loss": 0.0147,
      "step": 382
    },
    {
      "epoch": 0.19121318022965553,
      "grad_norm": 0.4356749355792999,
      "learning_rate": 0.0004980878681977034,
      "loss": 0.0241,
      "step": 383
    },
    {
      "epoch": 0.19171243135297053,
      "grad_norm": 0.41483575105667114,
      "learning_rate": 0.0004980828756864703,
      "loss": 0.0368,
      "step": 384
    },
    {
      "epoch": 0.19221168247628556,
      "grad_norm": 0.4004632234573364,
      "learning_rate": 0.0004980778831752371,
      "loss": 0.015,
      "step": 385
    },
    {
      "epoch": 0.1927109335996006,
      "grad_norm": 0.1644093096256256,
      "learning_rate": 0.000498072890664004,
      "loss": 0.0098,
      "step": 386
    },
    {
      "epoch": 0.19321018472291562,
      "grad_norm": 0.462887704372406,
      "learning_rate": 0.0004980678981527708,
      "loss": 0.0176,
      "step": 387
    },
    {
      "epoch": 0.19370943584623065,
      "grad_norm": 0.43044671416282654,
      "learning_rate": 0.0004980629056415377,
      "loss": 0.0199,
      "step": 388
    },
    {
      "epoch": 0.19420868696954569,
      "grad_norm": 0.29080748558044434,
      "learning_rate": 0.0004980579131303045,
      "loss": 0.0091,
      "step": 389
    },
    {
      "epoch": 0.19470793809286072,
      "grad_norm": 0.36438918113708496,
      "learning_rate": 0.0004980529206190714,
      "loss": 0.0197,
      "step": 390
    },
    {
      "epoch": 0.19520718921617575,
      "grad_norm": 0.34221944212913513,
      "learning_rate": 0.0004980479281078382,
      "loss": 0.0081,
      "step": 391
    },
    {
      "epoch": 0.19570644033949075,
      "grad_norm": 0.3415336012840271,
      "learning_rate": 0.0004980429355966052,
      "loss": 0.0093,
      "step": 392
    },
    {
      "epoch": 0.19620569146280578,
      "grad_norm": 0.39262285828590393,
      "learning_rate": 0.000498037943085372,
      "loss": 0.0273,
      "step": 393
    },
    {
      "epoch": 0.1967049425861208,
      "grad_norm": 0.8043699860572815,
      "learning_rate": 0.0004980329505741387,
      "loss": 0.0476,
      "step": 394
    },
    {
      "epoch": 0.19720419370943584,
      "grad_norm": 0.6797760725021362,
      "learning_rate": 0.0004980279580629057,
      "loss": 0.0203,
      "step": 395
    },
    {
      "epoch": 0.19770344483275087,
      "grad_norm": 0.25347307324409485,
      "learning_rate": 0.0004980229655516725,
      "loss": 0.0175,
      "step": 396
    },
    {
      "epoch": 0.1982026959560659,
      "grad_norm": 0.4367769658565521,
      "learning_rate": 0.0004980179730404394,
      "loss": 0.0137,
      "step": 397
    },
    {
      "epoch": 0.19870194707938094,
      "grad_norm": 0.5607976317405701,
      "learning_rate": 0.0004980129805292062,
      "loss": 0.0411,
      "step": 398
    },
    {
      "epoch": 0.19920119820269597,
      "grad_norm": 1.8950492143630981,
      "learning_rate": 0.0004980079880179731,
      "loss": 0.0311,
      "step": 399
    },
    {
      "epoch": 0.19970044932601097,
      "grad_norm": 0.5127127170562744,
      "learning_rate": 0.0004980029955067399,
      "loss": 0.0325,
      "step": 400
    },
    {
      "epoch": 0.200199700449326,
      "grad_norm": 0.10265443474054337,
      "learning_rate": 0.0004979980029955068,
      "loss": 0.0071,
      "step": 401
    },
    {
      "epoch": 0.20069895157264103,
      "grad_norm": 0.33449554443359375,
      "learning_rate": 0.0004979930104842736,
      "loss": 0.0213,
      "step": 402
    },
    {
      "epoch": 0.20119820269595606,
      "grad_norm": 0.3482711613178253,
      "learning_rate": 0.0004979880179730405,
      "loss": 0.0186,
      "step": 403
    },
    {
      "epoch": 0.2016974538192711,
      "grad_norm": 0.11904166638851166,
      "learning_rate": 0.0004979830254618073,
      "loss": 0.0076,
      "step": 404
    },
    {
      "epoch": 0.20219670494258613,
      "grad_norm": 0.39712226390838623,
      "learning_rate": 0.0004979780329505742,
      "loss": 0.02,
      "step": 405
    },
    {
      "epoch": 0.20269595606590116,
      "grad_norm": 0.7151080369949341,
      "learning_rate": 0.000497973040439341,
      "loss": 0.0181,
      "step": 406
    },
    {
      "epoch": 0.2031952071892162,
      "grad_norm": 0.6826079487800598,
      "learning_rate": 0.0004979680479281079,
      "loss": 0.0201,
      "step": 407
    },
    {
      "epoch": 0.2036944583125312,
      "grad_norm": 0.4518817067146301,
      "learning_rate": 0.0004979630554168747,
      "loss": 0.044,
      "step": 408
    },
    {
      "epoch": 0.20419370943584622,
      "grad_norm": 0.6408411264419556,
      "learning_rate": 0.0004979580629056416,
      "loss": 0.0222,
      "step": 409
    },
    {
      "epoch": 0.20469296055916125,
      "grad_norm": 0.7281955480575562,
      "learning_rate": 0.0004979530703944084,
      "loss": 0.0559,
      "step": 410
    },
    {
      "epoch": 0.20519221168247628,
      "grad_norm": 0.8896526098251343,
      "learning_rate": 0.0004979480778831753,
      "loss": 0.0287,
      "step": 411
    },
    {
      "epoch": 0.20569146280579131,
      "grad_norm": 0.4758574366569519,
      "learning_rate": 0.0004979430853719421,
      "loss": 0.0367,
      "step": 412
    },
    {
      "epoch": 0.20619071392910635,
      "grad_norm": 0.10576330125331879,
      "learning_rate": 0.000497938092860709,
      "loss": 0.0072,
      "step": 413
    },
    {
      "epoch": 0.20668996505242138,
      "grad_norm": 0.656683087348938,
      "learning_rate": 0.0004979331003494758,
      "loss": 0.0218,
      "step": 414
    },
    {
      "epoch": 0.2071892161757364,
      "grad_norm": 0.2572716772556305,
      "learning_rate": 0.0004979281078382427,
      "loss": 0.0115,
      "step": 415
    },
    {
      "epoch": 0.2076884672990514,
      "grad_norm": 0.3172103762626648,
      "learning_rate": 0.0004979231153270095,
      "loss": 0.0142,
      "step": 416
    },
    {
      "epoch": 0.20818771842236644,
      "grad_norm": 0.4653237462043762,
      "learning_rate": 0.0004979181228157764,
      "loss": 0.0243,
      "step": 417
    },
    {
      "epoch": 0.20868696954568147,
      "grad_norm": 0.5954961776733398,
      "learning_rate": 0.0004979131303045432,
      "loss": 0.0151,
      "step": 418
    },
    {
      "epoch": 0.2091862206689965,
      "grad_norm": 0.4224779009819031,
      "learning_rate": 0.0004979081377933101,
      "loss": 0.0252,
      "step": 419
    },
    {
      "epoch": 0.20968547179231153,
      "grad_norm": 0.4427594840526581,
      "learning_rate": 0.0004979031452820769,
      "loss": 0.0202,
      "step": 420
    },
    {
      "epoch": 0.21018472291562657,
      "grad_norm": 0.3406355679035187,
      "learning_rate": 0.0004978981527708438,
      "loss": 0.0148,
      "step": 421
    },
    {
      "epoch": 0.2106839740389416,
      "grad_norm": 0.23539091646671295,
      "learning_rate": 0.0004978931602596106,
      "loss": 0.0111,
      "step": 422
    },
    {
      "epoch": 0.21118322516225663,
      "grad_norm": 0.9061277508735657,
      "learning_rate": 0.0004978881677483774,
      "loss": 0.0424,
      "step": 423
    },
    {
      "epoch": 0.21168247628557163,
      "grad_norm": 0.25198373198509216,
      "learning_rate": 0.0004978831752371443,
      "loss": 0.013,
      "step": 424
    },
    {
      "epoch": 0.21218172740888666,
      "grad_norm": 0.1831667423248291,
      "learning_rate": 0.0004978781827259111,
      "loss": 0.0105,
      "step": 425
    },
    {
      "epoch": 0.2126809785322017,
      "grad_norm": 0.7054685950279236,
      "learning_rate": 0.000497873190214678,
      "loss": 0.0329,
      "step": 426
    },
    {
      "epoch": 0.21318022965551672,
      "grad_norm": 0.5923913717269897,
      "learning_rate": 0.0004978681977034448,
      "loss": 0.0289,
      "step": 427
    },
    {
      "epoch": 0.21367948077883175,
      "grad_norm": 0.23662997782230377,
      "learning_rate": 0.0004978632051922117,
      "loss": 0.0095,
      "step": 428
    },
    {
      "epoch": 0.21417873190214679,
      "grad_norm": 0.3540072441101074,
      "learning_rate": 0.0004978582126809785,
      "loss": 0.0271,
      "step": 429
    },
    {
      "epoch": 0.21467798302546182,
      "grad_norm": 0.26510974764823914,
      "learning_rate": 0.0004978532201697454,
      "loss": 0.0136,
      "step": 430
    },
    {
      "epoch": 0.21517723414877685,
      "grad_norm": 0.39812910556793213,
      "learning_rate": 0.0004978482276585122,
      "loss": 0.0286,
      "step": 431
    },
    {
      "epoch": 0.21567648527209185,
      "grad_norm": 0.17231446504592896,
      "learning_rate": 0.0004978432351472791,
      "loss": 0.0109,
      "step": 432
    },
    {
      "epoch": 0.21617573639540688,
      "grad_norm": 0.2364531010389328,
      "learning_rate": 0.0004978382426360459,
      "loss": 0.0107,
      "step": 433
    },
    {
      "epoch": 0.2166749875187219,
      "grad_norm": 0.4610908031463623,
      "learning_rate": 0.0004978332501248128,
      "loss": 0.0335,
      "step": 434
    },
    {
      "epoch": 0.21717423864203694,
      "grad_norm": 0.45729297399520874,
      "learning_rate": 0.0004978282576135796,
      "loss": 0.0133,
      "step": 435
    },
    {
      "epoch": 0.21767348976535197,
      "grad_norm": 0.31068676710128784,
      "learning_rate": 0.0004978232651023465,
      "loss": 0.0124,
      "step": 436
    },
    {
      "epoch": 0.218172740888667,
      "grad_norm": 0.342297226190567,
      "learning_rate": 0.0004978182725911133,
      "loss": 0.0101,
      "step": 437
    },
    {
      "epoch": 0.21867199201198204,
      "grad_norm": 0.14761732518672943,
      "learning_rate": 0.0004978132800798802,
      "loss": 0.0081,
      "step": 438
    },
    {
      "epoch": 0.21917124313529707,
      "grad_norm": 1.0826786756515503,
      "learning_rate": 0.000497808287568647,
      "loss": 0.0233,
      "step": 439
    },
    {
      "epoch": 0.21967049425861207,
      "grad_norm": 0.4059954881668091,
      "learning_rate": 0.0004978032950574139,
      "loss": 0.0268,
      "step": 440
    },
    {
      "epoch": 0.2201697453819271,
      "grad_norm": 0.3560104966163635,
      "learning_rate": 0.0004977983025461807,
      "loss": 0.0142,
      "step": 441
    },
    {
      "epoch": 0.22066899650524213,
      "grad_norm": 0.34067675471305847,
      "learning_rate": 0.0004977933100349476,
      "loss": 0.0154,
      "step": 442
    },
    {
      "epoch": 0.22116824762855716,
      "grad_norm": 0.12392249703407288,
      "learning_rate": 0.0004977883175237144,
      "loss": 0.0067,
      "step": 443
    },
    {
      "epoch": 0.2216674987518722,
      "grad_norm": 0.2913663387298584,
      "learning_rate": 0.0004977833250124814,
      "loss": 0.0071,
      "step": 444
    },
    {
      "epoch": 0.22216674987518722,
      "grad_norm": 0.5311196446418762,
      "learning_rate": 0.0004977783325012482,
      "loss": 0.0204,
      "step": 445
    },
    {
      "epoch": 0.22266600099850226,
      "grad_norm": 0.25609177350997925,
      "learning_rate": 0.0004977733399900151,
      "loss": 0.0157,
      "step": 446
    },
    {
      "epoch": 0.2231652521218173,
      "grad_norm": 1.2032849788665771,
      "learning_rate": 0.0004977683474787819,
      "loss": 0.0497,
      "step": 447
    },
    {
      "epoch": 0.2236645032451323,
      "grad_norm": 0.11622841656208038,
      "learning_rate": 0.0004977633549675488,
      "loss": 0.0075,
      "step": 448
    },
    {
      "epoch": 0.22416375436844732,
      "grad_norm": 0.2736823558807373,
      "learning_rate": 0.0004977583624563156,
      "loss": 0.0102,
      "step": 449
    },
    {
      "epoch": 0.22466300549176235,
      "grad_norm": 0.42552053928375244,
      "learning_rate": 0.0004977533699450825,
      "loss": 0.015,
      "step": 450
    },
    {
      "epoch": 0.22516225661507738,
      "grad_norm": 0.3392028510570526,
      "learning_rate": 0.0004977483774338493,
      "loss": 0.0164,
      "step": 451
    },
    {
      "epoch": 0.22566150773839241,
      "grad_norm": 0.5841475129127502,
      "learning_rate": 0.0004977433849226161,
      "loss": 0.041,
      "step": 452
    },
    {
      "epoch": 0.22616075886170744,
      "grad_norm": 0.4682171940803528,
      "learning_rate": 0.0004977383924113829,
      "loss": 0.0208,
      "step": 453
    },
    {
      "epoch": 0.22666000998502248,
      "grad_norm": 0.831426739692688,
      "learning_rate": 0.0004977333999001498,
      "loss": 0.0153,
      "step": 454
    },
    {
      "epoch": 0.2271592611083375,
      "grad_norm": 0.2822071611881256,
      "learning_rate": 0.0004977284073889166,
      "loss": 0.0114,
      "step": 455
    },
    {
      "epoch": 0.2276585122316525,
      "grad_norm": 0.24419744312763214,
      "learning_rate": 0.0004977234148776835,
      "loss": 0.0122,
      "step": 456
    },
    {
      "epoch": 0.22815776335496754,
      "grad_norm": 0.2852911651134491,
      "learning_rate": 0.0004977184223664503,
      "loss": 0.0158,
      "step": 457
    },
    {
      "epoch": 0.22865701447828257,
      "grad_norm": 0.2960560917854309,
      "learning_rate": 0.0004977134298552172,
      "loss": 0.011,
      "step": 458
    },
    {
      "epoch": 0.2291562656015976,
      "grad_norm": 0.4534308910369873,
      "learning_rate": 0.000497708437343984,
      "loss": 0.0221,
      "step": 459
    },
    {
      "epoch": 0.22965551672491263,
      "grad_norm": 0.3528191149234772,
      "learning_rate": 0.0004977034448327509,
      "loss": 0.0107,
      "step": 460
    },
    {
      "epoch": 0.23015476784822766,
      "grad_norm": 0.4989500641822815,
      "learning_rate": 0.0004976984523215177,
      "loss": 0.0402,
      "step": 461
    },
    {
      "epoch": 0.2306540189715427,
      "grad_norm": 0.3103615939617157,
      "learning_rate": 0.0004976934598102846,
      "loss": 0.017,
      "step": 462
    },
    {
      "epoch": 0.23115327009485773,
      "grad_norm": 0.20017288625240326,
      "learning_rate": 0.0004976884672990514,
      "loss": 0.009,
      "step": 463
    },
    {
      "epoch": 0.23165252121817273,
      "grad_norm": 0.4004080593585968,
      "learning_rate": 0.0004976834747878183,
      "loss": 0.0209,
      "step": 464
    },
    {
      "epoch": 0.23215177234148776,
      "grad_norm": 0.4593498110771179,
      "learning_rate": 0.0004976784822765851,
      "loss": 0.0314,
      "step": 465
    },
    {
      "epoch": 0.2326510234648028,
      "grad_norm": 0.21695592999458313,
      "learning_rate": 0.000497673489765352,
      "loss": 0.0105,
      "step": 466
    },
    {
      "epoch": 0.23315027458811782,
      "grad_norm": 0.6713131666183472,
      "learning_rate": 0.0004976684972541188,
      "loss": 0.0334,
      "step": 467
    },
    {
      "epoch": 0.23364952571143285,
      "grad_norm": 0.4261476397514343,
      "learning_rate": 0.0004976635047428857,
      "loss": 0.0197,
      "step": 468
    },
    {
      "epoch": 0.23414877683474788,
      "grad_norm": 0.6602514386177063,
      "learning_rate": 0.0004976585122316525,
      "loss": 0.0418,
      "step": 469
    },
    {
      "epoch": 0.23464802795806292,
      "grad_norm": 0.24012231826782227,
      "learning_rate": 0.0004976535197204194,
      "loss": 0.0105,
      "step": 470
    },
    {
      "epoch": 0.23514727908137795,
      "grad_norm": 0.2287817746400833,
      "learning_rate": 0.0004976485272091862,
      "loss": 0.0134,
      "step": 471
    },
    {
      "epoch": 0.23564653020469295,
      "grad_norm": 0.6722608804702759,
      "learning_rate": 0.0004976435346979531,
      "loss": 0.0214,
      "step": 472
    },
    {
      "epoch": 0.23614578132800798,
      "grad_norm": 0.3135509192943573,
      "learning_rate": 0.0004976385421867199,
      "loss": 0.0108,
      "step": 473
    },
    {
      "epoch": 0.236645032451323,
      "grad_norm": 0.2726273536682129,
      "learning_rate": 0.0004976335496754868,
      "loss": 0.012,
      "step": 474
    },
    {
      "epoch": 0.23714428357463804,
      "grad_norm": 0.10395726561546326,
      "learning_rate": 0.0004976285571642536,
      "loss": 0.0061,
      "step": 475
    },
    {
      "epoch": 0.23764353469795307,
      "grad_norm": 0.19602984189987183,
      "learning_rate": 0.0004976235646530205,
      "loss": 0.0145,
      "step": 476
    },
    {
      "epoch": 0.2381427858212681,
      "grad_norm": 0.527260422706604,
      "learning_rate": 0.0004976185721417873,
      "loss": 0.0159,
      "step": 477
    },
    {
      "epoch": 0.23864203694458314,
      "grad_norm": 0.5286039113998413,
      "learning_rate": 0.0004976135796305542,
      "loss": 0.0269,
      "step": 478
    },
    {
      "epoch": 0.23914128806789814,
      "grad_norm": 0.5188347697257996,
      "learning_rate": 0.000497608587119321,
      "loss": 0.0418,
      "step": 479
    },
    {
      "epoch": 0.23964053919121317,
      "grad_norm": 0.5474590063095093,
      "learning_rate": 0.0004976035946080879,
      "loss": 0.0206,
      "step": 480
    },
    {
      "epoch": 0.2401397903145282,
      "grad_norm": 0.5054095387458801,
      "learning_rate": 0.0004975986020968547,
      "loss": 0.0293,
      "step": 481
    },
    {
      "epoch": 0.24063904143784323,
      "grad_norm": 0.36387741565704346,
      "learning_rate": 0.0004975936095856215,
      "loss": 0.0179,
      "step": 482
    },
    {
      "epoch": 0.24113829256115826,
      "grad_norm": 0.19145028293132782,
      "learning_rate": 0.0004975886170743884,
      "loss": 0.0079,
      "step": 483
    },
    {
      "epoch": 0.2416375436844733,
      "grad_norm": 0.19485890865325928,
      "learning_rate": 0.0004975836245631552,
      "loss": 0.0102,
      "step": 484
    },
    {
      "epoch": 0.24213679480778832,
      "grad_norm": 0.22330009937286377,
      "learning_rate": 0.0004975786320519221,
      "loss": 0.0091,
      "step": 485
    },
    {
      "epoch": 0.24263604593110336,
      "grad_norm": 0.3621061146259308,
      "learning_rate": 0.0004975736395406889,
      "loss": 0.0146,
      "step": 486
    },
    {
      "epoch": 0.24313529705441836,
      "grad_norm": 0.5967867374420166,
      "learning_rate": 0.0004975686470294558,
      "loss": 0.0315,
      "step": 487
    },
    {
      "epoch": 0.2436345481777334,
      "grad_norm": 0.11396866291761398,
      "learning_rate": 0.0004975636545182226,
      "loss": 0.0069,
      "step": 488
    },
    {
      "epoch": 0.24413379930104842,
      "grad_norm": 0.3112516403198242,
      "learning_rate": 0.0004975586620069895,
      "loss": 0.0092,
      "step": 489
    },
    {
      "epoch": 0.24463305042436345,
      "grad_norm": 0.47774460911750793,
      "learning_rate": 0.0004975536694957563,
      "loss": 0.028,
      "step": 490
    },
    {
      "epoch": 0.24513230154767848,
      "grad_norm": 0.15426816046237946,
      "learning_rate": 0.0004975486769845232,
      "loss": 0.006,
      "step": 491
    },
    {
      "epoch": 0.2456315526709935,
      "grad_norm": 0.3879881501197815,
      "learning_rate": 0.00049754368447329,
      "loss": 0.0202,
      "step": 492
    },
    {
      "epoch": 0.24613080379430854,
      "grad_norm": 0.34649983048439026,
      "learning_rate": 0.0004975386919620569,
      "loss": 0.0311,
      "step": 493
    },
    {
      "epoch": 0.24663005491762358,
      "grad_norm": 0.2751879096031189,
      "learning_rate": 0.0004975336994508237,
      "loss": 0.0215,
      "step": 494
    },
    {
      "epoch": 0.24712930604093858,
      "grad_norm": 0.29786938428878784,
      "learning_rate": 0.0004975287069395906,
      "loss": 0.0207,
      "step": 495
    },
    {
      "epoch": 0.2476285571642536,
      "grad_norm": 0.14018620550632477,
      "learning_rate": 0.0004975237144283574,
      "loss": 0.0082,
      "step": 496
    },
    {
      "epoch": 0.24812780828756864,
      "grad_norm": 0.10039366781711578,
      "learning_rate": 0.0004975187219171243,
      "loss": 0.006,
      "step": 497
    },
    {
      "epoch": 0.24862705941088367,
      "grad_norm": 0.12484975159168243,
      "learning_rate": 0.0004975137294058911,
      "loss": 0.0071,
      "step": 498
    },
    {
      "epoch": 0.2491263105341987,
      "grad_norm": 0.37436240911483765,
      "learning_rate": 0.000497508736894658,
      "loss": 0.0153,
      "step": 499
    },
    {
      "epoch": 0.24962556165751373,
      "grad_norm": 0.421562522649765,
      "learning_rate": 0.0004975037443834249,
      "loss": 0.0155,
      "step": 500
    },
    {
      "epoch": 0.25012481278082876,
      "grad_norm": 0.2808127999305725,
      "learning_rate": 0.0004974987518721918,
      "loss": 0.0099,
      "step": 501
    },
    {
      "epoch": 0.25062406390414377,
      "grad_norm": 0.2958044111728668,
      "learning_rate": 0.0004974937593609586,
      "loss": 0.0114,
      "step": 502
    },
    {
      "epoch": 0.2511233150274588,
      "grad_norm": 0.09761922061443329,
      "learning_rate": 0.0004974887668497255,
      "loss": 0.0052,
      "step": 503
    },
    {
      "epoch": 0.25162256615077383,
      "grad_norm": 0.22451110184192657,
      "learning_rate": 0.0004974837743384923,
      "loss": 0.0112,
      "step": 504
    },
    {
      "epoch": 0.2521218172740889,
      "grad_norm": 0.24369734525680542,
      "learning_rate": 0.0004974787818272592,
      "loss": 0.0103,
      "step": 505
    },
    {
      "epoch": 0.2526210683974039,
      "grad_norm": 0.12409916520118713,
      "learning_rate": 0.000497473789316026,
      "loss": 0.007,
      "step": 506
    },
    {
      "epoch": 0.2531203195207189,
      "grad_norm": 0.23548008501529694,
      "learning_rate": 0.0004974687968047929,
      "loss": 0.0115,
      "step": 507
    },
    {
      "epoch": 0.25361957064403395,
      "grad_norm": 0.27682793140411377,
      "learning_rate": 0.0004974638042935597,
      "loss": 0.0078,
      "step": 508
    },
    {
      "epoch": 0.25411882176734896,
      "grad_norm": 0.12769237160682678,
      "learning_rate": 0.0004974588117823266,
      "loss": 0.0056,
      "step": 509
    },
    {
      "epoch": 0.254618072890664,
      "grad_norm": 0.06449143588542938,
      "learning_rate": 0.0004974538192710934,
      "loss": 0.0039,
      "step": 510
    },
    {
      "epoch": 0.255117324013979,
      "grad_norm": 0.4094061553478241,
      "learning_rate": 0.0004974488267598602,
      "loss": 0.0165,
      "step": 511
    },
    {
      "epoch": 0.2556165751372941,
      "grad_norm": 0.13847176730632782,
      "learning_rate": 0.0004974438342486271,
      "loss": 0.0049,
      "step": 512
    },
    {
      "epoch": 0.2561158262606091,
      "grad_norm": 0.4546605944633484,
      "learning_rate": 0.0004974388417373939,
      "loss": 0.0144,
      "step": 513
    },
    {
      "epoch": 0.25661507738392414,
      "grad_norm": 0.03287729248404503,
      "learning_rate": 0.0004974338492261608,
      "loss": 0.0027,
      "step": 514
    },
    {
      "epoch": 0.25711432850723914,
      "grad_norm": 0.09838037937879562,
      "learning_rate": 0.0004974288567149276,
      "loss": 0.0047,
      "step": 515
    },
    {
      "epoch": 0.25761357963055415,
      "grad_norm": 0.35333195328712463,
      "learning_rate": 0.0004974238642036945,
      "loss": 0.0093,
      "step": 516
    },
    {
      "epoch": 0.2581128307538692,
      "grad_norm": 0.30428940057754517,
      "learning_rate": 0.0004974188716924613,
      "loss": 0.0069,
      "step": 517
    },
    {
      "epoch": 0.2586120818771842,
      "grad_norm": 0.09002097696065903,
      "learning_rate": 0.0004974138791812282,
      "loss": 0.004,
      "step": 518
    },
    {
      "epoch": 0.25911133300049927,
      "grad_norm": 0.07981818914413452,
      "learning_rate": 0.000497408886669995,
      "loss": 0.0041,
      "step": 519
    },
    {
      "epoch": 0.25961058412381427,
      "grad_norm": 0.2681529223918915,
      "learning_rate": 0.0004974038941587619,
      "loss": 0.0064,
      "step": 520
    },
    {
      "epoch": 0.26010983524712933,
      "grad_norm": 0.3404737710952759,
      "learning_rate": 0.0004973989016475287,
      "loss": 0.025,
      "step": 521
    },
    {
      "epoch": 0.26060908637044433,
      "grad_norm": 0.47533586621284485,
      "learning_rate": 0.0004973939091362956,
      "loss": 0.0167,
      "step": 522
    },
    {
      "epoch": 0.26110833749375933,
      "grad_norm": 0.2730868458747864,
      "learning_rate": 0.0004973889166250624,
      "loss": 0.0106,
      "step": 523
    },
    {
      "epoch": 0.2616075886170744,
      "grad_norm": 0.43411147594451904,
      "learning_rate": 0.0004973839241138293,
      "loss": 0.0303,
      "step": 524
    },
    {
      "epoch": 0.2621068397403894,
      "grad_norm": 0.32843828201293945,
      "learning_rate": 0.0004973789316025961,
      "loss": 0.0111,
      "step": 525
    },
    {
      "epoch": 0.26260609086370446,
      "grad_norm": 0.18297185003757477,
      "learning_rate": 0.000497373939091363,
      "loss": 0.0094,
      "step": 526
    },
    {
      "epoch": 0.26310534198701946,
      "grad_norm": 0.47050178050994873,
      "learning_rate": 0.0004973689465801298,
      "loss": 0.0244,
      "step": 527
    },
    {
      "epoch": 0.2636045931103345,
      "grad_norm": 0.08151901513338089,
      "learning_rate": 0.0004973639540688967,
      "loss": 0.0041,
      "step": 528
    },
    {
      "epoch": 0.2641038442336495,
      "grad_norm": 0.3401853144168854,
      "learning_rate": 0.0004973589615576635,
      "loss": 0.0262,
      "step": 529
    },
    {
      "epoch": 0.2646030953569646,
      "grad_norm": 0.21242432296276093,
      "learning_rate": 0.0004973539690464304,
      "loss": 0.0133,
      "step": 530
    },
    {
      "epoch": 0.2651023464802796,
      "grad_norm": 0.3884665071964264,
      "learning_rate": 0.0004973489765351972,
      "loss": 0.0253,
      "step": 531
    },
    {
      "epoch": 0.2656015976035946,
      "grad_norm": 0.1628330647945404,
      "learning_rate": 0.0004973439840239641,
      "loss": 0.0078,
      "step": 532
    },
    {
      "epoch": 0.26610084872690964,
      "grad_norm": 0.05428473278880119,
      "learning_rate": 0.0004973389915127309,
      "loss": 0.0036,
      "step": 533
    },
    {
      "epoch": 0.26660009985022465,
      "grad_norm": 0.1348518431186676,
      "learning_rate": 0.0004973339990014978,
      "loss": 0.0081,
      "step": 534
    },
    {
      "epoch": 0.2670993509735397,
      "grad_norm": 0.12901152670383453,
      "learning_rate": 0.0004973290064902646,
      "loss": 0.0054,
      "step": 535
    },
    {
      "epoch": 0.2675986020968547,
      "grad_norm": 0.33479276299476624,
      "learning_rate": 0.0004973240139790315,
      "loss": 0.0106,
      "step": 536
    },
    {
      "epoch": 0.26809785322016977,
      "grad_norm": 0.17144444584846497,
      "learning_rate": 0.0004973190214677983,
      "loss": 0.006,
      "step": 537
    },
    {
      "epoch": 0.26859710434348477,
      "grad_norm": 0.15416297316551208,
      "learning_rate": 0.0004973140289565652,
      "loss": 0.0057,
      "step": 538
    },
    {
      "epoch": 0.2690963554667998,
      "grad_norm": 0.3106882870197296,
      "learning_rate": 0.000497309036445332,
      "loss": 0.0106,
      "step": 539
    },
    {
      "epoch": 0.26959560659011483,
      "grad_norm": 0.1989317089319229,
      "learning_rate": 0.0004973040439340988,
      "loss": 0.0082,
      "step": 540
    },
    {
      "epoch": 0.27009485771342984,
      "grad_norm": 0.14520885050296783,
      "learning_rate": 0.0004972990514228657,
      "loss": 0.0051,
      "step": 541
    },
    {
      "epoch": 0.2705941088367449,
      "grad_norm": 0.6280295252799988,
      "learning_rate": 0.0004972940589116325,
      "loss": 0.0276,
      "step": 542
    },
    {
      "epoch": 0.2710933599600599,
      "grad_norm": 0.2821381390094757,
      "learning_rate": 0.0004972890664003994,
      "loss": 0.0205,
      "step": 543
    },
    {
      "epoch": 0.27159261108337496,
      "grad_norm": 0.43616971373558044,
      "learning_rate": 0.0004972840738891662,
      "loss": 0.0147,
      "step": 544
    },
    {
      "epoch": 0.27209186220668996,
      "grad_norm": 0.5848397016525269,
      "learning_rate": 0.0004972790813779331,
      "loss": 0.0195,
      "step": 545
    },
    {
      "epoch": 0.272591113330005,
      "grad_norm": 0.12241332978010178,
      "learning_rate": 0.0004972740888666999,
      "loss": 0.0054,
      "step": 546
    },
    {
      "epoch": 0.27309036445332,
      "grad_norm": 0.5571544766426086,
      "learning_rate": 0.0004972690963554668,
      "loss": 0.0295,
      "step": 547
    },
    {
      "epoch": 0.273589615576635,
      "grad_norm": 0.19601480662822723,
      "learning_rate": 0.0004972641038442336,
      "loss": 0.0105,
      "step": 548
    },
    {
      "epoch": 0.2740888666999501,
      "grad_norm": 0.126230388879776,
      "learning_rate": 0.0004972591113330005,
      "loss": 0.0051,
      "step": 549
    },
    {
      "epoch": 0.2745881178232651,
      "grad_norm": 0.24226561188697815,
      "learning_rate": 0.0004972541188217673,
      "loss": 0.0073,
      "step": 550
    },
    {
      "epoch": 0.27508736894658015,
      "grad_norm": 0.28119292855262756,
      "learning_rate": 0.0004972491263105343,
      "loss": 0.0151,
      "step": 551
    },
    {
      "epoch": 0.27558662006989515,
      "grad_norm": 0.29309678077697754,
      "learning_rate": 0.000497244133799301,
      "loss": 0.0089,
      "step": 552
    },
    {
      "epoch": 0.2760858711932102,
      "grad_norm": 0.27233371138572693,
      "learning_rate": 0.000497239141288068,
      "loss": 0.0097,
      "step": 553
    },
    {
      "epoch": 0.2765851223165252,
      "grad_norm": 0.08507966995239258,
      "learning_rate": 0.0004972341487768348,
      "loss": 0.0029,
      "step": 554
    },
    {
      "epoch": 0.2770843734398402,
      "grad_norm": 0.07916413992643356,
      "learning_rate": 0.0004972291562656017,
      "loss": 0.0046,
      "step": 555
    },
    {
      "epoch": 0.2775836245631553,
      "grad_norm": 0.06451600044965744,
      "learning_rate": 0.0004972241637543685,
      "loss": 0.004,
      "step": 556
    },
    {
      "epoch": 0.2780828756864703,
      "grad_norm": 0.3125411868095398,
      "learning_rate": 0.0004972191712431354,
      "loss": 0.0143,
      "step": 557
    },
    {
      "epoch": 0.27858212680978534,
      "grad_norm": 0.3625158667564392,
      "learning_rate": 0.0004972141787319022,
      "loss": 0.0136,
      "step": 558
    },
    {
      "epoch": 0.27908137793310034,
      "grad_norm": 0.18113961815834045,
      "learning_rate": 0.0004972091862206691,
      "loss": 0.0055,
      "step": 559
    },
    {
      "epoch": 0.2795806290564154,
      "grad_norm": 0.13786162436008453,
      "learning_rate": 0.0004972041937094359,
      "loss": 0.0027,
      "step": 560
    },
    {
      "epoch": 0.2800798801797304,
      "grad_norm": 0.6843329071998596,
      "learning_rate": 0.0004971992011982028,
      "loss": 0.0081,
      "step": 561
    },
    {
      "epoch": 0.28057913130304546,
      "grad_norm": 0.5062842965126038,
      "learning_rate": 0.0004971942086869696,
      "loss": 0.0187,
      "step": 562
    },
    {
      "epoch": 0.28107838242636046,
      "grad_norm": 0.5827528238296509,
      "learning_rate": 0.0004971892161757365,
      "loss": 0.0168,
      "step": 563
    },
    {
      "epoch": 0.28157763354967547,
      "grad_norm": 0.47176915407180786,
      "learning_rate": 0.0004971842236645033,
      "loss": 0.028,
      "step": 564
    },
    {
      "epoch": 0.2820768846729905,
      "grad_norm": 0.22202517092227936,
      "learning_rate": 0.0004971792311532702,
      "loss": 0.01,
      "step": 565
    },
    {
      "epoch": 0.2825761357963055,
      "grad_norm": 0.40936052799224854,
      "learning_rate": 0.000497174238642037,
      "loss": 0.0195,
      "step": 566
    },
    {
      "epoch": 0.2830753869196206,
      "grad_norm": 0.4021313488483429,
      "learning_rate": 0.0004971692461308039,
      "loss": 0.0093,
      "step": 567
    },
    {
      "epoch": 0.2835746380429356,
      "grad_norm": 0.31233108043670654,
      "learning_rate": 0.0004971642536195706,
      "loss": 0.0119,
      "step": 568
    },
    {
      "epoch": 0.28407388916625065,
      "grad_norm": 0.2368549406528473,
      "learning_rate": 0.0004971592611083375,
      "loss": 0.0059,
      "step": 569
    },
    {
      "epoch": 0.28457314028956565,
      "grad_norm": 0.10436473041772842,
      "learning_rate": 0.0004971542685971043,
      "loss": 0.0049,
      "step": 570
    },
    {
      "epoch": 0.28507239141288065,
      "grad_norm": 0.07569325715303421,
      "learning_rate": 0.0004971492760858712,
      "loss": 0.0043,
      "step": 571
    },
    {
      "epoch": 0.2855716425361957,
      "grad_norm": 0.3122921884059906,
      "learning_rate": 0.000497144283574638,
      "loss": 0.0081,
      "step": 572
    },
    {
      "epoch": 0.2860708936595107,
      "grad_norm": 0.6980115175247192,
      "learning_rate": 0.0004971392910634049,
      "loss": 0.014,
      "step": 573
    },
    {
      "epoch": 0.2865701447828258,
      "grad_norm": 0.38489121198654175,
      "learning_rate": 0.0004971342985521717,
      "loss": 0.0115,
      "step": 574
    },
    {
      "epoch": 0.2870693959061408,
      "grad_norm": 0.10463137179613113,
      "learning_rate": 0.0004971293060409386,
      "loss": 0.0045,
      "step": 575
    },
    {
      "epoch": 0.28756864702945584,
      "grad_norm": 0.1686391979455948,
      "learning_rate": 0.0004971243135297054,
      "loss": 0.0068,
      "step": 576
    },
    {
      "epoch": 0.28806789815277084,
      "grad_norm": 0.12350894510746002,
      "learning_rate": 0.0004971193210184723,
      "loss": 0.0065,
      "step": 577
    },
    {
      "epoch": 0.28856714927608584,
      "grad_norm": 0.1904495656490326,
      "learning_rate": 0.0004971143285072391,
      "loss": 0.0065,
      "step": 578
    },
    {
      "epoch": 0.2890664003994009,
      "grad_norm": 0.44460487365722656,
      "learning_rate": 0.000497109335996006,
      "loss": 0.0105,
      "step": 579
    },
    {
      "epoch": 0.2895656515227159,
      "grad_norm": 0.345329225063324,
      "learning_rate": 0.0004971043434847728,
      "loss": 0.0138,
      "step": 580
    },
    {
      "epoch": 0.29006490264603096,
      "grad_norm": 0.30766281485557556,
      "learning_rate": 0.0004970993509735397,
      "loss": 0.009,
      "step": 581
    },
    {
      "epoch": 0.29056415376934597,
      "grad_norm": 0.3096223771572113,
      "learning_rate": 0.0004970943584623065,
      "loss": 0.0058,
      "step": 582
    },
    {
      "epoch": 0.291063404892661,
      "grad_norm": 0.6485788226127625,
      "learning_rate": 0.0004970893659510734,
      "loss": 0.0184,
      "step": 583
    },
    {
      "epoch": 0.29156265601597603,
      "grad_norm": 0.32862383127212524,
      "learning_rate": 0.0004970843734398402,
      "loss": 0.0125,
      "step": 584
    },
    {
      "epoch": 0.2920619071392911,
      "grad_norm": 0.06464998424053192,
      "learning_rate": 0.0004970793809286071,
      "loss": 0.0035,
      "step": 585
    },
    {
      "epoch": 0.2925611582626061,
      "grad_norm": 0.20557266473770142,
      "learning_rate": 0.0004970743884173739,
      "loss": 0.0145,
      "step": 586
    },
    {
      "epoch": 0.2930604093859211,
      "grad_norm": 0.23507060110569,
      "learning_rate": 0.0004970693959061408,
      "loss": 0.0106,
      "step": 587
    },
    {
      "epoch": 0.29355966050923615,
      "grad_norm": 0.7718417048454285,
      "learning_rate": 0.0004970644033949076,
      "loss": 0.0207,
      "step": 588
    },
    {
      "epoch": 0.29405891163255116,
      "grad_norm": 0.7100785970687866,
      "learning_rate": 0.0004970594108836745,
      "loss": 0.0103,
      "step": 589
    },
    {
      "epoch": 0.2945581627558662,
      "grad_norm": 0.3842492997646332,
      "learning_rate": 0.0004970544183724413,
      "loss": 0.0219,
      "step": 590
    },
    {
      "epoch": 0.2950574138791812,
      "grad_norm": 0.12245526909828186,
      "learning_rate": 0.0004970494258612082,
      "loss": 0.0051,
      "step": 591
    },
    {
      "epoch": 0.2955566650024963,
      "grad_norm": 0.07685684412717819,
      "learning_rate": 0.000497044433349975,
      "loss": 0.0035,
      "step": 592
    },
    {
      "epoch": 0.2960559161258113,
      "grad_norm": 0.6370843648910522,
      "learning_rate": 0.0004970394408387419,
      "loss": 0.0195,
      "step": 593
    },
    {
      "epoch": 0.2965551672491263,
      "grad_norm": 0.432134211063385,
      "learning_rate": 0.0004970344483275087,
      "loss": 0.01,
      "step": 594
    },
    {
      "epoch": 0.29705441837244134,
      "grad_norm": 0.4717816412448883,
      "learning_rate": 0.0004970294558162756,
      "loss": 0.0124,
      "step": 595
    },
    {
      "epoch": 0.29755366949575635,
      "grad_norm": 0.42630085349082947,
      "learning_rate": 0.0004970244633050424,
      "loss": 0.014,
      "step": 596
    },
    {
      "epoch": 0.2980529206190714,
      "grad_norm": 0.21061989665031433,
      "learning_rate": 0.0004970194707938092,
      "loss": 0.0079,
      "step": 597
    },
    {
      "epoch": 0.2985521717423864,
      "grad_norm": 0.284775972366333,
      "learning_rate": 0.0004970144782825761,
      "loss": 0.0253,
      "step": 598
    },
    {
      "epoch": 0.29905142286570147,
      "grad_norm": 0.2746272087097168,
      "learning_rate": 0.0004970094857713429,
      "loss": 0.0096,
      "step": 599
    },
    {
      "epoch": 0.29955067398901647,
      "grad_norm": 0.4617641866207123,
      "learning_rate": 0.0004970044932601098,
      "loss": 0.0269,
      "step": 600
    },
    {
      "epoch": 0.30004992511233153,
      "grad_norm": 0.3236202597618103,
      "learning_rate": 0.0004969995007488766,
      "loss": 0.0126,
      "step": 601
    },
    {
      "epoch": 0.30054917623564653,
      "grad_norm": 0.3130728006362915,
      "learning_rate": 0.0004969945082376435,
      "loss": 0.0119,
      "step": 602
    },
    {
      "epoch": 0.30104842735896153,
      "grad_norm": 0.24224838614463806,
      "learning_rate": 0.0004969895157264103,
      "loss": 0.009,
      "step": 603
    },
    {
      "epoch": 0.3015476784822766,
      "grad_norm": 0.436201274394989,
      "learning_rate": 0.0004969845232151772,
      "loss": 0.0091,
      "step": 604
    },
    {
      "epoch": 0.3020469296055916,
      "grad_norm": 0.4349399209022522,
      "learning_rate": 0.000496979530703944,
      "loss": 0.0078,
      "step": 605
    },
    {
      "epoch": 0.30254618072890666,
      "grad_norm": 0.8755477070808411,
      "learning_rate": 0.000496974538192711,
      "loss": 0.0127,
      "step": 606
    },
    {
      "epoch": 0.30304543185222166,
      "grad_norm": 0.2111244648694992,
      "learning_rate": 0.0004969695456814778,
      "loss": 0.01,
      "step": 607
    },
    {
      "epoch": 0.3035446829755367,
      "grad_norm": 0.29924264550209045,
      "learning_rate": 0.0004969645531702447,
      "loss": 0.0158,
      "step": 608
    },
    {
      "epoch": 0.3040439340988517,
      "grad_norm": 0.18247048556804657,
      "learning_rate": 0.0004969595606590115,
      "loss": 0.0067,
      "step": 609
    },
    {
      "epoch": 0.3045431852221667,
      "grad_norm": 0.6746491193771362,
      "learning_rate": 0.0004969545681477784,
      "loss": 0.0192,
      "step": 610
    },
    {
      "epoch": 0.3050424363454818,
      "grad_norm": 0.14235419034957886,
      "learning_rate": 0.0004969495756365452,
      "loss": 0.0068,
      "step": 611
    },
    {
      "epoch": 0.3055416874687968,
      "grad_norm": 0.25959840416908264,
      "learning_rate": 0.0004969445831253121,
      "loss": 0.0121,
      "step": 612
    },
    {
      "epoch": 0.30604093859211184,
      "grad_norm": 0.5863710045814514,
      "learning_rate": 0.0004969395906140789,
      "loss": 0.0112,
      "step": 613
    },
    {
      "epoch": 0.30654018971542685,
      "grad_norm": 0.24207469820976257,
      "learning_rate": 0.0004969345981028458,
      "loss": 0.0084,
      "step": 614
    },
    {
      "epoch": 0.3070394408387419,
      "grad_norm": 0.2187420278787613,
      "learning_rate": 0.0004969296055916126,
      "loss": 0.0249,
      "step": 615
    },
    {
      "epoch": 0.3075386919620569,
      "grad_norm": 0.27375784516334534,
      "learning_rate": 0.0004969246130803795,
      "loss": 0.0142,
      "step": 616
    },
    {
      "epoch": 0.30803794308537197,
      "grad_norm": 0.3294435739517212,
      "learning_rate": 0.0004969196205691463,
      "loss": 0.0169,
      "step": 617
    },
    {
      "epoch": 0.30853719420868697,
      "grad_norm": 0.6963824033737183,
      "learning_rate": 0.0004969146280579132,
      "loss": 0.0584,
      "step": 618
    },
    {
      "epoch": 0.309036445332002,
      "grad_norm": 0.07111494243144989,
      "learning_rate": 0.00049690963554668,
      "loss": 0.0044,
      "step": 619
    },
    {
      "epoch": 0.30953569645531703,
      "grad_norm": 0.30439576506614685,
      "learning_rate": 0.0004969046430354469,
      "loss": 0.0078,
      "step": 620
    },
    {
      "epoch": 0.31003494757863204,
      "grad_norm": 0.11277440935373306,
      "learning_rate": 0.0004968996505242137,
      "loss": 0.0049,
      "step": 621
    },
    {
      "epoch": 0.3105341987019471,
      "grad_norm": 0.2672474682331085,
      "learning_rate": 0.0004968946580129806,
      "loss": 0.0127,
      "step": 622
    },
    {
      "epoch": 0.3110334498252621,
      "grad_norm": 0.5881180763244629,
      "learning_rate": 0.0004968896655017474,
      "loss": 0.0167,
      "step": 623
    },
    {
      "epoch": 0.31153270094857716,
      "grad_norm": 0.319542795419693,
      "learning_rate": 0.0004968846729905143,
      "loss": 0.0085,
      "step": 624
    },
    {
      "epoch": 0.31203195207189216,
      "grad_norm": 0.29770320653915405,
      "learning_rate": 0.0004968796804792811,
      "loss": 0.0207,
      "step": 625
    },
    {
      "epoch": 0.31253120319520716,
      "grad_norm": 0.5108262896537781,
      "learning_rate": 0.0004968746879680479,
      "loss": 0.0169,
      "step": 626
    },
    {
      "epoch": 0.3130304543185222,
      "grad_norm": 0.563126266002655,
      "learning_rate": 0.0004968696954568148,
      "loss": 0.0324,
      "step": 627
    },
    {
      "epoch": 0.3135297054418372,
      "grad_norm": 2.240605592727661,
      "learning_rate": 0.0004968647029455816,
      "loss": 0.0206,
      "step": 628
    },
    {
      "epoch": 0.3140289565651523,
      "grad_norm": 0.7216410040855408,
      "learning_rate": 0.0004968597104343485,
      "loss": 0.0177,
      "step": 629
    },
    {
      "epoch": 0.3145282076884673,
      "grad_norm": 0.5487627983093262,
      "learning_rate": 0.0004968547179231153,
      "loss": 0.0186,
      "step": 630
    },
    {
      "epoch": 0.31502745881178235,
      "grad_norm": 0.2822868525981903,
      "learning_rate": 0.0004968497254118822,
      "loss": 0.0116,
      "step": 631
    },
    {
      "epoch": 0.31552670993509735,
      "grad_norm": 0.1309092938899994,
      "learning_rate": 0.000496844732900649,
      "loss": 0.0062,
      "step": 632
    },
    {
      "epoch": 0.3160259610584124,
      "grad_norm": 0.9093643426895142,
      "learning_rate": 0.0004968397403894159,
      "loss": 0.0094,
      "step": 633
    },
    {
      "epoch": 0.3165252121817274,
      "grad_norm": 0.1669957935810089,
      "learning_rate": 0.0004968347478781827,
      "loss": 0.0076,
      "step": 634
    },
    {
      "epoch": 0.3170244633050424,
      "grad_norm": 0.2131391316652298,
      "learning_rate": 0.0004968297553669496,
      "loss": 0.0089,
      "step": 635
    },
    {
      "epoch": 0.3175237144283575,
      "grad_norm": 0.5635338425636292,
      "learning_rate": 0.0004968247628557164,
      "loss": 0.054,
      "step": 636
    },
    {
      "epoch": 0.3180229655516725,
      "grad_norm": 0.3608749508857727,
      "learning_rate": 0.0004968197703444833,
      "loss": 0.0127,
      "step": 637
    },
    {
      "epoch": 0.31852221667498753,
      "grad_norm": 0.3076210916042328,
      "learning_rate": 0.0004968147778332501,
      "loss": 0.0094,
      "step": 638
    },
    {
      "epoch": 0.31902146779830254,
      "grad_norm": 0.041022889316082,
      "learning_rate": 0.000496809785322017,
      "loss": 0.0034,
      "step": 639
    },
    {
      "epoch": 0.3195207189216176,
      "grad_norm": 0.21101871132850647,
      "learning_rate": 0.0004968047928107838,
      "loss": 0.0062,
      "step": 640
    },
    {
      "epoch": 0.3200199700449326,
      "grad_norm": 0.08817055076360703,
      "learning_rate": 0.0004967998002995507,
      "loss": 0.0045,
      "step": 641
    },
    {
      "epoch": 0.3205192211682476,
      "grad_norm": 0.7820327877998352,
      "learning_rate": 0.0004967948077883175,
      "loss": 0.0278,
      "step": 642
    },
    {
      "epoch": 0.32101847229156266,
      "grad_norm": 1.337487816810608,
      "learning_rate": 0.0004967898152770844,
      "loss": 0.0372,
      "step": 643
    },
    {
      "epoch": 0.32151772341487767,
      "grad_norm": 0.2748870849609375,
      "learning_rate": 0.0004967848227658512,
      "loss": 0.0108,
      "step": 644
    },
    {
      "epoch": 0.3220169745381927,
      "grad_norm": 0.1775524914264679,
      "learning_rate": 0.0004967798302546181,
      "loss": 0.0068,
      "step": 645
    },
    {
      "epoch": 0.3225162256615077,
      "grad_norm": 0.4836961627006531,
      "learning_rate": 0.0004967748377433849,
      "loss": 0.03,
      "step": 646
    },
    {
      "epoch": 0.3230154767848228,
      "grad_norm": 0.6260434985160828,
      "learning_rate": 0.0004967698452321518,
      "loss": 0.0138,
      "step": 647
    },
    {
      "epoch": 0.3235147279081378,
      "grad_norm": 1.9136296510696411,
      "learning_rate": 0.0004967648527209186,
      "loss": 0.0257,
      "step": 648
    },
    {
      "epoch": 0.32401397903145285,
      "grad_norm": 0.1708941012620926,
      "learning_rate": 0.0004967598602096855,
      "loss": 0.0078,
      "step": 649
    },
    {
      "epoch": 0.32451323015476785,
      "grad_norm": 0.6054290533065796,
      "learning_rate": 0.0004967548676984523,
      "loss": 0.0159,
      "step": 650
    },
    {
      "epoch": 0.32501248127808285,
      "grad_norm": 0.2848382592201233,
      "learning_rate": 0.0004967498751872192,
      "loss": 0.0094,
      "step": 651
    },
    {
      "epoch": 0.3255117324013979,
      "grad_norm": 0.19516140222549438,
      "learning_rate": 0.000496744882675986,
      "loss": 0.0058,
      "step": 652
    },
    {
      "epoch": 0.3260109835247129,
      "grad_norm": 0.9435105919837952,
      "learning_rate": 0.000496739890164753,
      "loss": 0.0077,
      "step": 653
    },
    {
      "epoch": 0.326510234648028,
      "grad_norm": 0.25383028388023376,
      "learning_rate": 0.0004967348976535197,
      "loss": 0.0074,
      "step": 654
    },
    {
      "epoch": 0.327009485771343,
      "grad_norm": 0.16129904985427856,
      "learning_rate": 0.0004967299051422865,
      "loss": 0.0065,
      "step": 655
    },
    {
      "epoch": 0.32750873689465804,
      "grad_norm": 0.4053809940814972,
      "learning_rate": 0.0004967249126310534,
      "loss": 0.0144,
      "step": 656
    },
    {
      "epoch": 0.32800798801797304,
      "grad_norm": 1.3843512535095215,
      "learning_rate": 0.0004967199201198202,
      "loss": 0.0673,
      "step": 657
    },
    {
      "epoch": 0.32850723914128804,
      "grad_norm": 0.23300988972187042,
      "learning_rate": 0.0004967149276085872,
      "loss": 0.0066,
      "step": 658
    },
    {
      "epoch": 0.3290064902646031,
      "grad_norm": 0.509119987487793,
      "learning_rate": 0.000496709935097354,
      "loss": 0.0194,
      "step": 659
    },
    {
      "epoch": 0.3295057413879181,
      "grad_norm": 0.21898017823696136,
      "learning_rate": 0.0004967049425861209,
      "loss": 0.0083,
      "step": 660
    },
    {
      "epoch": 0.33000499251123316,
      "grad_norm": 0.29233595728874207,
      "learning_rate": 0.0004966999500748877,
      "loss": 0.0175,
      "step": 661
    },
    {
      "epoch": 0.33050424363454817,
      "grad_norm": 0.3637441396713257,
      "learning_rate": 0.0004966949575636546,
      "loss": 0.0136,
      "step": 662
    },
    {
      "epoch": 0.3310034947578632,
      "grad_norm": 0.32422301173210144,
      "learning_rate": 0.0004966899650524214,
      "loss": 0.0086,
      "step": 663
    },
    {
      "epoch": 0.33150274588117823,
      "grad_norm": 0.519454836845398,
      "learning_rate": 0.0004966849725411883,
      "loss": 0.0222,
      "step": 664
    },
    {
      "epoch": 0.3320019970044933,
      "grad_norm": 0.16910098493099213,
      "learning_rate": 0.0004966799800299551,
      "loss": 0.0062,
      "step": 665
    },
    {
      "epoch": 0.3325012481278083,
      "grad_norm": 0.31148090958595276,
      "learning_rate": 0.000496674987518722,
      "loss": 0.0122,
      "step": 666
    },
    {
      "epoch": 0.3330004992511233,
      "grad_norm": 0.35912030935287476,
      "learning_rate": 0.0004966699950074888,
      "loss": 0.0111,
      "step": 667
    },
    {
      "epoch": 0.33349975037443835,
      "grad_norm": 0.4273650348186493,
      "learning_rate": 0.0004966650024962557,
      "loss": 0.0145,
      "step": 668
    },
    {
      "epoch": 0.33399900149775336,
      "grad_norm": 0.07024715840816498,
      "learning_rate": 0.0004966600099850225,
      "loss": 0.0045,
      "step": 669
    },
    {
      "epoch": 0.3344982526210684,
      "grad_norm": 0.07527684420347214,
      "learning_rate": 0.0004966550174737894,
      "loss": 0.0039,
      "step": 670
    },
    {
      "epoch": 0.3349975037443834,
      "grad_norm": 0.0577298104763031,
      "learning_rate": 0.0004966500249625562,
      "loss": 0.0038,
      "step": 671
    },
    {
      "epoch": 0.3354967548676985,
      "grad_norm": 0.354460209608078,
      "learning_rate": 0.0004966450324513231,
      "loss": 0.038,
      "step": 672
    },
    {
      "epoch": 0.3359960059910135,
      "grad_norm": 0.19746218621730804,
      "learning_rate": 0.0004966400399400899,
      "loss": 0.0035,
      "step": 673
    },
    {
      "epoch": 0.3364952571143285,
      "grad_norm": 0.9524733424186707,
      "learning_rate": 0.0004966350474288568,
      "loss": 0.0258,
      "step": 674
    },
    {
      "epoch": 0.33699450823764354,
      "grad_norm": 0.23693124949932098,
      "learning_rate": 0.0004966300549176236,
      "loss": 0.0086,
      "step": 675
    },
    {
      "epoch": 0.33749375936095855,
      "grad_norm": 0.3031395673751831,
      "learning_rate": 0.0004966250624063905,
      "loss": 0.0151,
      "step": 676
    },
    {
      "epoch": 0.3379930104842736,
      "grad_norm": 0.11877436190843582,
      "learning_rate": 0.0004966200698951573,
      "loss": 0.0064,
      "step": 677
    },
    {
      "epoch": 0.3384922616075886,
      "grad_norm": 0.4760551452636719,
      "learning_rate": 0.0004966150773839242,
      "loss": 0.0356,
      "step": 678
    },
    {
      "epoch": 0.33899151273090367,
      "grad_norm": 0.3738250136375427,
      "learning_rate": 0.000496610084872691,
      "loss": 0.0162,
      "step": 679
    },
    {
      "epoch": 0.33949076385421867,
      "grad_norm": 0.22028391063213348,
      "learning_rate": 0.0004966050923614579,
      "loss": 0.0072,
      "step": 680
    },
    {
      "epoch": 0.33999001497753367,
      "grad_norm": 1.103153109550476,
      "learning_rate": 0.0004966000998502247,
      "loss": 0.0187,
      "step": 681
    },
    {
      "epoch": 0.34048926610084873,
      "grad_norm": 0.10908850282430649,
      "learning_rate": 0.0004965951073389915,
      "loss": 0.0059,
      "step": 682
    },
    {
      "epoch": 0.34098851722416373,
      "grad_norm": 0.5682674050331116,
      "learning_rate": 0.0004965901148277584,
      "loss": 0.0088,
      "step": 683
    },
    {
      "epoch": 0.3414877683474788,
      "grad_norm": 0.17216739058494568,
      "learning_rate": 0.0004965851223165252,
      "loss": 0.0075,
      "step": 684
    },
    {
      "epoch": 0.3419870194707938,
      "grad_norm": 0.7948315143585205,
      "learning_rate": 0.000496580129805292,
      "loss": 0.0359,
      "step": 685
    },
    {
      "epoch": 0.34248627059410885,
      "grad_norm": 0.3577567934989929,
      "learning_rate": 0.0004965751372940589,
      "loss": 0.0571,
      "step": 686
    },
    {
      "epoch": 0.34298552171742386,
      "grad_norm": 0.11012893915176392,
      "learning_rate": 0.0004965701447828257,
      "loss": 0.0049,
      "step": 687
    },
    {
      "epoch": 0.3434847728407389,
      "grad_norm": 0.3062971532344818,
      "learning_rate": 0.0004965651522715926,
      "loss": 0.0067,
      "step": 688
    },
    {
      "epoch": 0.3439840239640539,
      "grad_norm": 0.4315239191055298,
      "learning_rate": 0.0004965601597603594,
      "loss": 0.0141,
      "step": 689
    },
    {
      "epoch": 0.3444832750873689,
      "grad_norm": 0.36216190457344055,
      "learning_rate": 0.0004965551672491263,
      "loss": 0.0216,
      "step": 690
    },
    {
      "epoch": 0.344982526210684,
      "grad_norm": 0.5843075513839722,
      "learning_rate": 0.0004965501747378931,
      "loss": 0.0122,
      "step": 691
    },
    {
      "epoch": 0.345481777333999,
      "grad_norm": 0.1318884789943695,
      "learning_rate": 0.00049654518222666,
      "loss": 0.0072,
      "step": 692
    },
    {
      "epoch": 0.34598102845731404,
      "grad_norm": 0.2683383822441101,
      "learning_rate": 0.0004965401897154268,
      "loss": 0.006,
      "step": 693
    },
    {
      "epoch": 0.34648027958062905,
      "grad_norm": 0.1208217516541481,
      "learning_rate": 0.0004965351972041937,
      "loss": 0.0057,
      "step": 694
    },
    {
      "epoch": 0.3469795307039441,
      "grad_norm": 0.29272595047950745,
      "learning_rate": 0.0004965302046929605,
      "loss": 0.0134,
      "step": 695
    },
    {
      "epoch": 0.3474787818272591,
      "grad_norm": 0.09002172201871872,
      "learning_rate": 0.0004965252121817274,
      "loss": 0.005,
      "step": 696
    },
    {
      "epoch": 0.3479780329505741,
      "grad_norm": 0.040900152176618576,
      "learning_rate": 0.0004965202196704942,
      "loss": 0.0032,
      "step": 697
    },
    {
      "epoch": 0.34847728407388917,
      "grad_norm": 2.345519781112671,
      "learning_rate": 0.0004965152271592611,
      "loss": 0.0206,
      "step": 698
    },
    {
      "epoch": 0.3489765351972042,
      "grad_norm": 0.34685251116752625,
      "learning_rate": 0.0004965102346480279,
      "loss": 0.0179,
      "step": 699
    },
    {
      "epoch": 0.34947578632051923,
      "grad_norm": 0.3357340395450592,
      "learning_rate": 0.0004965052421367948,
      "loss": 0.0308,
      "step": 700
    },
    {
      "epoch": 0.34997503744383424,
      "grad_norm": 0.756810188293457,
      "learning_rate": 0.0004965002496255616,
      "loss": 0.0588,
      "step": 701
    },
    {
      "epoch": 0.3504742885671493,
      "grad_norm": 0.27655431628227234,
      "learning_rate": 0.0004964952571143285,
      "loss": 0.0129,
      "step": 702
    },
    {
      "epoch": 0.3509735396904643,
      "grad_norm": 0.11413546651601791,
      "learning_rate": 0.0004964902646030953,
      "loss": 0.0053,
      "step": 703
    },
    {
      "epoch": 0.35147279081377936,
      "grad_norm": 0.05829457938671112,
      "learning_rate": 0.0004964852720918622,
      "loss": 0.004,
      "step": 704
    },
    {
      "epoch": 0.35197204193709436,
      "grad_norm": 0.37892213463783264,
      "learning_rate": 0.000496480279580629,
      "loss": 0.0168,
      "step": 705
    },
    {
      "epoch": 0.35247129306040936,
      "grad_norm": 0.22565244138240814,
      "learning_rate": 0.0004964752870693959,
      "loss": 0.0118,
      "step": 706
    },
    {
      "epoch": 0.3529705441837244,
      "grad_norm": 0.37110233306884766,
      "learning_rate": 0.0004964702945581627,
      "loss": 0.0118,
      "step": 707
    },
    {
      "epoch": 0.3534697953070394,
      "grad_norm": 0.31190019845962524,
      "learning_rate": 0.0004964653020469296,
      "loss": 0.0087,
      "step": 708
    },
    {
      "epoch": 0.3539690464303545,
      "grad_norm": 0.9735540747642517,
      "learning_rate": 0.0004964603095356964,
      "loss": 0.0463,
      "step": 709
    },
    {
      "epoch": 0.3544682975536695,
      "grad_norm": 0.45507392287254333,
      "learning_rate": 0.0004964553170244634,
      "loss": 0.0284,
      "step": 710
    },
    {
      "epoch": 0.35496754867698455,
      "grad_norm": 0.31248822808265686,
      "learning_rate": 0.0004964503245132301,
      "loss": 0.0124,
      "step": 711
    },
    {
      "epoch": 0.35546679980029955,
      "grad_norm": 0.18829067051410675,
      "learning_rate": 0.0004964453320019971,
      "loss": 0.0055,
      "step": 712
    },
    {
      "epoch": 0.35596605092361455,
      "grad_norm": 0.4367496371269226,
      "learning_rate": 0.0004964403394907639,
      "loss": 0.0183,
      "step": 713
    },
    {
      "epoch": 0.3564653020469296,
      "grad_norm": 1.2888625860214233,
      "learning_rate": 0.0004964353469795307,
      "loss": 0.0087,
      "step": 714
    },
    {
      "epoch": 0.3569645531702446,
      "grad_norm": 0.38344529271125793,
      "learning_rate": 0.0004964303544682976,
      "loss": 0.0135,
      "step": 715
    },
    {
      "epoch": 0.3574638042935597,
      "grad_norm": 0.2828005254268646,
      "learning_rate": 0.0004964253619570644,
      "loss": 0.0174,
      "step": 716
    },
    {
      "epoch": 0.3579630554168747,
      "grad_norm": 0.10218153893947601,
      "learning_rate": 0.0004964203694458313,
      "loss": 0.0055,
      "step": 717
    },
    {
      "epoch": 0.35846230654018973,
      "grad_norm": 0.20752336084842682,
      "learning_rate": 0.0004964153769345981,
      "loss": 0.0055,
      "step": 718
    },
    {
      "epoch": 0.35896155766350474,
      "grad_norm": 0.23894309997558594,
      "learning_rate": 0.000496410384423365,
      "loss": 0.0105,
      "step": 719
    },
    {
      "epoch": 0.3594608087868198,
      "grad_norm": 0.08980882167816162,
      "learning_rate": 0.0004964053919121318,
      "loss": 0.0037,
      "step": 720
    },
    {
      "epoch": 0.3599600599101348,
      "grad_norm": 0.4719776511192322,
      "learning_rate": 0.0004964003994008987,
      "loss": 0.0092,
      "step": 721
    },
    {
      "epoch": 0.3604593110334498,
      "grad_norm": 0.2685638964176178,
      "learning_rate": 0.0004963954068896655,
      "loss": 0.0086,
      "step": 722
    },
    {
      "epoch": 0.36095856215676486,
      "grad_norm": 1.2039154767990112,
      "learning_rate": 0.0004963904143784324,
      "loss": 0.0069,
      "step": 723
    },
    {
      "epoch": 0.36145781328007986,
      "grad_norm": 2.8636608123779297,
      "learning_rate": 0.0004963854218671992,
      "loss": 0.0138,
      "step": 724
    },
    {
      "epoch": 0.3619570644033949,
      "grad_norm": 0.027485881000757217,
      "learning_rate": 0.0004963804293559661,
      "loss": 0.0025,
      "step": 725
    },
    {
      "epoch": 0.3624563155267099,
      "grad_norm": 0.7540563941001892,
      "learning_rate": 0.0004963754368447329,
      "loss": 0.0211,
      "step": 726
    },
    {
      "epoch": 0.362955566650025,
      "grad_norm": 0.14411161839962006,
      "learning_rate": 0.0004963704443334998,
      "loss": 0.0057,
      "step": 727
    },
    {
      "epoch": 0.36345481777334,
      "grad_norm": 0.37858638167381287,
      "learning_rate": 0.0004963654518222666,
      "loss": 0.0122,
      "step": 728
    },
    {
      "epoch": 0.363954068896655,
      "grad_norm": 1.3398387432098389,
      "learning_rate": 0.0004963604593110335,
      "loss": 0.0946,
      "step": 729
    },
    {
      "epoch": 0.36445332001997005,
      "grad_norm": 0.18017426133155823,
      "learning_rate": 0.0004963554667998003,
      "loss": 0.0071,
      "step": 730
    },
    {
      "epoch": 0.36495257114328505,
      "grad_norm": 0.1521914154291153,
      "learning_rate": 0.0004963504742885672,
      "loss": 0.0058,
      "step": 731
    },
    {
      "epoch": 0.3654518222666001,
      "grad_norm": 0.6562700867652893,
      "learning_rate": 0.000496345481777334,
      "loss": 0.0118,
      "step": 732
    },
    {
      "epoch": 0.3659510733899151,
      "grad_norm": 0.19081230461597443,
      "learning_rate": 0.0004963404892661009,
      "loss": 0.0079,
      "step": 733
    },
    {
      "epoch": 0.3664503245132302,
      "grad_norm": 0.09797781705856323,
      "learning_rate": 0.0004963354967548677,
      "loss": 0.0053,
      "step": 734
    },
    {
      "epoch": 0.3669495756365452,
      "grad_norm": 0.7928274869918823,
      "learning_rate": 0.0004963305042436346,
      "loss": 0.0173,
      "step": 735
    },
    {
      "epoch": 0.36744882675986024,
      "grad_norm": 0.16232730448246002,
      "learning_rate": 0.0004963255117324014,
      "loss": 0.0065,
      "step": 736
    },
    {
      "epoch": 0.36794807788317524,
      "grad_norm": 0.4299168288707733,
      "learning_rate": 0.0004963205192211683,
      "loss": 0.011,
      "step": 737
    },
    {
      "epoch": 0.36844732900649024,
      "grad_norm": 0.618812084197998,
      "learning_rate": 0.0004963155267099351,
      "loss": 0.0113,
      "step": 738
    },
    {
      "epoch": 0.3689465801298053,
      "grad_norm": 0.572091817855835,
      "learning_rate": 0.000496310534198702,
      "loss": 0.015,
      "step": 739
    },
    {
      "epoch": 0.3694458312531203,
      "grad_norm": 2.1861989498138428,
      "learning_rate": 0.0004963055416874688,
      "loss": 0.0551,
      "step": 740
    },
    {
      "epoch": 0.36994508237643536,
      "grad_norm": 0.29545384645462036,
      "learning_rate": 0.0004963005491762357,
      "loss": 0.0065,
      "step": 741
    },
    {
      "epoch": 0.37044433349975037,
      "grad_norm": 0.29077693819999695,
      "learning_rate": 0.0004962955566650025,
      "loss": 0.0127,
      "step": 742
    },
    {
      "epoch": 0.3709435846230654,
      "grad_norm": 0.3020886182785034,
      "learning_rate": 0.0004962905641537693,
      "loss": 0.0074,
      "step": 743
    },
    {
      "epoch": 0.37144283574638043,
      "grad_norm": 0.4408015012741089,
      "learning_rate": 0.0004962855716425362,
      "loss": 0.0244,
      "step": 744
    },
    {
      "epoch": 0.37194208686969543,
      "grad_norm": 0.2767491340637207,
      "learning_rate": 0.000496280579131303,
      "loss": 0.0071,
      "step": 745
    },
    {
      "epoch": 0.3724413379930105,
      "grad_norm": 0.18110142648220062,
      "learning_rate": 0.0004962755866200699,
      "loss": 0.009,
      "step": 746
    },
    {
      "epoch": 0.3729405891163255,
      "grad_norm": 0.2575370967388153,
      "learning_rate": 0.0004962705941088367,
      "loss": 0.008,
      "step": 747
    },
    {
      "epoch": 0.37343984023964055,
      "grad_norm": 0.33036553859710693,
      "learning_rate": 0.0004962656015976036,
      "loss": 0.0105,
      "step": 748
    },
    {
      "epoch": 0.37393909136295556,
      "grad_norm": 0.6147710680961609,
      "learning_rate": 0.0004962606090863704,
      "loss": 0.0206,
      "step": 749
    },
    {
      "epoch": 0.3744383424862706,
      "grad_norm": 0.12327113747596741,
      "learning_rate": 0.0004962556165751373,
      "loss": 0.0055,
      "step": 750
    },
    {
      "epoch": 0.3749375936095856,
      "grad_norm": 0.22937636077404022,
      "learning_rate": 0.0004962506240639041,
      "loss": 0.0065,
      "step": 751
    },
    {
      "epoch": 0.3754368447329007,
      "grad_norm": 0.6756322979927063,
      "learning_rate": 0.000496245631552671,
      "loss": 0.0113,
      "step": 752
    },
    {
      "epoch": 0.3759360958562157,
      "grad_norm": 0.13272187113761902,
      "learning_rate": 0.0004962406390414378,
      "loss": 0.0046,
      "step": 753
    },
    {
      "epoch": 0.3764353469795307,
      "grad_norm": 1.4115779399871826,
      "learning_rate": 0.0004962356465302047,
      "loss": 0.0241,
      "step": 754
    },
    {
      "epoch": 0.37693459810284574,
      "grad_norm": 0.1965697854757309,
      "learning_rate": 0.0004962306540189715,
      "loss": 0.0072,
      "step": 755
    },
    {
      "epoch": 0.37743384922616074,
      "grad_norm": 0.05722139775753021,
      "learning_rate": 0.0004962256615077384,
      "loss": 0.0035,
      "step": 756
    },
    {
      "epoch": 0.3779331003494758,
      "grad_norm": 0.21759876608848572,
      "learning_rate": 0.0004962206689965052,
      "loss": 0.0077,
      "step": 757
    },
    {
      "epoch": 0.3784323514727908,
      "grad_norm": 0.32858529686927795,
      "learning_rate": 0.0004962156764852721,
      "loss": 0.0054,
      "step": 758
    },
    {
      "epoch": 0.37893160259610587,
      "grad_norm": 0.3487827479839325,
      "learning_rate": 0.0004962106839740389,
      "loss": 0.0065,
      "step": 759
    },
    {
      "epoch": 0.37943085371942087,
      "grad_norm": 0.5513826608657837,
      "learning_rate": 0.0004962056914628058,
      "loss": 0.0075,
      "step": 760
    },
    {
      "epoch": 0.37993010484273587,
      "grad_norm": 0.28286874294281006,
      "learning_rate": 0.0004962006989515726,
      "loss": 0.0106,
      "step": 761
    },
    {
      "epoch": 0.38042935596605093,
      "grad_norm": 0.5690613985061646,
      "learning_rate": 0.0004961957064403396,
      "loss": 0.0155,
      "step": 762
    },
    {
      "epoch": 0.38092860708936593,
      "grad_norm": 0.7866655588150024,
      "learning_rate": 0.0004961907139291063,
      "loss": 0.011,
      "step": 763
    },
    {
      "epoch": 0.381427858212681,
      "grad_norm": 0.45475247502326965,
      "learning_rate": 0.0004961857214178733,
      "loss": 0.0233,
      "step": 764
    },
    {
      "epoch": 0.381927109335996,
      "grad_norm": 0.22692081332206726,
      "learning_rate": 0.00049618072890664,
      "loss": 0.0038,
      "step": 765
    },
    {
      "epoch": 0.38242636045931105,
      "grad_norm": 0.2280597984790802,
      "learning_rate": 0.000496175736395407,
      "loss": 0.0079,
      "step": 766
    },
    {
      "epoch": 0.38292561158262606,
      "grad_norm": 0.5307400226593018,
      "learning_rate": 0.0004961707438841738,
      "loss": 0.0152,
      "step": 767
    },
    {
      "epoch": 0.38342486270594106,
      "grad_norm": 0.10438655316829681,
      "learning_rate": 0.0004961657513729407,
      "loss": 0.0053,
      "step": 768
    },
    {
      "epoch": 0.3839241138292561,
      "grad_norm": 0.4903002381324768,
      "learning_rate": 0.0004961607588617075,
      "loss": 0.0249,
      "step": 769
    },
    {
      "epoch": 0.3844233649525711,
      "grad_norm": 0.23433589935302734,
      "learning_rate": 0.0004961557663504744,
      "loss": 0.0056,
      "step": 770
    },
    {
      "epoch": 0.3849226160758862,
      "grad_norm": 0.44850364327430725,
      "learning_rate": 0.0004961507738392412,
      "loss": 0.0132,
      "step": 771
    },
    {
      "epoch": 0.3854218671992012,
      "grad_norm": 0.8914335370063782,
      "learning_rate": 0.000496145781328008,
      "loss": 0.0342,
      "step": 772
    },
    {
      "epoch": 0.38592111832251624,
      "grad_norm": 0.9970248341560364,
      "learning_rate": 0.0004961407888167749,
      "loss": 0.0231,
      "step": 773
    },
    {
      "epoch": 0.38642036944583125,
      "grad_norm": 0.1973653882741928,
      "learning_rate": 0.0004961357963055417,
      "loss": 0.0061,
      "step": 774
    },
    {
      "epoch": 0.3869196205691463,
      "grad_norm": 0.41292804479599,
      "learning_rate": 0.0004961308037943086,
      "loss": 0.0145,
      "step": 775
    },
    {
      "epoch": 0.3874188716924613,
      "grad_norm": 0.5813784003257751,
      "learning_rate": 0.0004961258112830754,
      "loss": 0.0207,
      "step": 776
    },
    {
      "epoch": 0.3879181228157763,
      "grad_norm": 0.6135480403900146,
      "learning_rate": 0.0004961208187718423,
      "loss": 0.0169,
      "step": 777
    },
    {
      "epoch": 0.38841737393909137,
      "grad_norm": 0.12745364010334015,
      "learning_rate": 0.0004961158262606091,
      "loss": 0.005,
      "step": 778
    },
    {
      "epoch": 0.3889166250624064,
      "grad_norm": 0.14193955063819885,
      "learning_rate": 0.000496110833749376,
      "loss": 0.0071,
      "step": 779
    },
    {
      "epoch": 0.38941587618572143,
      "grad_norm": 0.2062140703201294,
      "learning_rate": 0.0004961058412381428,
      "loss": 0.0112,
      "step": 780
    },
    {
      "epoch": 0.38991512730903644,
      "grad_norm": 0.26158350706100464,
      "learning_rate": 0.0004961008487269097,
      "loss": 0.0084,
      "step": 781
    },
    {
      "epoch": 0.3904143784323515,
      "grad_norm": 0.3416847884654999,
      "learning_rate": 0.0004960958562156765,
      "loss": 0.0157,
      "step": 782
    },
    {
      "epoch": 0.3909136295556665,
      "grad_norm": 0.45342347025871277,
      "learning_rate": 0.0004960908637044434,
      "loss": 0.0242,
      "step": 783
    },
    {
      "epoch": 0.3914128806789815,
      "grad_norm": 0.2460058182477951,
      "learning_rate": 0.0004960858711932102,
      "loss": 0.0255,
      "step": 784
    },
    {
      "epoch": 0.39191213180229656,
      "grad_norm": 0.410497784614563,
      "learning_rate": 0.0004960808786819771,
      "loss": 0.0114,
      "step": 785
    },
    {
      "epoch": 0.39241138292561156,
      "grad_norm": 0.3671035170555115,
      "learning_rate": 0.0004960758861707439,
      "loss": 0.0176,
      "step": 786
    },
    {
      "epoch": 0.3929106340489266,
      "grad_norm": 0.471082478761673,
      "learning_rate": 0.0004960708936595108,
      "loss": 0.0192,
      "step": 787
    },
    {
      "epoch": 0.3934098851722416,
      "grad_norm": 0.16026081144809723,
      "learning_rate": 0.0004960659011482776,
      "loss": 0.0067,
      "step": 788
    },
    {
      "epoch": 0.3939091362955567,
      "grad_norm": 0.32264506816864014,
      "learning_rate": 0.0004960609086370445,
      "loss": 0.0174,
      "step": 789
    },
    {
      "epoch": 0.3944083874188717,
      "grad_norm": 0.2151179015636444,
      "learning_rate": 0.0004960559161258113,
      "loss": 0.0076,
      "step": 790
    },
    {
      "epoch": 0.39490763854218675,
      "grad_norm": 0.09374000877141953,
      "learning_rate": 0.0004960509236145782,
      "loss": 0.0043,
      "step": 791
    },
    {
      "epoch": 0.39540688966550175,
      "grad_norm": 0.5901176333427429,
      "learning_rate": 0.000496045931103345,
      "loss": 0.0177,
      "step": 792
    },
    {
      "epoch": 0.39590614078881675,
      "grad_norm": 0.26380500197410583,
      "learning_rate": 0.0004960409385921119,
      "loss": 0.0084,
      "step": 793
    },
    {
      "epoch": 0.3964053919121318,
      "grad_norm": 0.1975165158510208,
      "learning_rate": 0.0004960359460808787,
      "loss": 0.0073,
      "step": 794
    },
    {
      "epoch": 0.3969046430354468,
      "grad_norm": 0.33348897099494934,
      "learning_rate": 0.0004960309535696455,
      "loss": 0.0128,
      "step": 795
    },
    {
      "epoch": 0.3974038941587619,
      "grad_norm": 0.33225974440574646,
      "learning_rate": 0.0004960259610584124,
      "loss": 0.0132,
      "step": 796
    },
    {
      "epoch": 0.3979031452820769,
      "grad_norm": 0.7443768978118896,
      "learning_rate": 0.0004960209685471792,
      "loss": 0.0268,
      "step": 797
    },
    {
      "epoch": 0.39840239640539193,
      "grad_norm": 0.28673550486564636,
      "learning_rate": 0.0004960159760359461,
      "loss": 0.009,
      "step": 798
    },
    {
      "epoch": 0.39890164752870694,
      "grad_norm": 0.15711502730846405,
      "learning_rate": 0.0004960109835247129,
      "loss": 0.0063,
      "step": 799
    },
    {
      "epoch": 0.39940089865202194,
      "grad_norm": 0.19504314661026,
      "learning_rate": 0.0004960059910134798,
      "loss": 0.0063,
      "step": 800
    },
    {
      "epoch": 0.399900149775337,
      "grad_norm": 0.21661867201328278,
      "learning_rate": 0.0004960009985022466,
      "loss": 0.0068,
      "step": 801
    },
    {
      "epoch": 0.400399400898652,
      "grad_norm": 0.23862417042255402,
      "learning_rate": 0.0004959960059910134,
      "loss": 0.0081,
      "step": 802
    },
    {
      "epoch": 0.40089865202196706,
      "grad_norm": 0.4522654712200165,
      "learning_rate": 0.0004959910134797803,
      "loss": 0.0114,
      "step": 803
    },
    {
      "epoch": 0.40139790314528206,
      "grad_norm": 0.11945290118455887,
      "learning_rate": 0.0004959860209685471,
      "loss": 0.0061,
      "step": 804
    },
    {
      "epoch": 0.4018971542685971,
      "grad_norm": 0.28101831674575806,
      "learning_rate": 0.000495981028457314,
      "loss": 0.0112,
      "step": 805
    },
    {
      "epoch": 0.4023964053919121,
      "grad_norm": 0.10958057641983032,
      "learning_rate": 0.0004959760359460808,
      "loss": 0.0048,
      "step": 806
    },
    {
      "epoch": 0.4028956565152272,
      "grad_norm": 0.2725202739238739,
      "learning_rate": 0.0004959710434348477,
      "loss": 0.0085,
      "step": 807
    },
    {
      "epoch": 0.4033949076385422,
      "grad_norm": 0.1317337304353714,
      "learning_rate": 0.0004959660509236145,
      "loss": 0.0046,
      "step": 808
    },
    {
      "epoch": 0.4038941587618572,
      "grad_norm": 0.2837214171886444,
      "learning_rate": 0.0004959610584123814,
      "loss": 0.0106,
      "step": 809
    },
    {
      "epoch": 0.40439340988517225,
      "grad_norm": 0.3835282027721405,
      "learning_rate": 0.0004959560659011482,
      "loss": 0.011,
      "step": 810
    },
    {
      "epoch": 0.40489266100848725,
      "grad_norm": 0.1306755542755127,
      "learning_rate": 0.0004959510733899151,
      "loss": 0.0053,
      "step": 811
    },
    {
      "epoch": 0.4053919121318023,
      "grad_norm": 0.17924872040748596,
      "learning_rate": 0.0004959460808786819,
      "loss": 0.0069,
      "step": 812
    },
    {
      "epoch": 0.4058911632551173,
      "grad_norm": 1.6778029203414917,
      "learning_rate": 0.0004959410883674488,
      "loss": 0.0079,
      "step": 813
    },
    {
      "epoch": 0.4063904143784324,
      "grad_norm": 0.29654690623283386,
      "learning_rate": 0.0004959360958562156,
      "loss": 0.0108,
      "step": 814
    },
    {
      "epoch": 0.4068896655017474,
      "grad_norm": 0.17129427194595337,
      "learning_rate": 0.0004959311033449825,
      "loss": 0.0038,
      "step": 815
    },
    {
      "epoch": 0.4073889166250624,
      "grad_norm": 0.18355436623096466,
      "learning_rate": 0.0004959261108337493,
      "loss": 0.0052,
      "step": 816
    },
    {
      "epoch": 0.40788816774837744,
      "grad_norm": 0.36496761441230774,
      "learning_rate": 0.0004959211183225163,
      "loss": 0.0127,
      "step": 817
    },
    {
      "epoch": 0.40838741887169244,
      "grad_norm": 0.05692170187830925,
      "learning_rate": 0.000495916125811283,
      "loss": 0.0029,
      "step": 818
    },
    {
      "epoch": 0.4088866699950075,
      "grad_norm": 0.23735974729061127,
      "learning_rate": 0.00049591113330005,
      "loss": 0.0084,
      "step": 819
    },
    {
      "epoch": 0.4093859211183225,
      "grad_norm": 0.7334282994270325,
      "learning_rate": 0.0004959061407888168,
      "loss": 0.0126,
      "step": 820
    },
    {
      "epoch": 0.40988517224163756,
      "grad_norm": 0.23648181557655334,
      "learning_rate": 0.0004959011482775837,
      "loss": 0.0049,
      "step": 821
    },
    {
      "epoch": 0.41038442336495257,
      "grad_norm": 0.15281011164188385,
      "learning_rate": 0.0004958961557663505,
      "loss": 0.0028,
      "step": 822
    },
    {
      "epoch": 0.4108836744882676,
      "grad_norm": 0.3245511054992676,
      "learning_rate": 0.0004958911632551174,
      "loss": 0.0061,
      "step": 823
    },
    {
      "epoch": 0.41138292561158263,
      "grad_norm": 0.65793377161026,
      "learning_rate": 0.0004958861707438842,
      "loss": 0.0148,
      "step": 824
    },
    {
      "epoch": 0.41188217673489763,
      "grad_norm": 0.17214800417423248,
      "learning_rate": 0.0004958811782326511,
      "loss": 0.0041,
      "step": 825
    },
    {
      "epoch": 0.4123814278582127,
      "grad_norm": 0.1019975021481514,
      "learning_rate": 0.0004958761857214179,
      "loss": 0.0048,
      "step": 826
    },
    {
      "epoch": 0.4128806789815277,
      "grad_norm": 0.14138884842395782,
      "learning_rate": 0.0004958711932101848,
      "loss": 0.0038,
      "step": 827
    },
    {
      "epoch": 0.41337993010484275,
      "grad_norm": 0.0794040635228157,
      "learning_rate": 0.0004958662006989516,
      "loss": 0.0045,
      "step": 828
    },
    {
      "epoch": 0.41387918122815776,
      "grad_norm": 0.7922735214233398,
      "learning_rate": 0.0004958612081877185,
      "loss": 0.0083,
      "step": 829
    },
    {
      "epoch": 0.4143784323514728,
      "grad_norm": 0.058168139308691025,
      "learning_rate": 0.0004958562156764853,
      "loss": 0.0031,
      "step": 830
    },
    {
      "epoch": 0.4148776834747878,
      "grad_norm": 1.3720731735229492,
      "learning_rate": 0.0004958512231652521,
      "loss": 0.0172,
      "step": 831
    },
    {
      "epoch": 0.4153769345981028,
      "grad_norm": 0.1451844871044159,
      "learning_rate": 0.000495846230654019,
      "loss": 0.005,
      "step": 832
    },
    {
      "epoch": 0.4158761857214179,
      "grad_norm": 0.2744368612766266,
      "learning_rate": 0.0004958412381427858,
      "loss": 0.0066,
      "step": 833
    },
    {
      "epoch": 0.4163754368447329,
      "grad_norm": 0.20356036722660065,
      "learning_rate": 0.0004958362456315527,
      "loss": 0.0074,
      "step": 834
    },
    {
      "epoch": 0.41687468796804794,
      "grad_norm": 0.2723202705383301,
      "learning_rate": 0.0004958312531203195,
      "loss": 0.0071,
      "step": 835
    },
    {
      "epoch": 0.41737393909136294,
      "grad_norm": 0.7244524955749512,
      "learning_rate": 0.0004958262606090864,
      "loss": 0.0315,
      "step": 836
    },
    {
      "epoch": 0.417873190214678,
      "grad_norm": 0.050462961196899414,
      "learning_rate": 0.0004958212680978532,
      "loss": 0.0029,
      "step": 837
    },
    {
      "epoch": 0.418372441337993,
      "grad_norm": 0.06332064419984818,
      "learning_rate": 0.0004958162755866201,
      "loss": 0.0033,
      "step": 838
    },
    {
      "epoch": 0.41887169246130806,
      "grad_norm": 0.11191627383232117,
      "learning_rate": 0.0004958112830753869,
      "loss": 0.0036,
      "step": 839
    },
    {
      "epoch": 0.41937094358462307,
      "grad_norm": 0.18573017418384552,
      "learning_rate": 0.0004958062905641538,
      "loss": 0.0063,
      "step": 840
    },
    {
      "epoch": 0.41987019470793807,
      "grad_norm": 1.4144693613052368,
      "learning_rate": 0.0004958012980529206,
      "loss": 0.0157,
      "step": 841
    },
    {
      "epoch": 0.42036944583125313,
      "grad_norm": 1.6781114339828491,
      "learning_rate": 0.0004957963055416875,
      "loss": 0.0495,
      "step": 842
    },
    {
      "epoch": 0.42086869695456813,
      "grad_norm": 0.6332401633262634,
      "learning_rate": 0.0004957913130304543,
      "loss": 0.0192,
      "step": 843
    },
    {
      "epoch": 0.4213679480778832,
      "grad_norm": 0.18381795287132263,
      "learning_rate": 0.0004957863205192212,
      "loss": 0.0053,
      "step": 844
    },
    {
      "epoch": 0.4218671992011982,
      "grad_norm": 0.5011924505233765,
      "learning_rate": 0.000495781328007988,
      "loss": 0.0201,
      "step": 845
    },
    {
      "epoch": 0.42236645032451325,
      "grad_norm": 0.07140413671731949,
      "learning_rate": 0.0004957763354967549,
      "loss": 0.0036,
      "step": 846
    },
    {
      "epoch": 0.42286570144782826,
      "grad_norm": 0.3514493405818939,
      "learning_rate": 0.0004957713429855217,
      "loss": 0.0058,
      "step": 847
    },
    {
      "epoch": 0.42336495257114326,
      "grad_norm": 0.10085566341876984,
      "learning_rate": 0.0004957663504742886,
      "loss": 0.0038,
      "step": 848
    },
    {
      "epoch": 0.4238642036944583,
      "grad_norm": 1.3236857652664185,
      "learning_rate": 0.0004957613579630554,
      "loss": 0.0096,
      "step": 849
    },
    {
      "epoch": 0.4243634548177733,
      "grad_norm": 0.5372418165206909,
      "learning_rate": 0.0004957563654518223,
      "loss": 0.0318,
      "step": 850
    },
    {
      "epoch": 0.4248627059410884,
      "grad_norm": 0.4686240553855896,
      "learning_rate": 0.0004957513729405891,
      "loss": 0.0155,
      "step": 851
    },
    {
      "epoch": 0.4253619570644034,
      "grad_norm": 0.28955522179603577,
      "learning_rate": 0.000495746380429356,
      "loss": 0.018,
      "step": 852
    },
    {
      "epoch": 0.42586120818771844,
      "grad_norm": 0.23662935197353363,
      "learning_rate": 0.0004957413879181228,
      "loss": 0.0071,
      "step": 853
    },
    {
      "epoch": 0.42636045931103345,
      "grad_norm": 0.1121131181716919,
      "learning_rate": 0.0004957363954068897,
      "loss": 0.0046,
      "step": 854
    },
    {
      "epoch": 0.4268597104343485,
      "grad_norm": 0.17252829670906067,
      "learning_rate": 0.0004957314028956565,
      "loss": 0.0046,
      "step": 855
    },
    {
      "epoch": 0.4273589615576635,
      "grad_norm": 0.06899154931306839,
      "learning_rate": 0.0004957264103844234,
      "loss": 0.0034,
      "step": 856
    },
    {
      "epoch": 0.4278582126809785,
      "grad_norm": 1.2784147262573242,
      "learning_rate": 0.0004957214178731902,
      "loss": 0.0375,
      "step": 857
    },
    {
      "epoch": 0.42835746380429357,
      "grad_norm": 0.08640353381633759,
      "learning_rate": 0.0004957164253619571,
      "loss": 0.0037,
      "step": 858
    },
    {
      "epoch": 0.4288567149276086,
      "grad_norm": 0.46560975909233093,
      "learning_rate": 0.0004957114328507239,
      "loss": 0.0128,
      "step": 859
    },
    {
      "epoch": 0.42935596605092363,
      "grad_norm": 0.2603989839553833,
      "learning_rate": 0.0004957064403394907,
      "loss": 0.009,
      "step": 860
    },
    {
      "epoch": 0.42985521717423864,
      "grad_norm": 0.8274389505386353,
      "learning_rate": 0.0004957014478282576,
      "loss": 0.0047,
      "step": 861
    },
    {
      "epoch": 0.4303544682975537,
      "grad_norm": 0.1784648299217224,
      "learning_rate": 0.0004956964553170244,
      "loss": 0.0051,
      "step": 862
    },
    {
      "epoch": 0.4308537194208687,
      "grad_norm": 0.2840350568294525,
      "learning_rate": 0.0004956914628057913,
      "loss": 0.0072,
      "step": 863
    },
    {
      "epoch": 0.4313529705441837,
      "grad_norm": 0.2359761744737625,
      "learning_rate": 0.0004956864702945581,
      "loss": 0.0059,
      "step": 864
    },
    {
      "epoch": 0.43185222166749876,
      "grad_norm": 0.20724895596504211,
      "learning_rate": 0.000495681477783325,
      "loss": 0.0062,
      "step": 865
    },
    {
      "epoch": 0.43235147279081376,
      "grad_norm": 0.2468184381723404,
      "learning_rate": 0.0004956764852720918,
      "loss": 0.0067,
      "step": 866
    },
    {
      "epoch": 0.4328507239141288,
      "grad_norm": 0.31854280829429626,
      "learning_rate": 0.0004956714927608587,
      "loss": 0.0111,
      "step": 867
    },
    {
      "epoch": 0.4333499750374438,
      "grad_norm": 0.29798972606658936,
      "learning_rate": 0.0004956665002496255,
      "loss": 0.0102,
      "step": 868
    },
    {
      "epoch": 0.4338492261607589,
      "grad_norm": 0.08668296784162521,
      "learning_rate": 0.0004956615077383925,
      "loss": 0.0036,
      "step": 869
    },
    {
      "epoch": 0.4343484772840739,
      "grad_norm": 1.1638368368148804,
      "learning_rate": 0.0004956565152271592,
      "loss": 0.0226,
      "step": 870
    },
    {
      "epoch": 0.4348477284073889,
      "grad_norm": 0.08136673271656036,
      "learning_rate": 0.0004956515227159262,
      "loss": 0.0034,
      "step": 871
    },
    {
      "epoch": 0.43534697953070395,
      "grad_norm": 0.11562783271074295,
      "learning_rate": 0.000495646530204693,
      "loss": 0.004,
      "step": 872
    },
    {
      "epoch": 0.43584623065401895,
      "grad_norm": 0.22553008794784546,
      "learning_rate": 0.0004956415376934599,
      "loss": 0.0084,
      "step": 873
    },
    {
      "epoch": 0.436345481777334,
      "grad_norm": 0.26829561591148376,
      "learning_rate": 0.0004956365451822267,
      "loss": 0.0101,
      "step": 874
    },
    {
      "epoch": 0.436844732900649,
      "grad_norm": 0.3617970943450928,
      "learning_rate": 0.0004956315526709936,
      "loss": 0.0212,
      "step": 875
    },
    {
      "epoch": 0.43734398402396407,
      "grad_norm": 2.7246177196502686,
      "learning_rate": 0.0004956265601597604,
      "loss": 0.0127,
      "step": 876
    },
    {
      "epoch": 0.4378432351472791,
      "grad_norm": 0.632818341255188,
      "learning_rate": 0.0004956215676485273,
      "loss": 0.0122,
      "step": 877
    },
    {
      "epoch": 0.43834248627059413,
      "grad_norm": 0.20545998215675354,
      "learning_rate": 0.0004956165751372941,
      "loss": 0.0093,
      "step": 878
    },
    {
      "epoch": 0.43884173739390914,
      "grad_norm": 0.03984804078936577,
      "learning_rate": 0.000495611582626061,
      "loss": 0.0027,
      "step": 879
    },
    {
      "epoch": 0.43934098851722414,
      "grad_norm": 0.32053276896476746,
      "learning_rate": 0.0004956065901148278,
      "loss": 0.0076,
      "step": 880
    },
    {
      "epoch": 0.4398402396405392,
      "grad_norm": 0.32769250869750977,
      "learning_rate": 0.0004956015976035947,
      "loss": 0.0133,
      "step": 881
    },
    {
      "epoch": 0.4403394907638542,
      "grad_norm": 0.12414111196994781,
      "learning_rate": 0.0004955966050923615,
      "loss": 0.0038,
      "step": 882
    },
    {
      "epoch": 0.44083874188716926,
      "grad_norm": 0.08724213391542435,
      "learning_rate": 0.0004955916125811284,
      "loss": 0.0029,
      "step": 883
    },
    {
      "epoch": 0.44133799301048426,
      "grad_norm": 1.132824182510376,
      "learning_rate": 0.0004955866200698952,
      "loss": 0.0122,
      "step": 884
    },
    {
      "epoch": 0.4418372441337993,
      "grad_norm": 0.1864863932132721,
      "learning_rate": 0.0004955816275586621,
      "loss": 0.005,
      "step": 885
    },
    {
      "epoch": 0.4423364952571143,
      "grad_norm": 0.3217424750328064,
      "learning_rate": 0.0004955766350474289,
      "loss": 0.0098,
      "step": 886
    },
    {
      "epoch": 0.44283574638042933,
      "grad_norm": 0.26953548192977905,
      "learning_rate": 0.0004955716425361958,
      "loss": 0.0048,
      "step": 887
    },
    {
      "epoch": 0.4433349975037444,
      "grad_norm": 0.2352495938539505,
      "learning_rate": 0.0004955666500249626,
      "loss": 0.0092,
      "step": 888
    },
    {
      "epoch": 0.4438342486270594,
      "grad_norm": 0.3330988585948944,
      "learning_rate": 0.0004955616575137294,
      "loss": 0.0155,
      "step": 889
    },
    {
      "epoch": 0.44433349975037445,
      "grad_norm": 1.6478444337844849,
      "learning_rate": 0.0004955566650024963,
      "loss": 0.011,
      "step": 890
    },
    {
      "epoch": 0.44483275087368945,
      "grad_norm": 0.11299854516983032,
      "learning_rate": 0.0004955516724912631,
      "loss": 0.0033,
      "step": 891
    },
    {
      "epoch": 0.4453320019970045,
      "grad_norm": 0.1401742398738861,
      "learning_rate": 0.00049554667998003,
      "loss": 0.0049,
      "step": 892
    },
    {
      "epoch": 0.4458312531203195,
      "grad_norm": 0.13070647418498993,
      "learning_rate": 0.0004955416874687968,
      "loss": 0.0041,
      "step": 893
    },
    {
      "epoch": 0.4463305042436346,
      "grad_norm": 0.16524024307727814,
      "learning_rate": 0.0004955366949575637,
      "loss": 0.0067,
      "step": 894
    },
    {
      "epoch": 0.4468297553669496,
      "grad_norm": 0.17149940133094788,
      "learning_rate": 0.0004955317024463305,
      "loss": 0.0053,
      "step": 895
    },
    {
      "epoch": 0.4473290064902646,
      "grad_norm": 0.5498072504997253,
      "learning_rate": 0.0004955267099350974,
      "loss": 0.0103,
      "step": 896
    },
    {
      "epoch": 0.44782825761357964,
      "grad_norm": 0.39593541622161865,
      "learning_rate": 0.0004955217174238642,
      "loss": 0.0088,
      "step": 897
    },
    {
      "epoch": 0.44832750873689464,
      "grad_norm": 0.37589409947395325,
      "learning_rate": 0.0004955167249126311,
      "loss": 0.0101,
      "step": 898
    },
    {
      "epoch": 0.4488267598602097,
      "grad_norm": 0.21771109104156494,
      "learning_rate": 0.0004955117324013979,
      "loss": 0.0063,
      "step": 899
    },
    {
      "epoch": 0.4493260109835247,
      "grad_norm": 0.9225530028343201,
      "learning_rate": 0.0004955067398901648,
      "loss": 0.0261,
      "step": 900
    },
    {
      "epoch": 0.44982526210683976,
      "grad_norm": 0.056142471730709076,
      "learning_rate": 0.0004955017473789316,
      "loss": 0.0025,
      "step": 901
    },
    {
      "epoch": 0.45032451323015477,
      "grad_norm": 0.4873965382575989,
      "learning_rate": 0.0004954967548676985,
      "loss": 0.0201,
      "step": 902
    },
    {
      "epoch": 0.45082376435346977,
      "grad_norm": 0.26013287901878357,
      "learning_rate": 0.0004954917623564653,
      "loss": 0.007,
      "step": 903
    },
    {
      "epoch": 0.45132301547678483,
      "grad_norm": 0.06341944634914398,
      "learning_rate": 0.0004954867698452322,
      "loss": 0.0031,
      "step": 904
    },
    {
      "epoch": 0.45182226660009983,
      "grad_norm": 0.3680298626422882,
      "learning_rate": 0.000495481777333999,
      "loss": 0.0099,
      "step": 905
    },
    {
      "epoch": 0.4523215177234149,
      "grad_norm": 0.6615604758262634,
      "learning_rate": 0.0004954767848227659,
      "loss": 0.0201,
      "step": 906
    },
    {
      "epoch": 0.4528207688467299,
      "grad_norm": 0.6397991180419922,
      "learning_rate": 0.0004954717923115327,
      "loss": 0.0666,
      "step": 907
    },
    {
      "epoch": 0.45332001997004495,
      "grad_norm": 0.11985411494970322,
      "learning_rate": 0.0004954667998002995,
      "loss": 0.0061,
      "step": 908
    },
    {
      "epoch": 0.45381927109335995,
      "grad_norm": 0.6232055425643921,
      "learning_rate": 0.0004954618072890664,
      "loss": 0.0117,
      "step": 909
    },
    {
      "epoch": 0.454318522216675,
      "grad_norm": 1.1975709199905396,
      "learning_rate": 0.0004954568147778332,
      "loss": 0.0108,
      "step": 910
    },
    {
      "epoch": 0.45481777333999,
      "grad_norm": 0.21864697337150574,
      "learning_rate": 0.0004954518222666001,
      "loss": 0.0044,
      "step": 911
    },
    {
      "epoch": 0.455317024463305,
      "grad_norm": 0.21792596578598022,
      "learning_rate": 0.0004954468297553669,
      "loss": 0.0058,
      "step": 912
    },
    {
      "epoch": 0.4558162755866201,
      "grad_norm": 0.4068431258201599,
      "learning_rate": 0.0004954418372441338,
      "loss": 0.0125,
      "step": 913
    },
    {
      "epoch": 0.4563155267099351,
      "grad_norm": 0.15592046082019806,
      "learning_rate": 0.0004954368447329006,
      "loss": 0.0034,
      "step": 914
    },
    {
      "epoch": 0.45681477783325014,
      "grad_norm": 1.0485737323760986,
      "learning_rate": 0.0004954318522216675,
      "loss": 0.0112,
      "step": 915
    },
    {
      "epoch": 0.45731402895656514,
      "grad_norm": 0.24931418895721436,
      "learning_rate": 0.0004954268597104343,
      "loss": 0.0055,
      "step": 916
    },
    {
      "epoch": 0.4578132800798802,
      "grad_norm": 0.07515942305326462,
      "learning_rate": 0.0004954218671992011,
      "loss": 0.0032,
      "step": 917
    },
    {
      "epoch": 0.4583125312031952,
      "grad_norm": 0.25799646973609924,
      "learning_rate": 0.000495416874687968,
      "loss": 0.0065,
      "step": 918
    },
    {
      "epoch": 0.4588117823265102,
      "grad_norm": 0.19664300978183746,
      "learning_rate": 0.0004954118821767348,
      "loss": 0.0025,
      "step": 919
    },
    {
      "epoch": 0.45931103344982527,
      "grad_norm": 1.3281997442245483,
      "learning_rate": 0.0004954068896655017,
      "loss": 0.0094,
      "step": 920
    },
    {
      "epoch": 0.45981028457314027,
      "grad_norm": 0.054312366992235184,
      "learning_rate": 0.0004954018971542685,
      "loss": 0.0032,
      "step": 921
    },
    {
      "epoch": 0.46030953569645533,
      "grad_norm": 0.2676018178462982,
      "learning_rate": 0.0004953969046430354,
      "loss": 0.0079,
      "step": 922
    },
    {
      "epoch": 0.46080878681977033,
      "grad_norm": 0.6996952891349792,
      "learning_rate": 0.0004953919121318022,
      "loss": 0.0353,
      "step": 923
    },
    {
      "epoch": 0.4613080379430854,
      "grad_norm": 0.7345839738845825,
      "learning_rate": 0.0004953869196205692,
      "loss": 0.0122,
      "step": 924
    },
    {
      "epoch": 0.4618072890664004,
      "grad_norm": 0.07908018678426743,
      "learning_rate": 0.000495381927109336,
      "loss": 0.0032,
      "step": 925
    },
    {
      "epoch": 0.46230654018971545,
      "grad_norm": 1.0084617137908936,
      "learning_rate": 0.0004953769345981029,
      "loss": 0.0247,
      "step": 926
    },
    {
      "epoch": 0.46280579131303046,
      "grad_norm": 0.7252710461616516,
      "learning_rate": 0.0004953719420868697,
      "loss": 0.036,
      "step": 927
    },
    {
      "epoch": 0.46330504243634546,
      "grad_norm": 0.39797911047935486,
      "learning_rate": 0.0004953669495756366,
      "loss": 0.0158,
      "step": 928
    },
    {
      "epoch": 0.4638042935596605,
      "grad_norm": 0.7271355986595154,
      "learning_rate": 0.0004953619570644034,
      "loss": 0.0218,
      "step": 929
    },
    {
      "epoch": 0.4643035446829755,
      "grad_norm": 0.1460568606853485,
      "learning_rate": 0.0004953569645531703,
      "loss": 0.0055,
      "step": 930
    },
    {
      "epoch": 0.4648027958062906,
      "grad_norm": 1.1963512897491455,
      "learning_rate": 0.0004953519720419371,
      "loss": 0.0394,
      "step": 931
    },
    {
      "epoch": 0.4653020469296056,
      "grad_norm": 0.14294351637363434,
      "learning_rate": 0.000495346979530704,
      "loss": 0.0051,
      "step": 932
    },
    {
      "epoch": 0.46580129805292064,
      "grad_norm": 0.13840624690055847,
      "learning_rate": 0.0004953419870194708,
      "loss": 0.0061,
      "step": 933
    },
    {
      "epoch": 0.46630054917623565,
      "grad_norm": 0.16813649237155914,
      "learning_rate": 0.0004953369945082377,
      "loss": 0.0062,
      "step": 934
    },
    {
      "epoch": 0.46679980029955065,
      "grad_norm": 0.35280707478523254,
      "learning_rate": 0.0004953320019970045,
      "loss": 0.0118,
      "step": 935
    },
    {
      "epoch": 0.4672990514228657,
      "grad_norm": 0.29063600301742554,
      "learning_rate": 0.0004953270094857714,
      "loss": 0.0117,
      "step": 936
    },
    {
      "epoch": 0.4677983025461807,
      "grad_norm": 0.05645306035876274,
      "learning_rate": 0.0004953220169745382,
      "loss": 0.0034,
      "step": 937
    },
    {
      "epoch": 0.46829755366949577,
      "grad_norm": 0.3442169725894928,
      "learning_rate": 0.0004953170244633051,
      "loss": 0.0099,
      "step": 938
    },
    {
      "epoch": 0.4687968047928108,
      "grad_norm": 0.6254501342773438,
      "learning_rate": 0.0004953120319520719,
      "loss": 0.0082,
      "step": 939
    },
    {
      "epoch": 0.46929605591612583,
      "grad_norm": 0.3013612926006317,
      "learning_rate": 0.0004953070394408388,
      "loss": 0.0092,
      "step": 940
    },
    {
      "epoch": 0.46979530703944083,
      "grad_norm": 0.2674052119255066,
      "learning_rate": 0.0004953020469296056,
      "loss": 0.0097,
      "step": 941
    },
    {
      "epoch": 0.4702945581627559,
      "grad_norm": 0.7325159311294556,
      "learning_rate": 0.0004952970544183725,
      "loss": 0.0071,
      "step": 942
    },
    {
      "epoch": 0.4707938092860709,
      "grad_norm": 1.171798825263977,
      "learning_rate": 0.0004952920619071393,
      "loss": 0.0159,
      "step": 943
    },
    {
      "epoch": 0.4712930604093859,
      "grad_norm": 0.11050018668174744,
      "learning_rate": 0.0004952870693959062,
      "loss": 0.0044,
      "step": 944
    },
    {
      "epoch": 0.47179231153270096,
      "grad_norm": 0.0895351693034172,
      "learning_rate": 0.000495282076884673,
      "loss": 0.0036,
      "step": 945
    },
    {
      "epoch": 0.47229156265601596,
      "grad_norm": 0.7598063945770264,
      "learning_rate": 0.0004952770843734398,
      "loss": 0.0135,
      "step": 946
    },
    {
      "epoch": 0.472790813779331,
      "grad_norm": 0.07207654416561127,
      "learning_rate": 0.0004952720918622067,
      "loss": 0.0042,
      "step": 947
    },
    {
      "epoch": 0.473290064902646,
      "grad_norm": 0.6024289727210999,
      "learning_rate": 0.0004952670993509735,
      "loss": 0.0218,
      "step": 948
    },
    {
      "epoch": 0.4737893160259611,
      "grad_norm": 0.31298208236694336,
      "learning_rate": 0.0004952621068397404,
      "loss": 0.0105,
      "step": 949
    },
    {
      "epoch": 0.4742885671492761,
      "grad_norm": 0.6003750562667847,
      "learning_rate": 0.0004952571143285072,
      "loss": 0.0101,
      "step": 950
    },
    {
      "epoch": 0.4747878182725911,
      "grad_norm": 0.8141809701919556,
      "learning_rate": 0.0004952521218172741,
      "loss": 0.0138,
      "step": 951
    },
    {
      "epoch": 0.47528706939590615,
      "grad_norm": 0.5596655607223511,
      "learning_rate": 0.0004952471293060409,
      "loss": 0.0097,
      "step": 952
    },
    {
      "epoch": 0.47578632051922115,
      "grad_norm": 0.2750963270664215,
      "learning_rate": 0.0004952421367948078,
      "loss": 0.0071,
      "step": 953
    },
    {
      "epoch": 0.4762855716425362,
      "grad_norm": 0.1269347220659256,
      "learning_rate": 0.0004952371442835746,
      "loss": 0.0052,
      "step": 954
    },
    {
      "epoch": 0.4767848227658512,
      "grad_norm": 1.7652286291122437,
      "learning_rate": 0.0004952321517723415,
      "loss": 0.0343,
      "step": 955
    },
    {
      "epoch": 0.47728407388916627,
      "grad_norm": 0.45039957761764526,
      "learning_rate": 0.0004952271592611083,
      "loss": 0.0161,
      "step": 956
    },
    {
      "epoch": 0.4777833250124813,
      "grad_norm": 0.27571144700050354,
      "learning_rate": 0.0004952221667498752,
      "loss": 0.0069,
      "step": 957
    },
    {
      "epoch": 0.4782825761357963,
      "grad_norm": 0.33669313788414,
      "learning_rate": 0.000495217174238642,
      "loss": 0.023,
      "step": 958
    },
    {
      "epoch": 0.47878182725911134,
      "grad_norm": 0.7662159204483032,
      "learning_rate": 0.0004952121817274089,
      "loss": 0.0413,
      "step": 959
    },
    {
      "epoch": 0.47928107838242634,
      "grad_norm": 0.41089972853660583,
      "learning_rate": 0.0004952071892161757,
      "loss": 0.0107,
      "step": 960
    },
    {
      "epoch": 0.4797803295057414,
      "grad_norm": 0.5496443510055542,
      "learning_rate": 0.0004952021967049426,
      "loss": 0.0134,
      "step": 961
    },
    {
      "epoch": 0.4802795806290564,
      "grad_norm": 1.0403494834899902,
      "learning_rate": 0.0004951972041937094,
      "loss": 0.0159,
      "step": 962
    },
    {
      "epoch": 0.48077883175237146,
      "grad_norm": 0.2667757272720337,
      "learning_rate": 0.0004951922116824763,
      "loss": 0.0057,
      "step": 963
    },
    {
      "epoch": 0.48127808287568646,
      "grad_norm": 0.5457704663276672,
      "learning_rate": 0.0004951872191712431,
      "loss": 0.0136,
      "step": 964
    },
    {
      "epoch": 0.4817773339990015,
      "grad_norm": 0.14867070317268372,
      "learning_rate": 0.00049518222666001,
      "loss": 0.0048,
      "step": 965
    },
    {
      "epoch": 0.4822765851223165,
      "grad_norm": 1.211013674736023,
      "learning_rate": 0.0004951772341487768,
      "loss": 0.0168,
      "step": 966
    },
    {
      "epoch": 0.48277583624563153,
      "grad_norm": 0.038496583700180054,
      "learning_rate": 0.0004951722416375437,
      "loss": 0.0026,
      "step": 967
    },
    {
      "epoch": 0.4832750873689466,
      "grad_norm": 0.32611003518104553,
      "learning_rate": 0.0004951672491263105,
      "loss": 0.0093,
      "step": 968
    },
    {
      "epoch": 0.4837743384922616,
      "grad_norm": 0.5694667100906372,
      "learning_rate": 0.0004951622566150774,
      "loss": 0.0203,
      "step": 969
    },
    {
      "epoch": 0.48427358961557665,
      "grad_norm": 0.4762550890445709,
      "learning_rate": 0.0004951572641038442,
      "loss": 0.0177,
      "step": 970
    },
    {
      "epoch": 0.48477284073889165,
      "grad_norm": 0.28042110800743103,
      "learning_rate": 0.0004951522715926111,
      "loss": 0.0074,
      "step": 971
    },
    {
      "epoch": 0.4852720918622067,
      "grad_norm": 0.20843614637851715,
      "learning_rate": 0.0004951472790813779,
      "loss": 0.0044,
      "step": 972
    },
    {
      "epoch": 0.4857713429855217,
      "grad_norm": 0.3743983507156372,
      "learning_rate": 0.0004951422865701448,
      "loss": 0.0157,
      "step": 973
    },
    {
      "epoch": 0.4862705941088367,
      "grad_norm": 0.5104740858078003,
      "learning_rate": 0.0004951372940589116,
      "loss": 0.0229,
      "step": 974
    },
    {
      "epoch": 0.4867698452321518,
      "grad_norm": 0.8361250758171082,
      "learning_rate": 0.0004951323015476784,
      "loss": 0.0321,
      "step": 975
    },
    {
      "epoch": 0.4872690963554668,
      "grad_norm": 0.678389310836792,
      "learning_rate": 0.0004951273090364454,
      "loss": 0.0117,
      "step": 976
    },
    {
      "epoch": 0.48776834747878184,
      "grad_norm": 2.6175084114074707,
      "learning_rate": 0.0004951223165252121,
      "loss": 0.0343,
      "step": 977
    },
    {
      "epoch": 0.48826759860209684,
      "grad_norm": 0.11316490173339844,
      "learning_rate": 0.0004951173240139791,
      "loss": 0.0041,
      "step": 978
    },
    {
      "epoch": 0.4887668497254119,
      "grad_norm": 0.1830073446035385,
      "learning_rate": 0.0004951123315027459,
      "loss": 0.0053,
      "step": 979
    },
    {
      "epoch": 0.4892661008487269,
      "grad_norm": 0.10169106721878052,
      "learning_rate": 0.0004951073389915128,
      "loss": 0.0038,
      "step": 980
    },
    {
      "epoch": 0.48976535197204196,
      "grad_norm": 1.4505255222320557,
      "learning_rate": 0.0004951023464802796,
      "loss": 0.0135,
      "step": 981
    },
    {
      "epoch": 0.49026460309535697,
      "grad_norm": 1.2792314291000366,
      "learning_rate": 0.0004950973539690465,
      "loss": 0.0146,
      "step": 982
    },
    {
      "epoch": 0.49076385421867197,
      "grad_norm": 0.23833152651786804,
      "learning_rate": 0.0004950923614578133,
      "loss": 0.008,
      "step": 983
    },
    {
      "epoch": 0.491263105341987,
      "grad_norm": 0.7269476652145386,
      "learning_rate": 0.0004950873689465802,
      "loss": 0.0173,
      "step": 984
    },
    {
      "epoch": 0.49176235646530203,
      "grad_norm": 0.2652823328971863,
      "learning_rate": 0.000495082376435347,
      "loss": 0.0075,
      "step": 985
    },
    {
      "epoch": 0.4922616075886171,
      "grad_norm": 0.6176941990852356,
      "learning_rate": 0.0004950773839241139,
      "loss": 0.0293,
      "step": 986
    },
    {
      "epoch": 0.4927608587119321,
      "grad_norm": 0.13311678171157837,
      "learning_rate": 0.0004950723914128807,
      "loss": 0.0043,
      "step": 987
    },
    {
      "epoch": 0.49326010983524715,
      "grad_norm": 0.2765597403049469,
      "learning_rate": 0.0004950673989016476,
      "loss": 0.0112,
      "step": 988
    },
    {
      "epoch": 0.49375936095856215,
      "grad_norm": 0.21721450984477997,
      "learning_rate": 0.0004950624063904144,
      "loss": 0.007,
      "step": 989
    },
    {
      "epoch": 0.49425861208187716,
      "grad_norm": 0.9857739210128784,
      "learning_rate": 0.0004950574138791813,
      "loss": 0.023,
      "step": 990
    },
    {
      "epoch": 0.4947578632051922,
      "grad_norm": 0.4077136516571045,
      "learning_rate": 0.0004950524213679481,
      "loss": 0.0113,
      "step": 991
    },
    {
      "epoch": 0.4952571143285072,
      "grad_norm": 0.46235448122024536,
      "learning_rate": 0.000495047428856715,
      "loss": 0.0058,
      "step": 992
    },
    {
      "epoch": 0.4957563654518223,
      "grad_norm": 0.29511353373527527,
      "learning_rate": 0.0004950424363454818,
      "loss": 0.007,
      "step": 993
    },
    {
      "epoch": 0.4962556165751373,
      "grad_norm": 0.19004030525684357,
      "learning_rate": 0.0004950374438342487,
      "loss": 0.0057,
      "step": 994
    },
    {
      "epoch": 0.49675486769845234,
      "grad_norm": 0.150530144572258,
      "learning_rate": 0.0004950324513230155,
      "loss": 0.0068,
      "step": 995
    },
    {
      "epoch": 0.49725411882176734,
      "grad_norm": 0.18105050921440125,
      "learning_rate": 0.0004950274588117824,
      "loss": 0.0066,
      "step": 996
    },
    {
      "epoch": 0.4977533699450824,
      "grad_norm": 0.24580100178718567,
      "learning_rate": 0.0004950224663005492,
      "loss": 0.0068,
      "step": 997
    },
    {
      "epoch": 0.4982526210683974,
      "grad_norm": 0.1504054069519043,
      "learning_rate": 0.0004950174737893161,
      "loss": 0.0051,
      "step": 998
    },
    {
      "epoch": 0.4987518721917124,
      "grad_norm": 0.26283735036849976,
      "learning_rate": 0.0004950124812780829,
      "loss": 0.0097,
      "step": 999
    },
    {
      "epoch": 0.49925112331502747,
      "grad_norm": 0.4972989857196808,
      "learning_rate": 0.0004950074887668498,
      "loss": 0.0136,
      "step": 1000
    },
    {
      "epoch": 0.49975037443834247,
      "grad_norm": 0.3521086871623993,
      "learning_rate": 0.0004950024962556166,
      "loss": 0.0081,
      "step": 1001
    },
    {
      "epoch": 0.5002496255616575,
      "grad_norm": 0.3475799262523651,
      "learning_rate": 0.0004949975037443835,
      "loss": 0.0391,
      "step": 1002
    },
    {
      "epoch": 0.5007488766849726,
      "grad_norm": 0.8771914839744568,
      "learning_rate": 0.0004949925112331503,
      "loss": 0.0472,
      "step": 1003
    },
    {
      "epoch": 0.5012481278082875,
      "grad_norm": 1.4814960956573486,
      "learning_rate": 0.0004949875187219171,
      "loss": 0.0427,
      "step": 1004
    },
    {
      "epoch": 0.5017473789316026,
      "grad_norm": 0.22550144791603088,
      "learning_rate": 0.000494982526210684,
      "loss": 0.0118,
      "step": 1005
    },
    {
      "epoch": 0.5022466300549177,
      "grad_norm": 0.6354542374610901,
      "learning_rate": 0.0004949775336994508,
      "loss": 0.0093,
      "step": 1006
    },
    {
      "epoch": 0.5027458811782326,
      "grad_norm": 0.5828709006309509,
      "learning_rate": 0.0004949725411882177,
      "loss": 0.026,
      "step": 1007
    },
    {
      "epoch": 0.5032451323015477,
      "grad_norm": 1.2101048231124878,
      "learning_rate": 0.0004949675486769845,
      "loss": 0.033,
      "step": 1008
    },
    {
      "epoch": 0.5037443834248627,
      "grad_norm": 0.4810703992843628,
      "learning_rate": 0.0004949625561657514,
      "loss": 0.0191,
      "step": 1009
    },
    {
      "epoch": 0.5042436345481778,
      "grad_norm": 0.10346242785453796,
      "learning_rate": 0.0004949575636545182,
      "loss": 0.0028,
      "step": 1010
    },
    {
      "epoch": 0.5047428856714927,
      "grad_norm": 0.6526390314102173,
      "learning_rate": 0.0004949525711432851,
      "loss": 0.0067,
      "step": 1011
    },
    {
      "epoch": 0.5052421367948078,
      "grad_norm": 0.5062901377677917,
      "learning_rate": 0.0004949475786320519,
      "loss": 0.0401,
      "step": 1012
    },
    {
      "epoch": 0.5057413879181228,
      "grad_norm": 0.28747379779815674,
      "learning_rate": 0.0004949425861208188,
      "loss": 0.0062,
      "step": 1013
    },
    {
      "epoch": 0.5062406390414378,
      "grad_norm": 0.22561588883399963,
      "learning_rate": 0.0004949375936095856,
      "loss": 0.008,
      "step": 1014
    },
    {
      "epoch": 0.5067398901647528,
      "grad_norm": 0.32513663172721863,
      "learning_rate": 0.0004949326010983525,
      "loss": 0.0113,
      "step": 1015
    },
    {
      "epoch": 0.5072391412880679,
      "grad_norm": 0.6719944477081299,
      "learning_rate": 0.0004949276085871193,
      "loss": 0.0129,
      "step": 1016
    },
    {
      "epoch": 0.507738392411383,
      "grad_norm": 0.6022768616676331,
      "learning_rate": 0.0004949226160758862,
      "loss": 0.0172,
      "step": 1017
    },
    {
      "epoch": 0.5082376435346979,
      "grad_norm": 0.23599125444889069,
      "learning_rate": 0.000494917623564653,
      "loss": 0.0057,
      "step": 1018
    },
    {
      "epoch": 0.508736894658013,
      "grad_norm": 0.24745729565620422,
      "learning_rate": 0.0004949126310534199,
      "loss": 0.0055,
      "step": 1019
    },
    {
      "epoch": 0.509236145781328,
      "grad_norm": 0.5378255248069763,
      "learning_rate": 0.0004949076385421867,
      "loss": 0.0524,
      "step": 1020
    },
    {
      "epoch": 0.5097353969046431,
      "grad_norm": 0.6641577482223511,
      "learning_rate": 0.0004949026460309535,
      "loss": 0.0219,
      "step": 1021
    },
    {
      "epoch": 0.510234648027958,
      "grad_norm": 0.08835101872682571,
      "learning_rate": 0.0004948976535197204,
      "loss": 0.0029,
      "step": 1022
    },
    {
      "epoch": 0.5107338991512731,
      "grad_norm": 1.1092793941497803,
      "learning_rate": 0.0004948926610084872,
      "loss": 0.0199,
      "step": 1023
    },
    {
      "epoch": 0.5112331502745882,
      "grad_norm": 0.9522692561149597,
      "learning_rate": 0.0004948876684972541,
      "loss": 0.0216,
      "step": 1024
    },
    {
      "epoch": 0.5117324013979031,
      "grad_norm": 0.3415576219558716,
      "learning_rate": 0.0004948826759860209,
      "loss": 0.0194,
      "step": 1025
    },
    {
      "epoch": 0.5122316525212182,
      "grad_norm": 0.5550140142440796,
      "learning_rate": 0.0004948776834747878,
      "loss": 0.0127,
      "step": 1026
    },
    {
      "epoch": 0.5127309036445332,
      "grad_norm": 0.6787649393081665,
      "learning_rate": 0.0004948726909635546,
      "loss": 0.012,
      "step": 1027
    },
    {
      "epoch": 0.5132301547678483,
      "grad_norm": 0.13211670517921448,
      "learning_rate": 0.0004948676984523215,
      "loss": 0.0049,
      "step": 1028
    },
    {
      "epoch": 0.5137294058911632,
      "grad_norm": 0.27245840430259705,
      "learning_rate": 0.0004948627059410883,
      "loss": 0.007,
      "step": 1029
    },
    {
      "epoch": 0.5142286570144783,
      "grad_norm": 0.13195165991783142,
      "learning_rate": 0.0004948577134298553,
      "loss": 0.0053,
      "step": 1030
    },
    {
      "epoch": 0.5147279081377933,
      "grad_norm": 0.10916852951049805,
      "learning_rate": 0.000494852720918622,
      "loss": 0.0045,
      "step": 1031
    },
    {
      "epoch": 0.5152271592611083,
      "grad_norm": 0.41219374537467957,
      "learning_rate": 0.000494847728407389,
      "loss": 0.0163,
      "step": 1032
    },
    {
      "epoch": 0.5157264103844234,
      "grad_norm": 0.2788037657737732,
      "learning_rate": 0.0004948427358961558,
      "loss": 0.0084,
      "step": 1033
    },
    {
      "epoch": 0.5162256615077384,
      "grad_norm": 0.5719658732414246,
      "learning_rate": 0.0004948377433849226,
      "loss": 0.0234,
      "step": 1034
    },
    {
      "epoch": 0.5167249126310535,
      "grad_norm": 0.2806393504142761,
      "learning_rate": 0.0004948327508736895,
      "loss": 0.0073,
      "step": 1035
    },
    {
      "epoch": 0.5172241637543684,
      "grad_norm": 0.1882394254207611,
      "learning_rate": 0.0004948277583624563,
      "loss": 0.0083,
      "step": 1036
    },
    {
      "epoch": 0.5177234148776835,
      "grad_norm": 0.7643625140190125,
      "learning_rate": 0.0004948227658512232,
      "loss": 0.0156,
      "step": 1037
    },
    {
      "epoch": 0.5182226660009985,
      "grad_norm": 0.5686871409416199,
      "learning_rate": 0.00049481777333999,
      "loss": 0.0211,
      "step": 1038
    },
    {
      "epoch": 0.5187219171243135,
      "grad_norm": 0.572176992893219,
      "learning_rate": 0.0004948127808287569,
      "loss": 0.0194,
      "step": 1039
    },
    {
      "epoch": 0.5192211682476285,
      "grad_norm": 0.2033364176750183,
      "learning_rate": 0.0004948077883175237,
      "loss": 0.007,
      "step": 1040
    },
    {
      "epoch": 0.5197204193709436,
      "grad_norm": 0.45052188634872437,
      "learning_rate": 0.0004948027958062906,
      "loss": 0.0117,
      "step": 1041
    },
    {
      "epoch": 0.5202196704942587,
      "grad_norm": 0.23591949045658112,
      "learning_rate": 0.0004947978032950574,
      "loss": 0.0067,
      "step": 1042
    },
    {
      "epoch": 0.5207189216175736,
      "grad_norm": 0.20244143903255463,
      "learning_rate": 0.0004947928107838243,
      "loss": 0.0051,
      "step": 1043
    },
    {
      "epoch": 0.5212181727408887,
      "grad_norm": 0.23528139293193817,
      "learning_rate": 0.0004947878182725911,
      "loss": 0.0059,
      "step": 1044
    },
    {
      "epoch": 0.5217174238642037,
      "grad_norm": 0.16194476187229156,
      "learning_rate": 0.000494782825761358,
      "loss": 0.0063,
      "step": 1045
    },
    {
      "epoch": 0.5222166749875187,
      "grad_norm": 0.1502772867679596,
      "learning_rate": 0.0004947778332501248,
      "loss": 0.0042,
      "step": 1046
    },
    {
      "epoch": 0.5227159261108337,
      "grad_norm": 0.25768932700157166,
      "learning_rate": 0.0004947728407388917,
      "loss": 0.004,
      "step": 1047
    },
    {
      "epoch": 0.5232151772341488,
      "grad_norm": 0.20960263907909393,
      "learning_rate": 0.0004947678482276585,
      "loss": 0.0045,
      "step": 1048
    },
    {
      "epoch": 0.5237144283574638,
      "grad_norm": 0.21052391827106476,
      "learning_rate": 0.0004947628557164254,
      "loss": 0.0059,
      "step": 1049
    },
    {
      "epoch": 0.5242136794807788,
      "grad_norm": 0.28632795810699463,
      "learning_rate": 0.0004947578632051922,
      "loss": 0.0231,
      "step": 1050
    },
    {
      "epoch": 0.5247129306040939,
      "grad_norm": 1.017110824584961,
      "learning_rate": 0.0004947528706939591,
      "loss": 0.0572,
      "step": 1051
    },
    {
      "epoch": 0.5252121817274089,
      "grad_norm": 0.0513320192694664,
      "learning_rate": 0.0004947478781827259,
      "loss": 0.0023,
      "step": 1052
    },
    {
      "epoch": 0.5257114328507239,
      "grad_norm": 0.35764971375465393,
      "learning_rate": 0.0004947428856714928,
      "loss": 0.0113,
      "step": 1053
    },
    {
      "epoch": 0.5262106839740389,
      "grad_norm": 0.9859760999679565,
      "learning_rate": 0.0004947378931602596,
      "loss": 0.0148,
      "step": 1054
    },
    {
      "epoch": 0.526709935097354,
      "grad_norm": 0.833623468875885,
      "learning_rate": 0.0004947329006490265,
      "loss": 0.0081,
      "step": 1055
    },
    {
      "epoch": 0.527209186220669,
      "grad_norm": 0.2878574728965759,
      "learning_rate": 0.0004947279081377933,
      "loss": 0.0094,
      "step": 1056
    },
    {
      "epoch": 0.527708437343984,
      "grad_norm": 0.3661104738712311,
      "learning_rate": 0.0004947229156265602,
      "loss": 0.0156,
      "step": 1057
    },
    {
      "epoch": 0.528207688467299,
      "grad_norm": 0.5257596969604492,
      "learning_rate": 0.000494717923115327,
      "loss": 0.011,
      "step": 1058
    },
    {
      "epoch": 0.5287069395906141,
      "grad_norm": 0.29180702567100525,
      "learning_rate": 0.0004947129306040939,
      "loss": 0.0075,
      "step": 1059
    },
    {
      "epoch": 0.5292061907139292,
      "grad_norm": 0.4177582263946533,
      "learning_rate": 0.0004947079380928607,
      "loss": 0.0156,
      "step": 1060
    },
    {
      "epoch": 0.5297054418372441,
      "grad_norm": 0.46695488691329956,
      "learning_rate": 0.0004947029455816276,
      "loss": 0.0113,
      "step": 1061
    },
    {
      "epoch": 0.5302046929605592,
      "grad_norm": 0.1145986020565033,
      "learning_rate": 0.0004946979530703944,
      "loss": 0.0036,
      "step": 1062
    },
    {
      "epoch": 0.5307039440838742,
      "grad_norm": 0.25037455558776855,
      "learning_rate": 0.0004946929605591612,
      "loss": 0.0071,
      "step": 1063
    },
    {
      "epoch": 0.5312031952071892,
      "grad_norm": 0.2661592960357666,
      "learning_rate": 0.0004946879680479281,
      "loss": 0.0177,
      "step": 1064
    },
    {
      "epoch": 0.5317024463305042,
      "grad_norm": 0.5928195714950562,
      "learning_rate": 0.0004946829755366949,
      "loss": 0.0171,
      "step": 1065
    },
    {
      "epoch": 0.5322016974538193,
      "grad_norm": 0.8673507571220398,
      "learning_rate": 0.0004946779830254618,
      "loss": 0.0308,
      "step": 1066
    },
    {
      "epoch": 0.5327009485771343,
      "grad_norm": 0.2704383134841919,
      "learning_rate": 0.0004946729905142286,
      "loss": 0.0063,
      "step": 1067
    },
    {
      "epoch": 0.5332001997004493,
      "grad_norm": 1.2236891984939575,
      "learning_rate": 0.0004946679980029955,
      "loss": 0.0184,
      "step": 1068
    },
    {
      "epoch": 0.5336994508237644,
      "grad_norm": 0.2786395251750946,
      "learning_rate": 0.0004946630054917623,
      "loss": 0.0138,
      "step": 1069
    },
    {
      "epoch": 0.5341987019470794,
      "grad_norm": 0.3254305422306061,
      "learning_rate": 0.0004946580129805292,
      "loss": 0.0146,
      "step": 1070
    },
    {
      "epoch": 0.5346979530703944,
      "grad_norm": 0.23232653737068176,
      "learning_rate": 0.000494653020469296,
      "loss": 0.0075,
      "step": 1071
    },
    {
      "epoch": 0.5351972041937094,
      "grad_norm": 0.15959575772285461,
      "learning_rate": 0.0004946480279580629,
      "loss": 0.0049,
      "step": 1072
    },
    {
      "epoch": 0.5356964553170245,
      "grad_norm": 0.8690748810768127,
      "learning_rate": 0.0004946430354468297,
      "loss": 0.0206,
      "step": 1073
    },
    {
      "epoch": 0.5361957064403395,
      "grad_norm": 0.14739617705345154,
      "learning_rate": 0.0004946380429355966,
      "loss": 0.0036,
      "step": 1074
    },
    {
      "epoch": 0.5366949575636545,
      "grad_norm": 0.07363014668226242,
      "learning_rate": 0.0004946330504243634,
      "loss": 0.0026,
      "step": 1075
    },
    {
      "epoch": 0.5371942086869695,
      "grad_norm": 0.3204682469367981,
      "learning_rate": 0.0004946280579131303,
      "loss": 0.0125,
      "step": 1076
    },
    {
      "epoch": 0.5376934598102846,
      "grad_norm": 0.3967818021774292,
      "learning_rate": 0.0004946230654018971,
      "loss": 0.0087,
      "step": 1077
    },
    {
      "epoch": 0.5381927109335995,
      "grad_norm": 0.14151184260845184,
      "learning_rate": 0.000494618072890664,
      "loss": 0.0062,
      "step": 1078
    },
    {
      "epoch": 0.5386919620569146,
      "grad_norm": 0.2986164391040802,
      "learning_rate": 0.0004946130803794308,
      "loss": 0.0043,
      "step": 1079
    },
    {
      "epoch": 0.5391912131802297,
      "grad_norm": 0.4694354832172394,
      "learning_rate": 0.0004946080878681977,
      "loss": 0.0174,
      "step": 1080
    },
    {
      "epoch": 0.5396904643035447,
      "grad_norm": 0.5997878909111023,
      "learning_rate": 0.0004946030953569645,
      "loss": 0.0201,
      "step": 1081
    },
    {
      "epoch": 0.5401897154268597,
      "grad_norm": 0.2387242466211319,
      "learning_rate": 0.0004945981028457315,
      "loss": 0.0067,
      "step": 1082
    },
    {
      "epoch": 0.5406889665501747,
      "grad_norm": 0.3337022066116333,
      "learning_rate": 0.0004945931103344983,
      "loss": 0.009,
      "step": 1083
    },
    {
      "epoch": 0.5411882176734898,
      "grad_norm": 0.2925187647342682,
      "learning_rate": 0.0004945881178232652,
      "loss": 0.0094,
      "step": 1084
    },
    {
      "epoch": 0.5416874687968047,
      "grad_norm": 0.33545875549316406,
      "learning_rate": 0.000494583125312032,
      "loss": 0.0084,
      "step": 1085
    },
    {
      "epoch": 0.5421867199201198,
      "grad_norm": 0.455014169216156,
      "learning_rate": 0.0004945781328007989,
      "loss": 0.0062,
      "step": 1086
    },
    {
      "epoch": 0.5426859710434349,
      "grad_norm": 0.3241285979747772,
      "learning_rate": 0.0004945731402895657,
      "loss": 0.0068,
      "step": 1087
    },
    {
      "epoch": 0.5431852221667499,
      "grad_norm": 0.3109196126461029,
      "learning_rate": 0.0004945681477783326,
      "loss": 0.0075,
      "step": 1088
    },
    {
      "epoch": 0.5436844732900649,
      "grad_norm": 0.12253787368535995,
      "learning_rate": 0.0004945631552670994,
      "loss": 0.0038,
      "step": 1089
    },
    {
      "epoch": 0.5441837244133799,
      "grad_norm": 0.40955984592437744,
      "learning_rate": 0.0004945581627558663,
      "loss": 0.0159,
      "step": 1090
    },
    {
      "epoch": 0.544682975536695,
      "grad_norm": 0.33882686495780945,
      "learning_rate": 0.0004945531702446331,
      "loss": 0.0094,
      "step": 1091
    },
    {
      "epoch": 0.54518222666001,
      "grad_norm": 0.31696394085884094,
      "learning_rate": 0.0004945481777333999,
      "loss": 0.0093,
      "step": 1092
    },
    {
      "epoch": 0.545681477783325,
      "grad_norm": 0.5668426156044006,
      "learning_rate": 0.0004945431852221668,
      "loss": 0.0082,
      "step": 1093
    },
    {
      "epoch": 0.54618072890664,
      "grad_norm": 0.2040298879146576,
      "learning_rate": 0.0004945381927109336,
      "loss": 0.0039,
      "step": 1094
    },
    {
      "epoch": 0.5466799800299551,
      "grad_norm": 0.29451560974121094,
      "learning_rate": 0.0004945332001997005,
      "loss": 0.0055,
      "step": 1095
    },
    {
      "epoch": 0.54717923115327,
      "grad_norm": 0.6221434473991394,
      "learning_rate": 0.0004945282076884673,
      "loss": 0.0048,
      "step": 1096
    },
    {
      "epoch": 0.5476784822765851,
      "grad_norm": 0.39499419927597046,
      "learning_rate": 0.0004945232151772342,
      "loss": 0.0089,
      "step": 1097
    },
    {
      "epoch": 0.5481777333999002,
      "grad_norm": 0.25680437684059143,
      "learning_rate": 0.000494518222666001,
      "loss": 0.0064,
      "step": 1098
    },
    {
      "epoch": 0.5486769845232152,
      "grad_norm": 0.7946133613586426,
      "learning_rate": 0.0004945132301547679,
      "loss": 0.0256,
      "step": 1099
    },
    {
      "epoch": 0.5491762356465302,
      "grad_norm": 0.19050858914852142,
      "learning_rate": 0.0004945082376435347,
      "loss": 0.005,
      "step": 1100
    },
    {
      "epoch": 0.5496754867698452,
      "grad_norm": 0.03477233648300171,
      "learning_rate": 0.0004945032451323016,
      "loss": 0.0019,
      "step": 1101
    },
    {
      "epoch": 0.5501747378931603,
      "grad_norm": 0.536064088344574,
      "learning_rate": 0.0004944982526210684,
      "loss": 0.0125,
      "step": 1102
    },
    {
      "epoch": 0.5506739890164752,
      "grad_norm": 0.1817954182624817,
      "learning_rate": 0.0004944932601098353,
      "loss": 0.0051,
      "step": 1103
    },
    {
      "epoch": 0.5511732401397903,
      "grad_norm": 0.09322533756494522,
      "learning_rate": 0.0004944882675986021,
      "loss": 0.0026,
      "step": 1104
    },
    {
      "epoch": 0.5516724912631054,
      "grad_norm": 0.13018852472305298,
      "learning_rate": 0.000494483275087369,
      "loss": 0.0041,
      "step": 1105
    },
    {
      "epoch": 0.5521717423864204,
      "grad_norm": 0.10352884232997894,
      "learning_rate": 0.0004944782825761358,
      "loss": 0.0029,
      "step": 1106
    },
    {
      "epoch": 0.5526709935097354,
      "grad_norm": 0.06094925478100777,
      "learning_rate": 0.0004944732900649027,
      "loss": 0.0022,
      "step": 1107
    },
    {
      "epoch": 0.5531702446330504,
      "grad_norm": 0.1591632068157196,
      "learning_rate": 0.0004944682975536695,
      "loss": 0.0044,
      "step": 1108
    },
    {
      "epoch": 0.5536694957563655,
      "grad_norm": 0.3108700215816498,
      "learning_rate": 0.0004944633050424364,
      "loss": 0.0103,
      "step": 1109
    },
    {
      "epoch": 0.5541687468796804,
      "grad_norm": 0.1316596418619156,
      "learning_rate": 0.0004944583125312032,
      "loss": 0.0031,
      "step": 1110
    },
    {
      "epoch": 0.5546679980029955,
      "grad_norm": 0.8393359184265137,
      "learning_rate": 0.0004944533200199701,
      "loss": 0.0055,
      "step": 1111
    },
    {
      "epoch": 0.5551672491263105,
      "grad_norm": 0.6166521906852722,
      "learning_rate": 0.0004944483275087369,
      "loss": 0.0053,
      "step": 1112
    },
    {
      "epoch": 0.5556665002496256,
      "grad_norm": 0.06975795328617096,
      "learning_rate": 0.0004944433349975038,
      "loss": 0.0023,
      "step": 1113
    },
    {
      "epoch": 0.5561657513729406,
      "grad_norm": 0.18056164681911469,
      "learning_rate": 0.0004944383424862706,
      "loss": 0.0031,
      "step": 1114
    },
    {
      "epoch": 0.5566650024962556,
      "grad_norm": 0.36587169766426086,
      "learning_rate": 0.0004944333499750375,
      "loss": 0.0091,
      "step": 1115
    },
    {
      "epoch": 0.5571642536195707,
      "grad_norm": 0.11282166093587875,
      "learning_rate": 0.0004944283574638043,
      "loss": 0.0029,
      "step": 1116
    },
    {
      "epoch": 0.5576635047428856,
      "grad_norm": 0.15571993589401245,
      "learning_rate": 0.0004944233649525712,
      "loss": 0.0029,
      "step": 1117
    },
    {
      "epoch": 0.5581627558662007,
      "grad_norm": 0.35514208674430847,
      "learning_rate": 0.000494418372441338,
      "loss": 0.0097,
      "step": 1118
    },
    {
      "epoch": 0.5586620069895157,
      "grad_norm": 0.4303053617477417,
      "learning_rate": 0.0004944133799301049,
      "loss": 0.0032,
      "step": 1119
    },
    {
      "epoch": 0.5591612581128308,
      "grad_norm": 0.7117800116539001,
      "learning_rate": 0.0004944083874188717,
      "loss": 0.0272,
      "step": 1120
    },
    {
      "epoch": 0.5596605092361457,
      "grad_norm": 0.14951562881469727,
      "learning_rate": 0.0004944033949076385,
      "loss": 0.0033,
      "step": 1121
    },
    {
      "epoch": 0.5601597603594608,
      "grad_norm": 0.23073643445968628,
      "learning_rate": 0.0004943984023964054,
      "loss": 0.0026,
      "step": 1122
    },
    {
      "epoch": 0.5606590114827759,
      "grad_norm": 0.38777366280555725,
      "learning_rate": 0.0004943934098851722,
      "loss": 0.0102,
      "step": 1123
    },
    {
      "epoch": 0.5611582626060909,
      "grad_norm": 0.5097828507423401,
      "learning_rate": 0.0004943884173739391,
      "loss": 0.0486,
      "step": 1124
    },
    {
      "epoch": 0.5616575137294059,
      "grad_norm": 0.17799413204193115,
      "learning_rate": 0.0004943834248627059,
      "loss": 0.004,
      "step": 1125
    },
    {
      "epoch": 0.5621567648527209,
      "grad_norm": 0.11480767279863358,
      "learning_rate": 0.0004943784323514728,
      "loss": 0.0031,
      "step": 1126
    },
    {
      "epoch": 0.562656015976036,
      "grad_norm": 0.1981116086244583,
      "learning_rate": 0.0004943734398402396,
      "loss": 0.0051,
      "step": 1127
    },
    {
      "epoch": 0.5631552670993509,
      "grad_norm": 0.19312353432178497,
      "learning_rate": 0.0004943684473290065,
      "loss": 0.0039,
      "step": 1128
    },
    {
      "epoch": 0.563654518222666,
      "grad_norm": 0.4714094400405884,
      "learning_rate": 0.0004943634548177733,
      "loss": 0.0806,
      "step": 1129
    },
    {
      "epoch": 0.564153769345981,
      "grad_norm": 0.22687742114067078,
      "learning_rate": 0.0004943584623065402,
      "loss": 0.0061,
      "step": 1130
    },
    {
      "epoch": 0.5646530204692961,
      "grad_norm": 0.2132302224636078,
      "learning_rate": 0.000494353469795307,
      "loss": 0.0056,
      "step": 1131
    },
    {
      "epoch": 0.565152271592611,
      "grad_norm": 0.2476896494626999,
      "learning_rate": 0.000494348477284074,
      "loss": 0.0114,
      "step": 1132
    },
    {
      "epoch": 0.5656515227159261,
      "grad_norm": 0.29548147320747375,
      "learning_rate": 0.0004943434847728407,
      "loss": 0.0072,
      "step": 1133
    },
    {
      "epoch": 0.5661507738392412,
      "grad_norm": 0.24558402597904205,
      "learning_rate": 0.0004943384922616075,
      "loss": 0.0067,
      "step": 1134
    },
    {
      "epoch": 0.5666500249625561,
      "grad_norm": 0.4665835499763489,
      "learning_rate": 0.0004943334997503745,
      "loss": 0.0226,
      "step": 1135
    },
    {
      "epoch": 0.5671492760858712,
      "grad_norm": 0.1606307327747345,
      "learning_rate": 0.0004943285072391412,
      "loss": 0.0041,
      "step": 1136
    },
    {
      "epoch": 0.5676485272091862,
      "grad_norm": 0.22145414352416992,
      "learning_rate": 0.0004943235147279082,
      "loss": 0.0067,
      "step": 1137
    },
    {
      "epoch": 0.5681477783325013,
      "grad_norm": 0.10291757434606552,
      "learning_rate": 0.000494318522216675,
      "loss": 0.0035,
      "step": 1138
    },
    {
      "epoch": 0.5686470294558162,
      "grad_norm": 0.03686980903148651,
      "learning_rate": 0.0004943135297054419,
      "loss": 0.0019,
      "step": 1139
    },
    {
      "epoch": 0.5691462805791313,
      "grad_norm": 0.05571450665593147,
      "learning_rate": 0.0004943085371942087,
      "loss": 0.0027,
      "step": 1140
    },
    {
      "epoch": 0.5696455317024464,
      "grad_norm": 0.11302153766155243,
      "learning_rate": 0.0004943035446829756,
      "loss": 0.004,
      "step": 1141
    },
    {
      "epoch": 0.5701447828257613,
      "grad_norm": 0.1256934106349945,
      "learning_rate": 0.0004942985521717424,
      "loss": 0.0031,
      "step": 1142
    },
    {
      "epoch": 0.5706440339490764,
      "grad_norm": 0.36324384808540344,
      "learning_rate": 0.0004942935596605093,
      "loss": 0.0059,
      "step": 1143
    },
    {
      "epoch": 0.5711432850723914,
      "grad_norm": 0.060161199420690536,
      "learning_rate": 0.0004942885671492761,
      "loss": 0.0026,
      "step": 1144
    },
    {
      "epoch": 0.5716425361957065,
      "grad_norm": 0.47942590713500977,
      "learning_rate": 0.000494283574638043,
      "loss": 0.0155,
      "step": 1145
    },
    {
      "epoch": 0.5721417873190214,
      "grad_norm": 0.07497432082891464,
      "learning_rate": 0.0004942785821268098,
      "loss": 0.0032,
      "step": 1146
    },
    {
      "epoch": 0.5726410384423365,
      "grad_norm": 0.024804487824440002,
      "learning_rate": 0.0004942735896155767,
      "loss": 0.0019,
      "step": 1147
    },
    {
      "epoch": 0.5731402895656516,
      "grad_norm": 0.13569746911525726,
      "learning_rate": 0.0004942685971043435,
      "loss": 0.0034,
      "step": 1148
    },
    {
      "epoch": 0.5736395406889665,
      "grad_norm": 0.18890973925590515,
      "learning_rate": 0.0004942636045931104,
      "loss": 0.003,
      "step": 1149
    },
    {
      "epoch": 0.5741387918122816,
      "grad_norm": 0.39638423919677734,
      "learning_rate": 0.0004942586120818772,
      "loss": 0.0064,
      "step": 1150
    },
    {
      "epoch": 0.5746380429355966,
      "grad_norm": 0.3800942599773407,
      "learning_rate": 0.000494253619570644,
      "loss": 0.0039,
      "step": 1151
    },
    {
      "epoch": 0.5751372940589117,
      "grad_norm": 1.1529988050460815,
      "learning_rate": 0.0004942486270594109,
      "loss": 0.018,
      "step": 1152
    },
    {
      "epoch": 0.5756365451822266,
      "grad_norm": 0.10154010355472565,
      "learning_rate": 0.0004942436345481777,
      "loss": 0.0023,
      "step": 1153
    },
    {
      "epoch": 0.5761357963055417,
      "grad_norm": 0.20611408352851868,
      "learning_rate": 0.0004942386420369446,
      "loss": 0.0055,
      "step": 1154
    },
    {
      "epoch": 0.5766350474288567,
      "grad_norm": 0.17216701805591583,
      "learning_rate": 0.0004942336495257114,
      "loss": 0.0043,
      "step": 1155
    },
    {
      "epoch": 0.5771342985521717,
      "grad_norm": 0.04587738960981369,
      "learning_rate": 0.0004942286570144783,
      "loss": 0.0019,
      "step": 1156
    },
    {
      "epoch": 0.5776335496754867,
      "grad_norm": 0.5977939367294312,
      "learning_rate": 0.0004942236645032451,
      "loss": 0.0311,
      "step": 1157
    },
    {
      "epoch": 0.5781328007988018,
      "grad_norm": 0.3814774751663208,
      "learning_rate": 0.000494218671992012,
      "loss": 0.0094,
      "step": 1158
    },
    {
      "epoch": 0.5786320519221169,
      "grad_norm": 0.09404230862855911,
      "learning_rate": 0.0004942136794807788,
      "loss": 0.0028,
      "step": 1159
    },
    {
      "epoch": 0.5791313030454318,
      "grad_norm": 0.08059294521808624,
      "learning_rate": 0.0004942086869695457,
      "loss": 0.0024,
      "step": 1160
    },
    {
      "epoch": 0.5796305541687469,
      "grad_norm": 0.08019973337650299,
      "learning_rate": 0.0004942036944583125,
      "loss": 0.0029,
      "step": 1161
    },
    {
      "epoch": 0.5801298052920619,
      "grad_norm": 0.5976668000221252,
      "learning_rate": 0.0004941987019470794,
      "loss": 0.0056,
      "step": 1162
    },
    {
      "epoch": 0.580629056415377,
      "grad_norm": 0.7956194281578064,
      "learning_rate": 0.0004941937094358462,
      "loss": 0.0174,
      "step": 1163
    },
    {
      "epoch": 0.5811283075386919,
      "grad_norm": 0.3443295955657959,
      "learning_rate": 0.0004941887169246131,
      "loss": 0.0104,
      "step": 1164
    },
    {
      "epoch": 0.581627558662007,
      "grad_norm": 0.27993130683898926,
      "learning_rate": 0.0004941837244133799,
      "loss": 0.0059,
      "step": 1165
    },
    {
      "epoch": 0.582126809785322,
      "grad_norm": 0.4061604142189026,
      "learning_rate": 0.0004941787319021468,
      "loss": 0.0068,
      "step": 1166
    },
    {
      "epoch": 0.582626060908637,
      "grad_norm": 0.0693337544798851,
      "learning_rate": 0.0004941737393909136,
      "loss": 0.0022,
      "step": 1167
    },
    {
      "epoch": 0.5831253120319521,
      "grad_norm": 0.6117417812347412,
      "learning_rate": 0.0004941687468796805,
      "loss": 0.01,
      "step": 1168
    },
    {
      "epoch": 0.5836245631552671,
      "grad_norm": 0.08272487670183182,
      "learning_rate": 0.0004941637543684473,
      "loss": 0.0027,
      "step": 1169
    },
    {
      "epoch": 0.5841238142785822,
      "grad_norm": 0.10593278706073761,
      "learning_rate": 0.0004941587618572142,
      "loss": 0.0028,
      "step": 1170
    },
    {
      "epoch": 0.5846230654018971,
      "grad_norm": 0.4702387750148773,
      "learning_rate": 0.000494153769345981,
      "loss": 0.0146,
      "step": 1171
    },
    {
      "epoch": 0.5851223165252122,
      "grad_norm": 0.0657941922545433,
      "learning_rate": 0.0004941487768347479,
      "loss": 0.0024,
      "step": 1172
    },
    {
      "epoch": 0.5856215676485272,
      "grad_norm": 0.38792359828948975,
      "learning_rate": 0.0004941437843235147,
      "loss": 0.0079,
      "step": 1173
    },
    {
      "epoch": 0.5861208187718422,
      "grad_norm": 0.24999366700649261,
      "learning_rate": 0.0004941387918122816,
      "loss": 0.0082,
      "step": 1174
    },
    {
      "epoch": 0.5866200698951572,
      "grad_norm": 0.394380122423172,
      "learning_rate": 0.0004941337993010484,
      "loss": 0.0145,
      "step": 1175
    },
    {
      "epoch": 0.5871193210184723,
      "grad_norm": 0.4743615686893463,
      "learning_rate": 0.0004941288067898153,
      "loss": 0.0193,
      "step": 1176
    },
    {
      "epoch": 0.5876185721417874,
      "grad_norm": 0.1066635474562645,
      "learning_rate": 0.0004941238142785821,
      "loss": 0.0043,
      "step": 1177
    },
    {
      "epoch": 0.5881178232651023,
      "grad_norm": 0.4266882836818695,
      "learning_rate": 0.000494118821767349,
      "loss": 0.0398,
      "step": 1178
    },
    {
      "epoch": 0.5886170743884174,
      "grad_norm": 0.3010869324207306,
      "learning_rate": 0.0004941138292561158,
      "loss": 0.0106,
      "step": 1179
    },
    {
      "epoch": 0.5891163255117324,
      "grad_norm": 0.1790926605463028,
      "learning_rate": 0.0004941088367448826,
      "loss": 0.0105,
      "step": 1180
    },
    {
      "epoch": 0.5896155766350474,
      "grad_norm": 0.7252098321914673,
      "learning_rate": 0.0004941038442336495,
      "loss": 0.013,
      "step": 1181
    },
    {
      "epoch": 0.5901148277583624,
      "grad_norm": 0.06738845258951187,
      "learning_rate": 0.0004940988517224163,
      "loss": 0.0031,
      "step": 1182
    },
    {
      "epoch": 0.5906140788816775,
      "grad_norm": 0.12489846348762512,
      "learning_rate": 0.0004940938592111832,
      "loss": 0.0044,
      "step": 1183
    },
    {
      "epoch": 0.5911133300049926,
      "grad_norm": 0.3243981897830963,
      "learning_rate": 0.00049408886669995,
      "loss": 0.0077,
      "step": 1184
    },
    {
      "epoch": 0.5916125811283075,
      "grad_norm": 0.5697801113128662,
      "learning_rate": 0.0004940838741887169,
      "loss": 0.0129,
      "step": 1185
    },
    {
      "epoch": 0.5921118322516226,
      "grad_norm": 1.3968360424041748,
      "learning_rate": 0.0004940788816774837,
      "loss": 0.03,
      "step": 1186
    },
    {
      "epoch": 0.5926110833749376,
      "grad_norm": 0.5523977279663086,
      "learning_rate": 0.0004940738891662506,
      "loss": 0.0237,
      "step": 1187
    },
    {
      "epoch": 0.5931103344982526,
      "grad_norm": 1.2882647514343262,
      "learning_rate": 0.0004940688966550174,
      "loss": 0.0473,
      "step": 1188
    },
    {
      "epoch": 0.5936095856215676,
      "grad_norm": 0.3427470922470093,
      "learning_rate": 0.0004940639041437844,
      "loss": 0.0101,
      "step": 1189
    },
    {
      "epoch": 0.5941088367448827,
      "grad_norm": 0.24640469253063202,
      "learning_rate": 0.0004940589116325512,
      "loss": 0.0064,
      "step": 1190
    },
    {
      "epoch": 0.5946080878681977,
      "grad_norm": 0.2793199121952057,
      "learning_rate": 0.0004940539191213181,
      "loss": 0.0075,
      "step": 1191
    },
    {
      "epoch": 0.5951073389915127,
      "grad_norm": 0.38926443457603455,
      "learning_rate": 0.0004940489266100849,
      "loss": 0.0107,
      "step": 1192
    },
    {
      "epoch": 0.5956065901148277,
      "grad_norm": 1.9434046745300293,
      "learning_rate": 0.0004940439340988518,
      "loss": 0.0266,
      "step": 1193
    },
    {
      "epoch": 0.5961058412381428,
      "grad_norm": 0.5579470992088318,
      "learning_rate": 0.0004940389415876186,
      "loss": 0.0155,
      "step": 1194
    },
    {
      "epoch": 0.5966050923614579,
      "grad_norm": 0.4633670151233673,
      "learning_rate": 0.0004940339490763855,
      "loss": 0.0127,
      "step": 1195
    },
    {
      "epoch": 0.5971043434847728,
      "grad_norm": 1.1363098621368408,
      "learning_rate": 0.0004940289565651523,
      "loss": 0.0168,
      "step": 1196
    },
    {
      "epoch": 0.5976035946080879,
      "grad_norm": 0.5395978093147278,
      "learning_rate": 0.0004940239640539192,
      "loss": 0.0195,
      "step": 1197
    },
    {
      "epoch": 0.5981028457314029,
      "grad_norm": 0.3949502110481262,
      "learning_rate": 0.000494018971542686,
      "loss": 0.0165,
      "step": 1198
    },
    {
      "epoch": 0.5986020968547179,
      "grad_norm": 0.061037734150886536,
      "learning_rate": 0.0004940139790314529,
      "loss": 0.0035,
      "step": 1199
    },
    {
      "epoch": 0.5991013479780329,
      "grad_norm": 0.06714393943548203,
      "learning_rate": 0.0004940089865202197,
      "loss": 0.0038,
      "step": 1200
    },
    {
      "epoch": 0.599600599101348,
      "grad_norm": 0.802909791469574,
      "learning_rate": 0.0004940039940089866,
      "loss": 0.0202,
      "step": 1201
    },
    {
      "epoch": 0.6000998502246631,
      "grad_norm": 1.202873945236206,
      "learning_rate": 0.0004939990014977534,
      "loss": 0.0525,
      "step": 1202
    },
    {
      "epoch": 0.600599101347978,
      "grad_norm": 0.6967864036560059,
      "learning_rate": 0.0004939940089865203,
      "loss": 0.0174,
      "step": 1203
    },
    {
      "epoch": 0.6010983524712931,
      "grad_norm": 1.0027943849563599,
      "learning_rate": 0.0004939890164752871,
      "loss": 0.0203,
      "step": 1204
    },
    {
      "epoch": 0.6015976035946081,
      "grad_norm": 0.6242245435714722,
      "learning_rate": 0.000493984023964054,
      "loss": 0.017,
      "step": 1205
    },
    {
      "epoch": 0.6020968547179231,
      "grad_norm": 0.15091192722320557,
      "learning_rate": 0.0004939790314528208,
      "loss": 0.0059,
      "step": 1206
    },
    {
      "epoch": 0.6025961058412381,
      "grad_norm": 2.4394235610961914,
      "learning_rate": 0.0004939740389415877,
      "loss": 0.0173,
      "step": 1207
    },
    {
      "epoch": 0.6030953569645532,
      "grad_norm": 0.1660117208957672,
      "learning_rate": 0.0004939690464303545,
      "loss": 0.0049,
      "step": 1208
    },
    {
      "epoch": 0.6035946080878682,
      "grad_norm": 0.22604845464229584,
      "learning_rate": 0.0004939640539191213,
      "loss": 0.0072,
      "step": 1209
    },
    {
      "epoch": 0.6040938592111832,
      "grad_norm": 0.3355877101421356,
      "learning_rate": 0.0004939590614078882,
      "loss": 0.0115,
      "step": 1210
    },
    {
      "epoch": 0.6045931103344983,
      "grad_norm": 0.3078790605068207,
      "learning_rate": 0.000493954068896655,
      "loss": 0.0147,
      "step": 1211
    },
    {
      "epoch": 0.6050923614578133,
      "grad_norm": 0.4196494221687317,
      "learning_rate": 0.0004939490763854219,
      "loss": 0.0118,
      "step": 1212
    },
    {
      "epoch": 0.6055916125811283,
      "grad_norm": 0.280705988407135,
      "learning_rate": 0.0004939440838741887,
      "loss": 0.0141,
      "step": 1213
    },
    {
      "epoch": 0.6060908637044433,
      "grad_norm": 0.5423350930213928,
      "learning_rate": 0.0004939390913629556,
      "loss": 0.0136,
      "step": 1214
    },
    {
      "epoch": 0.6065901148277584,
      "grad_norm": 3.1242434978485107,
      "learning_rate": 0.0004939340988517224,
      "loss": 0.0214,
      "step": 1215
    },
    {
      "epoch": 0.6070893659510734,
      "grad_norm": 0.2696564495563507,
      "learning_rate": 0.0004939291063404893,
      "loss": 0.006,
      "step": 1216
    },
    {
      "epoch": 0.6075886170743884,
      "grad_norm": 0.10922829061746597,
      "learning_rate": 0.0004939241138292561,
      "loss": 0.0035,
      "step": 1217
    },
    {
      "epoch": 0.6080878681977034,
      "grad_norm": 0.2111159861087799,
      "learning_rate": 0.000493919121318023,
      "loss": 0.0054,
      "step": 1218
    },
    {
      "epoch": 0.6085871193210185,
      "grad_norm": 0.2229175716638565,
      "learning_rate": 0.0004939141288067898,
      "loss": 0.0053,
      "step": 1219
    },
    {
      "epoch": 0.6090863704443334,
      "grad_norm": 3.5710246562957764,
      "learning_rate": 0.0004939091362955567,
      "loss": 0.0148,
      "step": 1220
    },
    {
      "epoch": 0.6095856215676485,
      "grad_norm": 0.10351202636957169,
      "learning_rate": 0.0004939041437843235,
      "loss": 0.0038,
      "step": 1221
    },
    {
      "epoch": 0.6100848726909636,
      "grad_norm": 1.3316776752471924,
      "learning_rate": 0.0004938991512730904,
      "loss": 0.0263,
      "step": 1222
    },
    {
      "epoch": 0.6105841238142786,
      "grad_norm": 0.5915860533714294,
      "learning_rate": 0.0004938941587618572,
      "loss": 0.0214,
      "step": 1223
    },
    {
      "epoch": 0.6110833749375936,
      "grad_norm": 0.33141714334487915,
      "learning_rate": 0.0004938891662506241,
      "loss": 0.0132,
      "step": 1224
    },
    {
      "epoch": 0.6115826260609086,
      "grad_norm": 0.7792071104049683,
      "learning_rate": 0.0004938841737393909,
      "loss": 0.0139,
      "step": 1225
    },
    {
      "epoch": 0.6120818771842237,
      "grad_norm": 0.09574105590581894,
      "learning_rate": 0.0004938791812281578,
      "loss": 0.0018,
      "step": 1226
    },
    {
      "epoch": 0.6125811283075387,
      "grad_norm": 0.29374855756759644,
      "learning_rate": 0.0004938741887169246,
      "loss": 0.0042,
      "step": 1227
    },
    {
      "epoch": 0.6130803794308537,
      "grad_norm": 0.043418169021606445,
      "learning_rate": 0.0004938691962056915,
      "loss": 0.0028,
      "step": 1228
    },
    {
      "epoch": 0.6135796305541688,
      "grad_norm": 0.31399035453796387,
      "learning_rate": 0.0004938642036944583,
      "loss": 0.0115,
      "step": 1229
    },
    {
      "epoch": 0.6140788816774838,
      "grad_norm": 0.6391595005989075,
      "learning_rate": 0.0004938592111832252,
      "loss": 0.0051,
      "step": 1230
    },
    {
      "epoch": 0.6145781328007988,
      "grad_norm": 0.2799132466316223,
      "learning_rate": 0.000493854218671992,
      "loss": 0.0067,
      "step": 1231
    },
    {
      "epoch": 0.6150773839241138,
      "grad_norm": 0.3808254599571228,
      "learning_rate": 0.0004938492261607589,
      "loss": 0.0185,
      "step": 1232
    },
    {
      "epoch": 0.6155766350474289,
      "grad_norm": 0.7757978439331055,
      "learning_rate": 0.0004938442336495257,
      "loss": 0.0238,
      "step": 1233
    },
    {
      "epoch": 0.6160758861707439,
      "grad_norm": 0.533135175704956,
      "learning_rate": 0.0004938392411382926,
      "loss": 0.0148,
      "step": 1234
    },
    {
      "epoch": 0.6165751372940589,
      "grad_norm": 0.059575315564870834,
      "learning_rate": 0.0004938342486270594,
      "loss": 0.0031,
      "step": 1235
    },
    {
      "epoch": 0.6170743884173739,
      "grad_norm": 0.18518802523612976,
      "learning_rate": 0.0004938292561158263,
      "loss": 0.0026,
      "step": 1236
    },
    {
      "epoch": 0.617573639540689,
      "grad_norm": 1.1809931993484497,
      "learning_rate": 0.0004938242636045931,
      "loss": 0.0269,
      "step": 1237
    },
    {
      "epoch": 0.618072890664004,
      "grad_norm": 3.81330943107605,
      "learning_rate": 0.0004938192710933599,
      "loss": 0.0322,
      "step": 1238
    },
    {
      "epoch": 0.618572141787319,
      "grad_norm": 1.2124379873275757,
      "learning_rate": 0.0004938142785821268,
      "loss": 0.0194,
      "step": 1239
    },
    {
      "epoch": 0.6190713929106341,
      "grad_norm": 0.07178022712469101,
      "learning_rate": 0.0004938092860708936,
      "loss": 0.0037,
      "step": 1240
    },
    {
      "epoch": 0.6195706440339491,
      "grad_norm": 0.6150462031364441,
      "learning_rate": 0.0004938042935596606,
      "loss": 0.0069,
      "step": 1241
    },
    {
      "epoch": 0.6200698951572641,
      "grad_norm": 0.31599676609039307,
      "learning_rate": 0.0004937993010484274,
      "loss": 0.0062,
      "step": 1242
    },
    {
      "epoch": 0.6205691462805791,
      "grad_norm": 0.08770239353179932,
      "learning_rate": 0.0004937943085371943,
      "loss": 0.0035,
      "step": 1243
    },
    {
      "epoch": 0.6210683974038942,
      "grad_norm": 0.16968168318271637,
      "learning_rate": 0.0004937893160259611,
      "loss": 0.0055,
      "step": 1244
    },
    {
      "epoch": 0.6215676485272091,
      "grad_norm": 1.0396896600723267,
      "learning_rate": 0.000493784323514728,
      "loss": 0.012,
      "step": 1245
    },
    {
      "epoch": 0.6220668996505242,
      "grad_norm": 0.4619391858577728,
      "learning_rate": 0.0004937793310034948,
      "loss": 0.011,
      "step": 1246
    },
    {
      "epoch": 0.6225661507738393,
      "grad_norm": 0.26320862770080566,
      "learning_rate": 0.0004937743384922616,
      "loss": 0.0081,
      "step": 1247
    },
    {
      "epoch": 0.6230654018971543,
      "grad_norm": 0.12582729756832123,
      "learning_rate": 0.0004937693459810285,
      "loss": 0.0032,
      "step": 1248
    },
    {
      "epoch": 0.6235646530204693,
      "grad_norm": 0.1119762510061264,
      "learning_rate": 0.0004937643534697953,
      "loss": 0.0032,
      "step": 1249
    },
    {
      "epoch": 0.6240639041437843,
      "grad_norm": 0.46071451902389526,
      "learning_rate": 0.0004937593609585622,
      "loss": 0.0093,
      "step": 1250
    },
    {
      "epoch": 0.6245631552670994,
      "grad_norm": 0.04320644214749336,
      "learning_rate": 0.000493754368447329,
      "loss": 0.0021,
      "step": 1251
    },
    {
      "epoch": 0.6250624063904143,
      "grad_norm": 0.3047848343849182,
      "learning_rate": 0.0004937493759360959,
      "loss": 0.0057,
      "step": 1252
    },
    {
      "epoch": 0.6255616575137294,
      "grad_norm": 0.7579964995384216,
      "learning_rate": 0.0004937443834248627,
      "loss": 0.0314,
      "step": 1253
    },
    {
      "epoch": 0.6260609086370444,
      "grad_norm": 0.06523217260837555,
      "learning_rate": 0.0004937393909136296,
      "loss": 0.0029,
      "step": 1254
    },
    {
      "epoch": 0.6265601597603595,
      "grad_norm": 0.2828463017940521,
      "learning_rate": 0.0004937343984023964,
      "loss": 0.0088,
      "step": 1255
    },
    {
      "epoch": 0.6270594108836745,
      "grad_norm": 0.5327789783477783,
      "learning_rate": 0.0004937294058911633,
      "loss": 0.0147,
      "step": 1256
    },
    {
      "epoch": 0.6275586620069895,
      "grad_norm": 0.7855798006057739,
      "learning_rate": 0.0004937244133799301,
      "loss": 0.009,
      "step": 1257
    },
    {
      "epoch": 0.6280579131303046,
      "grad_norm": 0.1283654123544693,
      "learning_rate": 0.000493719420868697,
      "loss": 0.0046,
      "step": 1258
    },
    {
      "epoch": 0.6285571642536195,
      "grad_norm": 0.0638338252902031,
      "learning_rate": 0.0004937144283574638,
      "loss": 0.0026,
      "step": 1259
    },
    {
      "epoch": 0.6290564153769346,
      "grad_norm": 0.11944513022899628,
      "learning_rate": 0.0004937094358462307,
      "loss": 0.0031,
      "step": 1260
    },
    {
      "epoch": 0.6295556665002496,
      "grad_norm": 0.254517525434494,
      "learning_rate": 0.0004937044433349975,
      "loss": 0.011,
      "step": 1261
    },
    {
      "epoch": 0.6300549176235647,
      "grad_norm": 0.4704553484916687,
      "learning_rate": 0.0004936994508237644,
      "loss": 0.0096,
      "step": 1262
    },
    {
      "epoch": 0.6305541687468796,
      "grad_norm": 0.12596867978572845,
      "learning_rate": 0.0004936944583125312,
      "loss": 0.0038,
      "step": 1263
    },
    {
      "epoch": 0.6310534198701947,
      "grad_norm": 0.41135138273239136,
      "learning_rate": 0.0004936894658012981,
      "loss": 0.0129,
      "step": 1264
    },
    {
      "epoch": 0.6315526709935098,
      "grad_norm": 0.1815067082643509,
      "learning_rate": 0.0004936844732900649,
      "loss": 0.0025,
      "step": 1265
    },
    {
      "epoch": 0.6320519221168248,
      "grad_norm": 0.4950389862060547,
      "learning_rate": 0.0004936794807788317,
      "loss": 0.0082,
      "step": 1266
    },
    {
      "epoch": 0.6325511732401398,
      "grad_norm": 0.1651468724012375,
      "learning_rate": 0.0004936744882675986,
      "loss": 0.0049,
      "step": 1267
    },
    {
      "epoch": 0.6330504243634548,
      "grad_norm": 1.6226236820220947,
      "learning_rate": 0.0004936694957563654,
      "loss": 0.0481,
      "step": 1268
    },
    {
      "epoch": 0.6335496754867699,
      "grad_norm": 0.8953654766082764,
      "learning_rate": 0.0004936645032451323,
      "loss": 0.0309,
      "step": 1269
    },
    {
      "epoch": 0.6340489266100848,
      "grad_norm": 0.3674778342247009,
      "learning_rate": 0.0004936595107338991,
      "loss": 0.008,
      "step": 1270
    },
    {
      "epoch": 0.6345481777333999,
      "grad_norm": 0.8229656219482422,
      "learning_rate": 0.000493654518222666,
      "loss": 0.0095,
      "step": 1271
    },
    {
      "epoch": 0.635047428856715,
      "grad_norm": 0.593615710735321,
      "learning_rate": 0.0004936495257114328,
      "loss": 0.0263,
      "step": 1272
    },
    {
      "epoch": 0.63554667998003,
      "grad_norm": 0.11812423169612885,
      "learning_rate": 0.0004936445332001997,
      "loss": 0.002,
      "step": 1273
    },
    {
      "epoch": 0.636045931103345,
      "grad_norm": 0.15533564984798431,
      "learning_rate": 0.0004936395406889665,
      "loss": 0.0043,
      "step": 1274
    },
    {
      "epoch": 0.63654518222666,
      "grad_norm": 0.09265626966953278,
      "learning_rate": 0.0004936345481777334,
      "loss": 0.0039,
      "step": 1275
    },
    {
      "epoch": 0.6370444333499751,
      "grad_norm": 0.2485157549381256,
      "learning_rate": 0.0004936295556665002,
      "loss": 0.0192,
      "step": 1276
    },
    {
      "epoch": 0.63754368447329,
      "grad_norm": 0.7348489165306091,
      "learning_rate": 0.0004936245631552671,
      "loss": 0.0481,
      "step": 1277
    },
    {
      "epoch": 0.6380429355966051,
      "grad_norm": 0.5747427344322205,
      "learning_rate": 0.0004936195706440339,
      "loss": 0.0313,
      "step": 1278
    },
    {
      "epoch": 0.6385421867199201,
      "grad_norm": 0.4883733093738556,
      "learning_rate": 0.0004936145781328008,
      "loss": 0.0406,
      "step": 1279
    },
    {
      "epoch": 0.6390414378432352,
      "grad_norm": 0.4959369897842407,
      "learning_rate": 0.0004936095856215676,
      "loss": 0.0096,
      "step": 1280
    },
    {
      "epoch": 0.6395406889665501,
      "grad_norm": 0.43247950077056885,
      "learning_rate": 0.0004936045931103345,
      "loss": 0.0082,
      "step": 1281
    },
    {
      "epoch": 0.6400399400898652,
      "grad_norm": 0.4774925410747528,
      "learning_rate": 0.0004935996005991013,
      "loss": 0.0084,
      "step": 1282
    },
    {
      "epoch": 0.6405391912131803,
      "grad_norm": 0.4175381064414978,
      "learning_rate": 0.0004935946080878682,
      "loss": 0.0414,
      "step": 1283
    },
    {
      "epoch": 0.6410384423364952,
      "grad_norm": 0.873241662979126,
      "learning_rate": 0.000493589615576635,
      "loss": 0.0274,
      "step": 1284
    },
    {
      "epoch": 0.6415376934598103,
      "grad_norm": 0.45371609926223755,
      "learning_rate": 0.0004935846230654019,
      "loss": 0.0143,
      "step": 1285
    },
    {
      "epoch": 0.6420369445831253,
      "grad_norm": 0.39086371660232544,
      "learning_rate": 0.0004935796305541687,
      "loss": 0.0092,
      "step": 1286
    },
    {
      "epoch": 0.6425361957064404,
      "grad_norm": 0.615679144859314,
      "learning_rate": 0.0004935746380429356,
      "loss": 0.0179,
      "step": 1287
    },
    {
      "epoch": 0.6430354468297553,
      "grad_norm": 0.1222735121846199,
      "learning_rate": 0.0004935696455317024,
      "loss": 0.0051,
      "step": 1288
    },
    {
      "epoch": 0.6435346979530704,
      "grad_norm": 0.06633573025465012,
      "learning_rate": 0.0004935646530204693,
      "loss": 0.0033,
      "step": 1289
    },
    {
      "epoch": 0.6440339490763854,
      "grad_norm": 0.14610552787780762,
      "learning_rate": 0.0004935596605092361,
      "loss": 0.0054,
      "step": 1290
    },
    {
      "epoch": 0.6445332001997004,
      "grad_norm": 0.17247097194194794,
      "learning_rate": 0.000493554667998003,
      "loss": 0.0062,
      "step": 1291
    },
    {
      "epoch": 0.6450324513230155,
      "grad_norm": 0.49477773904800415,
      "learning_rate": 0.0004935496754867698,
      "loss": 0.0121,
      "step": 1292
    },
    {
      "epoch": 0.6455317024463305,
      "grad_norm": 0.5429931282997131,
      "learning_rate": 0.0004935446829755368,
      "loss": 0.0223,
      "step": 1293
    },
    {
      "epoch": 0.6460309535696456,
      "grad_norm": 1.1013139486312866,
      "learning_rate": 0.0004935396904643035,
      "loss": 0.0515,
      "step": 1294
    },
    {
      "epoch": 0.6465302046929605,
      "grad_norm": 0.2608882486820221,
      "learning_rate": 0.0004935346979530703,
      "loss": 0.0095,
      "step": 1295
    },
    {
      "epoch": 0.6470294558162756,
      "grad_norm": 0.034235794097185135,
      "learning_rate": 0.0004935297054418373,
      "loss": 0.0021,
      "step": 1296
    },
    {
      "epoch": 0.6475287069395906,
      "grad_norm": 0.05019228905439377,
      "learning_rate": 0.000493524712930604,
      "loss": 0.003,
      "step": 1297
    },
    {
      "epoch": 0.6480279580629057,
      "grad_norm": 0.15453465282917023,
      "learning_rate": 0.000493519720419371,
      "loss": 0.0045,
      "step": 1298
    },
    {
      "epoch": 0.6485272091862206,
      "grad_norm": 0.24065671861171722,
      "learning_rate": 0.0004935147279081378,
      "loss": 0.0064,
      "step": 1299
    },
    {
      "epoch": 0.6490264603095357,
      "grad_norm": 0.2645472586154938,
      "learning_rate": 0.0004935097353969047,
      "loss": 0.0087,
      "step": 1300
    },
    {
      "epoch": 0.6495257114328508,
      "grad_norm": 0.6017588973045349,
      "learning_rate": 0.0004935047428856715,
      "loss": 0.0309,
      "step": 1301
    },
    {
      "epoch": 0.6500249625561657,
      "grad_norm": 0.42264026403427124,
      "learning_rate": 0.0004934997503744384,
      "loss": 0.0062,
      "step": 1302
    },
    {
      "epoch": 0.6505242136794808,
      "grad_norm": 0.49349337816238403,
      "learning_rate": 0.0004934947578632052,
      "loss": 0.0261,
      "step": 1303
    },
    {
      "epoch": 0.6510234648027958,
      "grad_norm": 0.2181594967842102,
      "learning_rate": 0.0004934897653519721,
      "loss": 0.0055,
      "step": 1304
    },
    {
      "epoch": 0.6515227159261109,
      "grad_norm": 0.4461900293827057,
      "learning_rate": 0.0004934847728407389,
      "loss": 0.0129,
      "step": 1305
    },
    {
      "epoch": 0.6520219670494258,
      "grad_norm": 0.18092165887355804,
      "learning_rate": 0.0004934797803295058,
      "loss": 0.0049,
      "step": 1306
    },
    {
      "epoch": 0.6525212181727409,
      "grad_norm": 0.09925814718008041,
      "learning_rate": 0.0004934747878182726,
      "loss": 0.0031,
      "step": 1307
    },
    {
      "epoch": 0.653020469296056,
      "grad_norm": 0.5402995944023132,
      "learning_rate": 0.0004934697953070395,
      "loss": 0.0174,
      "step": 1308
    },
    {
      "epoch": 0.6535197204193709,
      "grad_norm": 0.04205957427620888,
      "learning_rate": 0.0004934648027958063,
      "loss": 0.0022,
      "step": 1309
    },
    {
      "epoch": 0.654018971542686,
      "grad_norm": 0.3416699469089508,
      "learning_rate": 0.0004934598102845732,
      "loss": 0.0057,
      "step": 1310
    },
    {
      "epoch": 0.654518222666001,
      "grad_norm": 0.5772430300712585,
      "learning_rate": 0.00049345481777334,
      "loss": 0.0158,
      "step": 1311
    },
    {
      "epoch": 0.6550174737893161,
      "grad_norm": 0.11159668862819672,
      "learning_rate": 0.0004934498252621069,
      "loss": 0.0042,
      "step": 1312
    },
    {
      "epoch": 0.655516724912631,
      "grad_norm": 0.5824552774429321,
      "learning_rate": 0.0004934448327508737,
      "loss": 0.0106,
      "step": 1313
    },
    {
      "epoch": 0.6560159760359461,
      "grad_norm": 0.32273751497268677,
      "learning_rate": 0.0004934398402396406,
      "loss": 0.008,
      "step": 1314
    },
    {
      "epoch": 0.6565152271592611,
      "grad_norm": 0.358977735042572,
      "learning_rate": 0.0004934348477284074,
      "loss": 0.0124,
      "step": 1315
    },
    {
      "epoch": 0.6570144782825761,
      "grad_norm": 0.3597527742385864,
      "learning_rate": 0.0004934298552171743,
      "loss": 0.0091,
      "step": 1316
    },
    {
      "epoch": 0.6575137294058911,
      "grad_norm": 0.5314960479736328,
      "learning_rate": 0.0004934248627059411,
      "loss": 0.0224,
      "step": 1317
    },
    {
      "epoch": 0.6580129805292062,
      "grad_norm": 0.03434421867132187,
      "learning_rate": 0.000493419870194708,
      "loss": 0.0018,
      "step": 1318
    },
    {
      "epoch": 0.6585122316525213,
      "grad_norm": 0.3748888671398163,
      "learning_rate": 0.0004934148776834748,
      "loss": 0.0069,
      "step": 1319
    },
    {
      "epoch": 0.6590114827758362,
      "grad_norm": 0.13587816059589386,
      "learning_rate": 0.0004934098851722417,
      "loss": 0.0043,
      "step": 1320
    },
    {
      "epoch": 0.6595107338991513,
      "grad_norm": 0.18774887919425964,
      "learning_rate": 0.0004934048926610085,
      "loss": 0.0048,
      "step": 1321
    },
    {
      "epoch": 0.6600099850224663,
      "grad_norm": 0.4074241816997528,
      "learning_rate": 0.0004933999001497754,
      "loss": 0.0192,
      "step": 1322
    },
    {
      "epoch": 0.6605092361457813,
      "grad_norm": 0.17260494828224182,
      "learning_rate": 0.0004933949076385422,
      "loss": 0.0248,
      "step": 1323
    },
    {
      "epoch": 0.6610084872690963,
      "grad_norm": 0.6994523406028748,
      "learning_rate": 0.000493389915127309,
      "loss": 0.0129,
      "step": 1324
    },
    {
      "epoch": 0.6615077383924114,
      "grad_norm": 0.10842408239841461,
      "learning_rate": 0.0004933849226160759,
      "loss": 0.0029,
      "step": 1325
    },
    {
      "epoch": 0.6620069895157265,
      "grad_norm": 0.026361705735325813,
      "learning_rate": 0.0004933799301048427,
      "loss": 0.0018,
      "step": 1326
    },
    {
      "epoch": 0.6625062406390414,
      "grad_norm": 0.500472366809845,
      "learning_rate": 0.0004933749375936096,
      "loss": 0.0089,
      "step": 1327
    },
    {
      "epoch": 0.6630054917623565,
      "grad_norm": 0.0610443651676178,
      "learning_rate": 0.0004933699450823764,
      "loss": 0.0026,
      "step": 1328
    },
    {
      "epoch": 0.6635047428856715,
      "grad_norm": 0.08103717863559723,
      "learning_rate": 0.0004933649525711433,
      "loss": 0.0031,
      "step": 1329
    },
    {
      "epoch": 0.6640039940089866,
      "grad_norm": 0.10433779656887054,
      "learning_rate": 0.0004933599600599101,
      "loss": 0.0026,
      "step": 1330
    },
    {
      "epoch": 0.6645032451323015,
      "grad_norm": 0.5337755084037781,
      "learning_rate": 0.000493354967548677,
      "loss": 0.0068,
      "step": 1331
    },
    {
      "epoch": 0.6650024962556166,
      "grad_norm": 0.04917505383491516,
      "learning_rate": 0.0004933499750374438,
      "loss": 0.0024,
      "step": 1332
    },
    {
      "epoch": 0.6655017473789316,
      "grad_norm": 0.19257019460201263,
      "learning_rate": 0.0004933449825262107,
      "loss": 0.0057,
      "step": 1333
    },
    {
      "epoch": 0.6660009985022466,
      "grad_norm": 0.6489421725273132,
      "learning_rate": 0.0004933399900149775,
      "loss": 0.0126,
      "step": 1334
    },
    {
      "epoch": 0.6665002496255616,
      "grad_norm": 0.09094388037919998,
      "learning_rate": 0.0004933349975037444,
      "loss": 0.0032,
      "step": 1335
    },
    {
      "epoch": 0.6669995007488767,
      "grad_norm": 0.11422757059335709,
      "learning_rate": 0.0004933300049925112,
      "loss": 0.0028,
      "step": 1336
    },
    {
      "epoch": 0.6674987518721918,
      "grad_norm": 0.09417557716369629,
      "learning_rate": 0.0004933250124812781,
      "loss": 0.0026,
      "step": 1337
    },
    {
      "epoch": 0.6679980029955067,
      "grad_norm": 0.052936408668756485,
      "learning_rate": 0.0004933200199700449,
      "loss": 0.0025,
      "step": 1338
    },
    {
      "epoch": 0.6684972541188218,
      "grad_norm": 0.48617884516716003,
      "learning_rate": 0.0004933150274588118,
      "loss": 0.0097,
      "step": 1339
    },
    {
      "epoch": 0.6689965052421368,
      "grad_norm": 0.6400316953659058,
      "learning_rate": 0.0004933100349475786,
      "loss": 0.017,
      "step": 1340
    },
    {
      "epoch": 0.6694957563654518,
      "grad_norm": 0.4218195974826813,
      "learning_rate": 0.0004933050424363455,
      "loss": 0.0105,
      "step": 1341
    },
    {
      "epoch": 0.6699950074887668,
      "grad_norm": 0.39296984672546387,
      "learning_rate": 0.0004933000499251123,
      "loss": 0.0045,
      "step": 1342
    },
    {
      "epoch": 0.6704942586120819,
      "grad_norm": 0.6416890025138855,
      "learning_rate": 0.0004932950574138792,
      "loss": 0.0058,
      "step": 1343
    },
    {
      "epoch": 0.670993509735397,
      "grad_norm": 0.18457135558128357,
      "learning_rate": 0.000493290064902646,
      "loss": 0.003,
      "step": 1344
    },
    {
      "epoch": 0.6714927608587119,
      "grad_norm": 0.1579541265964508,
      "learning_rate": 0.000493285072391413,
      "loss": 0.004,
      "step": 1345
    },
    {
      "epoch": 0.671992011982027,
      "grad_norm": 0.9856986999511719,
      "learning_rate": 0.0004932800798801797,
      "loss": 0.0143,
      "step": 1346
    },
    {
      "epoch": 0.672491263105342,
      "grad_norm": 0.04590160772204399,
      "learning_rate": 0.0004932750873689467,
      "loss": 0.0014,
      "step": 1347
    },
    {
      "epoch": 0.672990514228657,
      "grad_norm": 0.28177762031555176,
      "learning_rate": 0.0004932700948577135,
      "loss": 0.0088,
      "step": 1348
    },
    {
      "epoch": 0.673489765351972,
      "grad_norm": 0.3789549469947815,
      "learning_rate": 0.0004932651023464804,
      "loss": 0.0056,
      "step": 1349
    },
    {
      "epoch": 0.6739890164752871,
      "grad_norm": 0.06701426208019257,
      "learning_rate": 0.0004932601098352472,
      "loss": 0.0026,
      "step": 1350
    },
    {
      "epoch": 0.6744882675986021,
      "grad_norm": 0.19923290610313416,
      "learning_rate": 0.0004932551173240141,
      "loss": 0.0029,
      "step": 1351
    },
    {
      "epoch": 0.6749875187219171,
      "grad_norm": 0.3091115653514862,
      "learning_rate": 0.0004932501248127809,
      "loss": 0.009,
      "step": 1352
    },
    {
      "epoch": 0.6754867698452321,
      "grad_norm": 0.06491442024707794,
      "learning_rate": 0.0004932451323015477,
      "loss": 0.0019,
      "step": 1353
    },
    {
      "epoch": 0.6759860209685472,
      "grad_norm": 0.5088055729866028,
      "learning_rate": 0.0004932401397903146,
      "loss": 0.009,
      "step": 1354
    },
    {
      "epoch": 0.6764852720918622,
      "grad_norm": 0.33121180534362793,
      "learning_rate": 0.0004932351472790814,
      "loss": 0.0137,
      "step": 1355
    },
    {
      "epoch": 0.6769845232151772,
      "grad_norm": 0.539272129535675,
      "learning_rate": 0.0004932301547678483,
      "loss": 0.0109,
      "step": 1356
    },
    {
      "epoch": 0.6774837743384923,
      "grad_norm": 0.18934787809848785,
      "learning_rate": 0.0004932251622566151,
      "loss": 0.0035,
      "step": 1357
    },
    {
      "epoch": 0.6779830254618073,
      "grad_norm": 0.38140666484832764,
      "learning_rate": 0.000493220169745382,
      "loss": 0.0109,
      "step": 1358
    },
    {
      "epoch": 0.6784822765851223,
      "grad_norm": 0.24107329547405243,
      "learning_rate": 0.0004932151772341488,
      "loss": 0.0032,
      "step": 1359
    },
    {
      "epoch": 0.6789815277084373,
      "grad_norm": 0.7505792379379272,
      "learning_rate": 0.0004932101847229156,
      "loss": 0.0094,
      "step": 1360
    },
    {
      "epoch": 0.6794807788317524,
      "grad_norm": 0.20420189201831818,
      "learning_rate": 0.0004932051922116825,
      "loss": 0.0028,
      "step": 1361
    },
    {
      "epoch": 0.6799800299550673,
      "grad_norm": 0.6760453581809998,
      "learning_rate": 0.0004932001997004493,
      "loss": 0.0068,
      "step": 1362
    },
    {
      "epoch": 0.6804792810783824,
      "grad_norm": 0.12087372690439224,
      "learning_rate": 0.0004931952071892162,
      "loss": 0.0019,
      "step": 1363
    },
    {
      "epoch": 0.6809785322016975,
      "grad_norm": 0.06848473846912384,
      "learning_rate": 0.000493190214677983,
      "loss": 0.0019,
      "step": 1364
    },
    {
      "epoch": 0.6814777833250125,
      "grad_norm": 0.1746622622013092,
      "learning_rate": 0.0004931852221667499,
      "loss": 0.0032,
      "step": 1365
    },
    {
      "epoch": 0.6819770344483275,
      "grad_norm": 0.3691515326499939,
      "learning_rate": 0.0004931802296555167,
      "loss": 0.0267,
      "step": 1366
    },
    {
      "epoch": 0.6824762855716425,
      "grad_norm": 0.2901509702205658,
      "learning_rate": 0.0004931752371442836,
      "loss": 0.0049,
      "step": 1367
    },
    {
      "epoch": 0.6829755366949576,
      "grad_norm": 0.11228304356336594,
      "learning_rate": 0.0004931702446330504,
      "loss": 0.0033,
      "step": 1368
    },
    {
      "epoch": 0.6834747878182726,
      "grad_norm": 0.5223414301872253,
      "learning_rate": 0.0004931652521218173,
      "loss": 0.0451,
      "step": 1369
    },
    {
      "epoch": 0.6839740389415876,
      "grad_norm": 0.34856924414634705,
      "learning_rate": 0.0004931602596105841,
      "loss": 0.0192,
      "step": 1370
    },
    {
      "epoch": 0.6844732900649027,
      "grad_norm": 0.4207838177680969,
      "learning_rate": 0.000493155267099351,
      "loss": 0.0516,
      "step": 1371
    },
    {
      "epoch": 0.6849725411882177,
      "grad_norm": 1.5906543731689453,
      "learning_rate": 0.0004931502745881178,
      "loss": 0.0467,
      "step": 1372
    },
    {
      "epoch": 0.6854717923115327,
      "grad_norm": 0.28586333990097046,
      "learning_rate": 0.0004931452820768847,
      "loss": 0.0078,
      "step": 1373
    },
    {
      "epoch": 0.6859710434348477,
      "grad_norm": 0.1313745528459549,
      "learning_rate": 0.0004931402895656515,
      "loss": 0.0038,
      "step": 1374
    },
    {
      "epoch": 0.6864702945581628,
      "grad_norm": 1.5033793449401855,
      "learning_rate": 0.0004931352970544184,
      "loss": 0.0116,
      "step": 1375
    },
    {
      "epoch": 0.6869695456814778,
      "grad_norm": 0.057029008865356445,
      "learning_rate": 0.0004931303045431852,
      "loss": 0.0019,
      "step": 1376
    },
    {
      "epoch": 0.6874687968047928,
      "grad_norm": 0.16101117432117462,
      "learning_rate": 0.0004931253120319521,
      "loss": 0.0037,
      "step": 1377
    },
    {
      "epoch": 0.6879680479281078,
      "grad_norm": 0.18150638043880463,
      "learning_rate": 0.0004931203195207189,
      "loss": 0.0037,
      "step": 1378
    },
    {
      "epoch": 0.6884672990514229,
      "grad_norm": 0.5682885050773621,
      "learning_rate": 0.0004931153270094858,
      "loss": 0.0254,
      "step": 1379
    },
    {
      "epoch": 0.6889665501747378,
      "grad_norm": 0.3943392336368561,
      "learning_rate": 0.0004931103344982526,
      "loss": 0.0265,
      "step": 1380
    },
    {
      "epoch": 0.6894658012980529,
      "grad_norm": 0.14215345680713654,
      "learning_rate": 0.0004931053419870195,
      "loss": 0.0025,
      "step": 1381
    },
    {
      "epoch": 0.689965052421368,
      "grad_norm": 0.194977805018425,
      "learning_rate": 0.0004931003494757863,
      "loss": 0.0052,
      "step": 1382
    },
    {
      "epoch": 0.690464303544683,
      "grad_norm": 0.05507868528366089,
      "learning_rate": 0.0004930953569645531,
      "loss": 0.0024,
      "step": 1383
    },
    {
      "epoch": 0.690963554667998,
      "grad_norm": 0.6224998831748962,
      "learning_rate": 0.00049309036445332,
      "loss": 0.0119,
      "step": 1384
    },
    {
      "epoch": 0.691462805791313,
      "grad_norm": 0.1430491954088211,
      "learning_rate": 0.0004930853719420868,
      "loss": 0.0033,
      "step": 1385
    },
    {
      "epoch": 0.6919620569146281,
      "grad_norm": 0.08813855051994324,
      "learning_rate": 0.0004930803794308537,
      "loss": 0.0035,
      "step": 1386
    },
    {
      "epoch": 0.692461308037943,
      "grad_norm": 0.8151083588600159,
      "learning_rate": 0.0004930753869196205,
      "loss": 0.0129,
      "step": 1387
    },
    {
      "epoch": 0.6929605591612581,
      "grad_norm": 0.5074914693832397,
      "learning_rate": 0.0004930703944083874,
      "loss": 0.0181,
      "step": 1388
    },
    {
      "epoch": 0.6934598102845732,
      "grad_norm": 0.45766741037368774,
      "learning_rate": 0.0004930654018971542,
      "loss": 0.0086,
      "step": 1389
    },
    {
      "epoch": 0.6939590614078882,
      "grad_norm": 0.17664645612239838,
      "learning_rate": 0.0004930604093859211,
      "loss": 0.0045,
      "step": 1390
    },
    {
      "epoch": 0.6944583125312032,
      "grad_norm": 0.3150056004524231,
      "learning_rate": 0.0004930554168746879,
      "loss": 0.0085,
      "step": 1391
    },
    {
      "epoch": 0.6949575636545182,
      "grad_norm": 0.22285793721675873,
      "learning_rate": 0.0004930504243634548,
      "loss": 0.0061,
      "step": 1392
    },
    {
      "epoch": 0.6954568147778333,
      "grad_norm": 0.1608603596687317,
      "learning_rate": 0.0004930454318522216,
      "loss": 0.0049,
      "step": 1393
    },
    {
      "epoch": 0.6959560659011482,
      "grad_norm": 0.6554352641105652,
      "learning_rate": 0.0004930404393409885,
      "loss": 0.0062,
      "step": 1394
    },
    {
      "epoch": 0.6964553170244633,
      "grad_norm": 0.15595237910747528,
      "learning_rate": 0.0004930354468297553,
      "loss": 0.0035,
      "step": 1395
    },
    {
      "epoch": 0.6969545681477783,
      "grad_norm": 0.47242194414138794,
      "learning_rate": 0.0004930304543185222,
      "loss": 0.0173,
      "step": 1396
    },
    {
      "epoch": 0.6974538192710934,
      "grad_norm": 0.5081331133842468,
      "learning_rate": 0.000493025461807289,
      "loss": 0.0102,
      "step": 1397
    },
    {
      "epoch": 0.6979530703944083,
      "grad_norm": 0.27130070328712463,
      "learning_rate": 0.000493020469296056,
      "loss": 0.006,
      "step": 1398
    },
    {
      "epoch": 0.6984523215177234,
      "grad_norm": 0.2891126275062561,
      "learning_rate": 0.0004930154767848227,
      "loss": 0.0071,
      "step": 1399
    },
    {
      "epoch": 0.6989515726410385,
      "grad_norm": 0.17918506264686584,
      "learning_rate": 0.0004930104842735897,
      "loss": 0.0047,
      "step": 1400
    },
    {
      "epoch": 0.6994508237643535,
      "grad_norm": 0.06420936435461044,
      "learning_rate": 0.0004930054917623564,
      "loss": 0.0026,
      "step": 1401
    },
    {
      "epoch": 0.6999500748876685,
      "grad_norm": 0.045237958431243896,
      "learning_rate": 0.0004930004992511234,
      "loss": 0.0023,
      "step": 1402
    },
    {
      "epoch": 0.7004493260109835,
      "grad_norm": 0.15017937123775482,
      "learning_rate": 0.0004929955067398902,
      "loss": 0.004,
      "step": 1403
    },
    {
      "epoch": 0.7009485771342986,
      "grad_norm": 0.11822178959846497,
      "learning_rate": 0.0004929905142286571,
      "loss": 0.003,
      "step": 1404
    },
    {
      "epoch": 0.7014478282576135,
      "grad_norm": 0.08112440258264542,
      "learning_rate": 0.0004929855217174239,
      "loss": 0.0023,
      "step": 1405
    },
    {
      "epoch": 0.7019470793809286,
      "grad_norm": 0.5589509010314941,
      "learning_rate": 0.0004929805292061908,
      "loss": 0.0143,
      "step": 1406
    },
    {
      "epoch": 0.7024463305042437,
      "grad_norm": 0.507693886756897,
      "learning_rate": 0.0004929755366949576,
      "loss": 0.0066,
      "step": 1407
    },
    {
      "epoch": 0.7029455816275587,
      "grad_norm": 0.07226821780204773,
      "learning_rate": 0.0004929705441837245,
      "loss": 0.0024,
      "step": 1408
    },
    {
      "epoch": 0.7034448327508737,
      "grad_norm": 0.041871558874845505,
      "learning_rate": 0.0004929655516724913,
      "loss": 0.0017,
      "step": 1409
    },
    {
      "epoch": 0.7039440838741887,
      "grad_norm": 0.058923520147800446,
      "learning_rate": 0.0004929605591612582,
      "loss": 0.0023,
      "step": 1410
    },
    {
      "epoch": 0.7044433349975038,
      "grad_norm": 0.19680987298488617,
      "learning_rate": 0.000492955566650025,
      "loss": 0.0046,
      "step": 1411
    },
    {
      "epoch": 0.7049425861208187,
      "grad_norm": 0.11001575738191605,
      "learning_rate": 0.0004929505741387918,
      "loss": 0.0032,
      "step": 1412
    },
    {
      "epoch": 0.7054418372441338,
      "grad_norm": 0.2522202134132385,
      "learning_rate": 0.0004929455816275587,
      "loss": 0.0038,
      "step": 1413
    },
    {
      "epoch": 0.7059410883674488,
      "grad_norm": 0.03553077206015587,
      "learning_rate": 0.0004929405891163255,
      "loss": 0.0016,
      "step": 1414
    },
    {
      "epoch": 0.7064403394907639,
      "grad_norm": 0.027470722794532776,
      "learning_rate": 0.0004929355966050924,
      "loss": 0.0014,
      "step": 1415
    },
    {
      "epoch": 0.7069395906140788,
      "grad_norm": 0.05765429139137268,
      "learning_rate": 0.0004929306040938592,
      "loss": 0.0022,
      "step": 1416
    },
    {
      "epoch": 0.7074388417373939,
      "grad_norm": 0.2731116712093353,
      "learning_rate": 0.0004929256115826261,
      "loss": 0.0035,
      "step": 1417
    },
    {
      "epoch": 0.707938092860709,
      "grad_norm": 0.422295480966568,
      "learning_rate": 0.0004929206190713929,
      "loss": 0.0063,
      "step": 1418
    },
    {
      "epoch": 0.7084373439840239,
      "grad_norm": 0.03556886315345764,
      "learning_rate": 0.0004929156265601598,
      "loss": 0.0016,
      "step": 1419
    },
    {
      "epoch": 0.708936595107339,
      "grad_norm": 0.012713934294879436,
      "learning_rate": 0.0004929106340489266,
      "loss": 0.001,
      "step": 1420
    },
    {
      "epoch": 0.709435846230654,
      "grad_norm": 0.35857436060905457,
      "learning_rate": 0.0004929056415376935,
      "loss": 0.0058,
      "step": 1421
    },
    {
      "epoch": 0.7099350973539691,
      "grad_norm": 0.6525152921676636,
      "learning_rate": 0.0004929006490264603,
      "loss": 0.0449,
      "step": 1422
    },
    {
      "epoch": 0.710434348477284,
      "grad_norm": 0.6949546933174133,
      "learning_rate": 0.0004928956565152272,
      "loss": 0.014,
      "step": 1423
    },
    {
      "epoch": 0.7109335996005991,
      "grad_norm": 0.009330490604043007,
      "learning_rate": 0.000492890664003994,
      "loss": 0.001,
      "step": 1424
    },
    {
      "epoch": 0.7114328507239142,
      "grad_norm": 0.5433329343795776,
      "learning_rate": 0.0004928856714927609,
      "loss": 0.0073,
      "step": 1425
    },
    {
      "epoch": 0.7119321018472291,
      "grad_norm": 0.05974219739437103,
      "learning_rate": 0.0004928806789815277,
      "loss": 0.002,
      "step": 1426
    },
    {
      "epoch": 0.7124313529705442,
      "grad_norm": 0.2981444001197815,
      "learning_rate": 0.0004928756864702946,
      "loss": 0.0034,
      "step": 1427
    },
    {
      "epoch": 0.7129306040938592,
      "grad_norm": 0.022220799699425697,
      "learning_rate": 0.0004928706939590614,
      "loss": 0.0013,
      "step": 1428
    },
    {
      "epoch": 0.7134298552171743,
      "grad_norm": 0.05771145969629288,
      "learning_rate": 0.0004928657014478283,
      "loss": 0.0015,
      "step": 1429
    },
    {
      "epoch": 0.7139291063404892,
      "grad_norm": 0.2976539731025696,
      "learning_rate": 0.0004928607089365951,
      "loss": 0.0064,
      "step": 1430
    },
    {
      "epoch": 0.7144283574638043,
      "grad_norm": 0.42773935198783875,
      "learning_rate": 0.000492855716425362,
      "loss": 0.0023,
      "step": 1431
    },
    {
      "epoch": 0.7149276085871193,
      "grad_norm": 0.9660201668739319,
      "learning_rate": 0.0004928507239141288,
      "loss": 0.0081,
      "step": 1432
    },
    {
      "epoch": 0.7154268597104343,
      "grad_norm": 0.03628569841384888,
      "learning_rate": 0.0004928457314028957,
      "loss": 0.0014,
      "step": 1433
    },
    {
      "epoch": 0.7159261108337494,
      "grad_norm": 0.09386856108903885,
      "learning_rate": 0.0004928407388916625,
      "loss": 0.0025,
      "step": 1434
    },
    {
      "epoch": 0.7164253619570644,
      "grad_norm": 0.20600005984306335,
      "learning_rate": 0.0004928357463804294,
      "loss": 0.0055,
      "step": 1435
    },
    {
      "epoch": 0.7169246130803795,
      "grad_norm": 0.26390761137008667,
      "learning_rate": 0.0004928307538691962,
      "loss": 0.0044,
      "step": 1436
    },
    {
      "epoch": 0.7174238642036944,
      "grad_norm": 0.5879124999046326,
      "learning_rate": 0.0004928257613579631,
      "loss": 0.0144,
      "step": 1437
    },
    {
      "epoch": 0.7179231153270095,
      "grad_norm": 0.5754067301750183,
      "learning_rate": 0.0004928207688467299,
      "loss": 0.0087,
      "step": 1438
    },
    {
      "epoch": 0.7184223664503245,
      "grad_norm": 0.13170132040977478,
      "learning_rate": 0.0004928157763354968,
      "loss": 0.0039,
      "step": 1439
    },
    {
      "epoch": 0.7189216175736396,
      "grad_norm": 0.22435316443443298,
      "learning_rate": 0.0004928107838242636,
      "loss": 0.0052,
      "step": 1440
    },
    {
      "epoch": 0.7194208686969545,
      "grad_norm": 0.24452269077301025,
      "learning_rate": 0.0004928057913130304,
      "loss": 0.0037,
      "step": 1441
    },
    {
      "epoch": 0.7199201198202696,
      "grad_norm": 0.23123379051685333,
      "learning_rate": 0.0004928007988017973,
      "loss": 0.004,
      "step": 1442
    },
    {
      "epoch": 0.7204193709435847,
      "grad_norm": 0.014406680129468441,
      "learning_rate": 0.0004927958062905641,
      "loss": 0.001,
      "step": 1443
    },
    {
      "epoch": 0.7209186220668996,
      "grad_norm": 0.44836950302124023,
      "learning_rate": 0.000492790813779331,
      "loss": 0.0057,
      "step": 1444
    },
    {
      "epoch": 0.7214178731902147,
      "grad_norm": 0.40628865361213684,
      "learning_rate": 0.0004927858212680978,
      "loss": 0.01,
      "step": 1445
    },
    {
      "epoch": 0.7219171243135297,
      "grad_norm": 0.7269193530082703,
      "learning_rate": 0.0004927808287568647,
      "loss": 0.0237,
      "step": 1446
    },
    {
      "epoch": 0.7224163754368448,
      "grad_norm": 0.43892616033554077,
      "learning_rate": 0.0004927758362456315,
      "loss": 0.0129,
      "step": 1447
    },
    {
      "epoch": 0.7229156265601597,
      "grad_norm": 0.5126250386238098,
      "learning_rate": 0.0004927708437343984,
      "loss": 0.013,
      "step": 1448
    },
    {
      "epoch": 0.7234148776834748,
      "grad_norm": 0.18754632771015167,
      "learning_rate": 0.0004927658512231652,
      "loss": 0.0034,
      "step": 1449
    },
    {
      "epoch": 0.7239141288067898,
      "grad_norm": 0.2167891263961792,
      "learning_rate": 0.0004927608587119321,
      "loss": 0.0047,
      "step": 1450
    },
    {
      "epoch": 0.7244133799301048,
      "grad_norm": 0.1279970109462738,
      "learning_rate": 0.0004927558662006989,
      "loss": 0.0026,
      "step": 1451
    },
    {
      "epoch": 0.7249126310534199,
      "grad_norm": 0.2301023304462433,
      "learning_rate": 0.0004927508736894659,
      "loss": 0.0069,
      "step": 1452
    },
    {
      "epoch": 0.7254118821767349,
      "grad_norm": 0.22271481156349182,
      "learning_rate": 0.0004927458811782326,
      "loss": 0.0053,
      "step": 1453
    },
    {
      "epoch": 0.72591113330005,
      "grad_norm": 0.4052756130695343,
      "learning_rate": 0.0004927408886669996,
      "loss": 0.0105,
      "step": 1454
    },
    {
      "epoch": 0.7264103844233649,
      "grad_norm": 0.04933810606598854,
      "learning_rate": 0.0004927358961557664,
      "loss": 0.0019,
      "step": 1455
    },
    {
      "epoch": 0.72690963554668,
      "grad_norm": 0.8069167137145996,
      "learning_rate": 0.0004927309036445333,
      "loss": 0.023,
      "step": 1456
    },
    {
      "epoch": 0.727408886669995,
      "grad_norm": 0.2269810140132904,
      "learning_rate": 0.0004927259111333001,
      "loss": 0.0049,
      "step": 1457
    },
    {
      "epoch": 0.72790813779331,
      "grad_norm": 0.8986949324607849,
      "learning_rate": 0.000492720918622067,
      "loss": 0.0251,
      "step": 1458
    },
    {
      "epoch": 0.728407388916625,
      "grad_norm": 0.09685103595256805,
      "learning_rate": 0.0004927159261108338,
      "loss": 0.0035,
      "step": 1459
    },
    {
      "epoch": 0.7289066400399401,
      "grad_norm": 0.18986776471138,
      "learning_rate": 0.0004927109335996007,
      "loss": 0.0035,
      "step": 1460
    },
    {
      "epoch": 0.7294058911632552,
      "grad_norm": 0.08282452821731567,
      "learning_rate": 0.0004927059410883675,
      "loss": 0.0028,
      "step": 1461
    },
    {
      "epoch": 0.7299051422865701,
      "grad_norm": 0.33387863636016846,
      "learning_rate": 0.0004927009485771344,
      "loss": 0.0157,
      "step": 1462
    },
    {
      "epoch": 0.7304043934098852,
      "grad_norm": 0.3003210425376892,
      "learning_rate": 0.0004926959560659012,
      "loss": 0.0052,
      "step": 1463
    },
    {
      "epoch": 0.7309036445332002,
      "grad_norm": 0.35428154468536377,
      "learning_rate": 0.0004926909635546681,
      "loss": 0.011,
      "step": 1464
    },
    {
      "epoch": 0.7314028956565152,
      "grad_norm": 0.5346254110336304,
      "learning_rate": 0.0004926859710434349,
      "loss": 0.0116,
      "step": 1465
    },
    {
      "epoch": 0.7319021467798302,
      "grad_norm": 0.30618274211883545,
      "learning_rate": 0.0004926809785322018,
      "loss": 0.0211,
      "step": 1466
    },
    {
      "epoch": 0.7324013979031453,
      "grad_norm": 0.44570839405059814,
      "learning_rate": 0.0004926759860209686,
      "loss": 0.009,
      "step": 1467
    },
    {
      "epoch": 0.7329006490264603,
      "grad_norm": 0.077949658036232,
      "learning_rate": 0.0004926709935097355,
      "loss": 0.0018,
      "step": 1468
    },
    {
      "epoch": 0.7333999001497753,
      "grad_norm": 0.6082760691642761,
      "learning_rate": 0.0004926660009985023,
      "loss": 0.0168,
      "step": 1469
    },
    {
      "epoch": 0.7338991512730904,
      "grad_norm": 0.3272852301597595,
      "learning_rate": 0.0004926610084872691,
      "loss": 0.0043,
      "step": 1470
    },
    {
      "epoch": 0.7343984023964054,
      "grad_norm": 0.24667125940322876,
      "learning_rate": 0.000492656015976036,
      "loss": 0.0074,
      "step": 1471
    },
    {
      "epoch": 0.7348976535197205,
      "grad_norm": 0.2978999614715576,
      "learning_rate": 0.0004926510234648028,
      "loss": 0.0078,
      "step": 1472
    },
    {
      "epoch": 0.7353969046430354,
      "grad_norm": 0.29134950041770935,
      "learning_rate": 0.0004926460309535696,
      "loss": 0.0065,
      "step": 1473
    },
    {
      "epoch": 0.7358961557663505,
      "grad_norm": 0.09266620129346848,
      "learning_rate": 0.0004926410384423365,
      "loss": 0.0039,
      "step": 1474
    },
    {
      "epoch": 0.7363954068896655,
      "grad_norm": 0.17962361872196198,
      "learning_rate": 0.0004926360459311033,
      "loss": 0.0036,
      "step": 1475
    },
    {
      "epoch": 0.7368946580129805,
      "grad_norm": 0.07897142320871353,
      "learning_rate": 0.0004926310534198702,
      "loss": 0.0027,
      "step": 1476
    },
    {
      "epoch": 0.7373939091362955,
      "grad_norm": 0.1869717687368393,
      "learning_rate": 0.000492626060908637,
      "loss": 0.0052,
      "step": 1477
    },
    {
      "epoch": 0.7378931602596106,
      "grad_norm": 0.45059120655059814,
      "learning_rate": 0.0004926210683974039,
      "loss": 0.0115,
      "step": 1478
    },
    {
      "epoch": 0.7383924113829257,
      "grad_norm": 0.5009093880653381,
      "learning_rate": 0.0004926160758861707,
      "loss": 0.0139,
      "step": 1479
    },
    {
      "epoch": 0.7388916625062406,
      "grad_norm": 0.6261418461799622,
      "learning_rate": 0.0004926110833749376,
      "loss": 0.0083,
      "step": 1480
    },
    {
      "epoch": 0.7393909136295557,
      "grad_norm": 0.1377345770597458,
      "learning_rate": 0.0004926060908637044,
      "loss": 0.0036,
      "step": 1481
    },
    {
      "epoch": 0.7398901647528707,
      "grad_norm": 0.0854824110865593,
      "learning_rate": 0.0004926010983524713,
      "loss": 0.003,
      "step": 1482
    },
    {
      "epoch": 0.7403894158761857,
      "grad_norm": 0.2874651551246643,
      "learning_rate": 0.0004925961058412381,
      "loss": 0.0042,
      "step": 1483
    },
    {
      "epoch": 0.7408886669995007,
      "grad_norm": 0.26682111620903015,
      "learning_rate": 0.000492591113330005,
      "loss": 0.0099,
      "step": 1484
    },
    {
      "epoch": 0.7413879181228158,
      "grad_norm": 0.4766261875629425,
      "learning_rate": 0.0004925861208187718,
      "loss": 0.0364,
      "step": 1485
    },
    {
      "epoch": 0.7418871692461309,
      "grad_norm": 1.4406609535217285,
      "learning_rate": 0.0004925811283075387,
      "loss": 0.0067,
      "step": 1486
    },
    {
      "epoch": 0.7423864203694458,
      "grad_norm": 0.20051057636737823,
      "learning_rate": 0.0004925761357963055,
      "loss": 0.0043,
      "step": 1487
    },
    {
      "epoch": 0.7428856714927609,
      "grad_norm": 0.03813180327415466,
      "learning_rate": 0.0004925711432850724,
      "loss": 0.0019,
      "step": 1488
    },
    {
      "epoch": 0.7433849226160759,
      "grad_norm": 0.3480542302131653,
      "learning_rate": 0.0004925661507738392,
      "loss": 0.0073,
      "step": 1489
    },
    {
      "epoch": 0.7438841737393909,
      "grad_norm": 0.049629781395196915,
      "learning_rate": 0.0004925611582626061,
      "loss": 0.0023,
      "step": 1490
    },
    {
      "epoch": 0.7443834248627059,
      "grad_norm": 0.19914847612380981,
      "learning_rate": 0.0004925561657513729,
      "loss": 0.0031,
      "step": 1491
    },
    {
      "epoch": 0.744882675986021,
      "grad_norm": 0.07029037177562714,
      "learning_rate": 0.0004925511732401398,
      "loss": 0.0024,
      "step": 1492
    },
    {
      "epoch": 0.745381927109336,
      "grad_norm": 0.3758452236652374,
      "learning_rate": 0.0004925461807289066,
      "loss": 0.0098,
      "step": 1493
    },
    {
      "epoch": 0.745881178232651,
      "grad_norm": 0.5574888586997986,
      "learning_rate": 0.0004925411882176735,
      "loss": 0.0218,
      "step": 1494
    },
    {
      "epoch": 0.746380429355966,
      "grad_norm": 0.2861091196537018,
      "learning_rate": 0.0004925361957064403,
      "loss": 0.0035,
      "step": 1495
    },
    {
      "epoch": 0.7468796804792811,
      "grad_norm": 0.45547470450401306,
      "learning_rate": 0.0004925312031952072,
      "loss": 0.0093,
      "step": 1496
    },
    {
      "epoch": 0.747378931602596,
      "grad_norm": 0.5654256343841553,
      "learning_rate": 0.000492526210683974,
      "loss": 0.0114,
      "step": 1497
    },
    {
      "epoch": 0.7478781827259111,
      "grad_norm": 0.3170764148235321,
      "learning_rate": 0.0004925212181727409,
      "loss": 0.0062,
      "step": 1498
    },
    {
      "epoch": 0.7483774338492262,
      "grad_norm": 0.12021717429161072,
      "learning_rate": 0.0004925162256615077,
      "loss": 0.0032,
      "step": 1499
    },
    {
      "epoch": 0.7488766849725412,
      "grad_norm": 0.18048611283302307,
      "learning_rate": 0.0004925112331502745,
      "loss": 0.0056,
      "step": 1500
    },
    {
      "epoch": 0.7493759360958562,
      "grad_norm": 0.34019532799720764,
      "learning_rate": 0.0004925062406390414,
      "loss": 0.0085,
      "step": 1501
    },
    {
      "epoch": 0.7498751872191712,
      "grad_norm": 0.11496569216251373,
      "learning_rate": 0.0004925012481278082,
      "loss": 0.0029,
      "step": 1502
    },
    {
      "epoch": 0.7503744383424863,
      "grad_norm": 0.2726275324821472,
      "learning_rate": 0.0004924962556165751,
      "loss": 0.0232,
      "step": 1503
    },
    {
      "epoch": 0.7508736894658014,
      "grad_norm": 0.091518335044384,
      "learning_rate": 0.0004924912631053419,
      "loss": 0.0036,
      "step": 1504
    },
    {
      "epoch": 0.7513729405891163,
      "grad_norm": 0.09169477224349976,
      "learning_rate": 0.0004924862705941088,
      "loss": 0.0029,
      "step": 1505
    },
    {
      "epoch": 0.7518721917124314,
      "grad_norm": 0.4753320515155792,
      "learning_rate": 0.0004924812780828756,
      "loss": 0.0283,
      "step": 1506
    },
    {
      "epoch": 0.7523714428357464,
      "grad_norm": 0.39736682176589966,
      "learning_rate": 0.0004924762855716426,
      "loss": 0.0117,
      "step": 1507
    },
    {
      "epoch": 0.7528706939590614,
      "grad_norm": 0.24269019067287445,
      "learning_rate": 0.0004924712930604093,
      "loss": 0.0052,
      "step": 1508
    },
    {
      "epoch": 0.7533699450823764,
      "grad_norm": 0.7331138253211975,
      "learning_rate": 0.0004924663005491763,
      "loss": 0.0283,
      "step": 1509
    },
    {
      "epoch": 0.7538691962056915,
      "grad_norm": 0.08947716653347015,
      "learning_rate": 0.000492461308037943,
      "loss": 0.0029,
      "step": 1510
    },
    {
      "epoch": 0.7543684473290065,
      "grad_norm": 0.2244945615530014,
      "learning_rate": 0.00049245631552671,
      "loss": 0.0056,
      "step": 1511
    },
    {
      "epoch": 0.7548676984523215,
      "grad_norm": 0.3647836446762085,
      "learning_rate": 0.0004924513230154768,
      "loss": 0.004,
      "step": 1512
    },
    {
      "epoch": 0.7553669495756365,
      "grad_norm": 0.04306644946336746,
      "learning_rate": 0.0004924463305042437,
      "loss": 0.0021,
      "step": 1513
    },
    {
      "epoch": 0.7558662006989516,
      "grad_norm": 0.23339763283729553,
      "learning_rate": 0.0004924413379930105,
      "loss": 0.0073,
      "step": 1514
    },
    {
      "epoch": 0.7563654518222666,
      "grad_norm": 0.3699338436126709,
      "learning_rate": 0.0004924363454817774,
      "loss": 0.0114,
      "step": 1515
    },
    {
      "epoch": 0.7568647029455816,
      "grad_norm": 0.09665914624929428,
      "learning_rate": 0.0004924313529705442,
      "loss": 0.0026,
      "step": 1516
    },
    {
      "epoch": 0.7573639540688967,
      "grad_norm": 0.3496290147304535,
      "learning_rate": 0.0004924263604593111,
      "loss": 0.0036,
      "step": 1517
    },
    {
      "epoch": 0.7578632051922117,
      "grad_norm": 0.1671646535396576,
      "learning_rate": 0.0004924213679480779,
      "loss": 0.0036,
      "step": 1518
    },
    {
      "epoch": 0.7583624563155267,
      "grad_norm": 0.06401737779378891,
      "learning_rate": 0.0004924163754368448,
      "loss": 0.0025,
      "step": 1519
    },
    {
      "epoch": 0.7588617074388417,
      "grad_norm": 0.1408308744430542,
      "learning_rate": 0.0004924113829256116,
      "loss": 0.0029,
      "step": 1520
    },
    {
      "epoch": 0.7593609585621568,
      "grad_norm": 0.1624918282032013,
      "learning_rate": 0.0004924063904143785,
      "loss": 0.0029,
      "step": 1521
    },
    {
      "epoch": 0.7598602096854717,
      "grad_norm": 0.05937647819519043,
      "learning_rate": 0.0004924013979031453,
      "loss": 0.0018,
      "step": 1522
    },
    {
      "epoch": 0.7603594608087868,
      "grad_norm": 0.2903021574020386,
      "learning_rate": 0.0004923964053919122,
      "loss": 0.0041,
      "step": 1523
    },
    {
      "epoch": 0.7608587119321019,
      "grad_norm": 0.09994727373123169,
      "learning_rate": 0.000492391412880679,
      "loss": 0.0031,
      "step": 1524
    },
    {
      "epoch": 0.7613579630554169,
      "grad_norm": 0.1841491311788559,
      "learning_rate": 0.0004923864203694459,
      "loss": 0.005,
      "step": 1525
    },
    {
      "epoch": 0.7618572141787319,
      "grad_norm": 0.05093813315033913,
      "learning_rate": 0.0004923814278582127,
      "loss": 0.0022,
      "step": 1526
    },
    {
      "epoch": 0.7623564653020469,
      "grad_norm": 0.20976614952087402,
      "learning_rate": 0.0004923764353469796,
      "loss": 0.0027,
      "step": 1527
    },
    {
      "epoch": 0.762855716425362,
      "grad_norm": 0.1381218582391739,
      "learning_rate": 0.0004923714428357464,
      "loss": 0.0021,
      "step": 1528
    },
    {
      "epoch": 0.7633549675486769,
      "grad_norm": 0.1960756629705429,
      "learning_rate": 0.0004923664503245132,
      "loss": 0.0031,
      "step": 1529
    },
    {
      "epoch": 0.763854218671992,
      "grad_norm": 0.23195688426494598,
      "learning_rate": 0.0004923614578132801,
      "loss": 0.0031,
      "step": 1530
    },
    {
      "epoch": 0.764353469795307,
      "grad_norm": 0.22511115670204163,
      "learning_rate": 0.0004923564653020469,
      "loss": 0.0042,
      "step": 1531
    },
    {
      "epoch": 0.7648527209186221,
      "grad_norm": 0.11018300801515579,
      "learning_rate": 0.0004923514727908138,
      "loss": 0.0032,
      "step": 1532
    },
    {
      "epoch": 0.7653519720419371,
      "grad_norm": 0.361979603767395,
      "learning_rate": 0.0004923464802795806,
      "loss": 0.0107,
      "step": 1533
    },
    {
      "epoch": 0.7658512231652521,
      "grad_norm": 0.13188748061656952,
      "learning_rate": 0.0004923414877683475,
      "loss": 0.0023,
      "step": 1534
    },
    {
      "epoch": 0.7663504742885672,
      "grad_norm": 0.1396186351776123,
      "learning_rate": 0.0004923364952571143,
      "loss": 0.0031,
      "step": 1535
    },
    {
      "epoch": 0.7668497254118821,
      "grad_norm": 0.040429212152957916,
      "learning_rate": 0.0004923315027458812,
      "loss": 0.0014,
      "step": 1536
    },
    {
      "epoch": 0.7673489765351972,
      "grad_norm": 0.0700746402144432,
      "learning_rate": 0.000492326510234648,
      "loss": 0.0018,
      "step": 1537
    },
    {
      "epoch": 0.7678482276585122,
      "grad_norm": 0.4905523955821991,
      "learning_rate": 0.0004923215177234149,
      "loss": 0.011,
      "step": 1538
    },
    {
      "epoch": 0.7683474787818273,
      "grad_norm": 1.255816102027893,
      "learning_rate": 0.0004923165252121817,
      "loss": 0.0124,
      "step": 1539
    },
    {
      "epoch": 0.7688467299051422,
      "grad_norm": 1.7111098766326904,
      "learning_rate": 0.0004923115327009486,
      "loss": 0.0284,
      "step": 1540
    },
    {
      "epoch": 0.7693459810284573,
      "grad_norm": 0.07687972486019135,
      "learning_rate": 0.0004923065401897154,
      "loss": 0.0021,
      "step": 1541
    },
    {
      "epoch": 0.7698452321517724,
      "grad_norm": 0.04436441883444786,
      "learning_rate": 0.0004923015476784823,
      "loss": 0.001,
      "step": 1542
    },
    {
      "epoch": 0.7703444832750874,
      "grad_norm": 0.019436663016676903,
      "learning_rate": 0.0004922965551672491,
      "loss": 0.001,
      "step": 1543
    },
    {
      "epoch": 0.7708437343984024,
      "grad_norm": 0.06148063763976097,
      "learning_rate": 0.000492291562656016,
      "loss": 0.0015,
      "step": 1544
    },
    {
      "epoch": 0.7713429855217174,
      "grad_norm": 0.07040981948375702,
      "learning_rate": 0.0004922865701447828,
      "loss": 0.0017,
      "step": 1545
    },
    {
      "epoch": 0.7718422366450325,
      "grad_norm": 0.3919598162174225,
      "learning_rate": 0.0004922815776335497,
      "loss": 0.0048,
      "step": 1546
    },
    {
      "epoch": 0.7723414877683474,
      "grad_norm": 0.46689626574516296,
      "learning_rate": 0.0004922765851223165,
      "loss": 0.006,
      "step": 1547
    },
    {
      "epoch": 0.7728407388916625,
      "grad_norm": 0.4928366541862488,
      "learning_rate": 0.0004922715926110834,
      "loss": 0.0155,
      "step": 1548
    },
    {
      "epoch": 0.7733399900149776,
      "grad_norm": 0.7126153111457825,
      "learning_rate": 0.0004922666000998502,
      "loss": 0.0275,
      "step": 1549
    },
    {
      "epoch": 0.7738392411382926,
      "grad_norm": 0.10365103930234909,
      "learning_rate": 0.0004922616075886171,
      "loss": 0.0023,
      "step": 1550
    },
    {
      "epoch": 0.7743384922616076,
      "grad_norm": 0.3329245448112488,
      "learning_rate": 0.0004922566150773839,
      "loss": 0.0043,
      "step": 1551
    },
    {
      "epoch": 0.7748377433849226,
      "grad_norm": 0.20854537189006805,
      "learning_rate": 0.0004922516225661508,
      "loss": 0.0056,
      "step": 1552
    },
    {
      "epoch": 0.7753369945082377,
      "grad_norm": 0.5150021910667419,
      "learning_rate": 0.0004922466300549176,
      "loss": 0.0141,
      "step": 1553
    },
    {
      "epoch": 0.7758362456315526,
      "grad_norm": 0.4420446753501892,
      "learning_rate": 0.0004922416375436845,
      "loss": 0.0233,
      "step": 1554
    },
    {
      "epoch": 0.7763354967548677,
      "grad_norm": 0.21984300017356873,
      "learning_rate": 0.0004922366450324513,
      "loss": 0.0029,
      "step": 1555
    },
    {
      "epoch": 0.7768347478781827,
      "grad_norm": 0.5883976221084595,
      "learning_rate": 0.0004922316525212182,
      "loss": 0.0048,
      "step": 1556
    },
    {
      "epoch": 0.7773339990014978,
      "grad_norm": 0.31571757793426514,
      "learning_rate": 0.000492226660009985,
      "loss": 0.0094,
      "step": 1557
    },
    {
      "epoch": 0.7778332501248127,
      "grad_norm": 0.18345312774181366,
      "learning_rate": 0.0004922216674987518,
      "loss": 0.0036,
      "step": 1558
    },
    {
      "epoch": 0.7783325012481278,
      "grad_norm": 0.099317766726017,
      "learning_rate": 0.0004922166749875188,
      "loss": 0.0032,
      "step": 1559
    },
    {
      "epoch": 0.7788317523714429,
      "grad_norm": 0.7242030501365662,
      "learning_rate": 0.0004922116824762855,
      "loss": 0.0079,
      "step": 1560
    },
    {
      "epoch": 0.7793310034947578,
      "grad_norm": 1.1232333183288574,
      "learning_rate": 0.0004922066899650525,
      "loss": 0.0341,
      "step": 1561
    },
    {
      "epoch": 0.7798302546180729,
      "grad_norm": 0.5487960577011108,
      "learning_rate": 0.0004922016974538193,
      "loss": 0.0154,
      "step": 1562
    },
    {
      "epoch": 0.7803295057413879,
      "grad_norm": 0.8711362481117249,
      "learning_rate": 0.0004921967049425862,
      "loss": 0.0145,
      "step": 1563
    },
    {
      "epoch": 0.780828756864703,
      "grad_norm": 1.2002789974212646,
      "learning_rate": 0.000492191712431353,
      "loss": 0.0231,
      "step": 1564
    },
    {
      "epoch": 0.7813280079880179,
      "grad_norm": 1.240777611732483,
      "learning_rate": 0.0004921867199201199,
      "loss": 0.0184,
      "step": 1565
    },
    {
      "epoch": 0.781827259111333,
      "grad_norm": 0.4511352479457855,
      "learning_rate": 0.0004921817274088867,
      "loss": 0.0259,
      "step": 1566
    },
    {
      "epoch": 0.782326510234648,
      "grad_norm": 0.43404364585876465,
      "learning_rate": 0.0004921767348976536,
      "loss": 0.0092,
      "step": 1567
    },
    {
      "epoch": 0.782825761357963,
      "grad_norm": 0.9097402095794678,
      "learning_rate": 0.0004921717423864204,
      "loss": 0.0264,
      "step": 1568
    },
    {
      "epoch": 0.7833250124812781,
      "grad_norm": 0.25130489468574524,
      "learning_rate": 0.0004921667498751873,
      "loss": 0.0061,
      "step": 1569
    },
    {
      "epoch": 0.7838242636045931,
      "grad_norm": 0.07526345551013947,
      "learning_rate": 0.0004921617573639541,
      "loss": 0.0028,
      "step": 1570
    },
    {
      "epoch": 0.7843235147279082,
      "grad_norm": 0.5449271202087402,
      "learning_rate": 0.000492156764852721,
      "loss": 0.0213,
      "step": 1571
    },
    {
      "epoch": 0.7848227658512231,
      "grad_norm": 0.7382591366767883,
      "learning_rate": 0.0004921517723414878,
      "loss": 0.0135,
      "step": 1572
    },
    {
      "epoch": 0.7853220169745382,
      "grad_norm": 0.9734811782836914,
      "learning_rate": 0.0004921467798302547,
      "loss": 0.032,
      "step": 1573
    },
    {
      "epoch": 0.7858212680978532,
      "grad_norm": 0.5387551784515381,
      "learning_rate": 0.0004921417873190215,
      "loss": 0.0271,
      "step": 1574
    },
    {
      "epoch": 0.7863205192211683,
      "grad_norm": 0.5820945501327515,
      "learning_rate": 0.0004921367948077884,
      "loss": 0.0124,
      "step": 1575
    },
    {
      "epoch": 0.7868197703444832,
      "grad_norm": 0.3636167049407959,
      "learning_rate": 0.0004921318022965552,
      "loss": 0.0079,
      "step": 1576
    },
    {
      "epoch": 0.7873190214677983,
      "grad_norm": 0.7030355930328369,
      "learning_rate": 0.0004921268097853221,
      "loss": 0.0373,
      "step": 1577
    },
    {
      "epoch": 0.7878182725911134,
      "grad_norm": 0.7130012512207031,
      "learning_rate": 0.0004921218172740889,
      "loss": 0.0157,
      "step": 1578
    },
    {
      "epoch": 0.7883175237144283,
      "grad_norm": 0.9382830262184143,
      "learning_rate": 0.0004921168247628558,
      "loss": 0.029,
      "step": 1579
    },
    {
      "epoch": 0.7888167748377434,
      "grad_norm": 0.1728566288948059,
      "learning_rate": 0.0004921118322516226,
      "loss": 0.0049,
      "step": 1580
    },
    {
      "epoch": 0.7893160259610584,
      "grad_norm": 0.42289501428604126,
      "learning_rate": 0.0004921068397403895,
      "loss": 0.0105,
      "step": 1581
    },
    {
      "epoch": 0.7898152770843735,
      "grad_norm": 0.16851398348808289,
      "learning_rate": 0.0004921018472291563,
      "loss": 0.0063,
      "step": 1582
    },
    {
      "epoch": 0.7903145282076884,
      "grad_norm": 0.13228179514408112,
      "learning_rate": 0.0004920968547179232,
      "loss": 0.0059,
      "step": 1583
    },
    {
      "epoch": 0.7908137793310035,
      "grad_norm": 0.1259194314479828,
      "learning_rate": 0.00049209186220669,
      "loss": 0.0032,
      "step": 1584
    },
    {
      "epoch": 0.7913130304543186,
      "grad_norm": 0.632163405418396,
      "learning_rate": 0.0004920868696954569,
      "loss": 0.018,
      "step": 1585
    },
    {
      "epoch": 0.7918122815776335,
      "grad_norm": 0.17062529921531677,
      "learning_rate": 0.0004920818771842236,
      "loss": 0.0049,
      "step": 1586
    },
    {
      "epoch": 0.7923115327009486,
      "grad_norm": 0.7863314747810364,
      "learning_rate": 0.0004920768846729905,
      "loss": 0.0284,
      "step": 1587
    },
    {
      "epoch": 0.7928107838242636,
      "grad_norm": 0.2281436026096344,
      "learning_rate": 0.0004920718921617573,
      "loss": 0.0062,
      "step": 1588
    },
    {
      "epoch": 0.7933100349475787,
      "grad_norm": 0.24963591992855072,
      "learning_rate": 0.0004920668996505242,
      "loss": 0.0057,
      "step": 1589
    },
    {
      "epoch": 0.7938092860708936,
      "grad_norm": 2.640181303024292,
      "learning_rate": 0.000492061907139291,
      "loss": 0.0373,
      "step": 1590
    },
    {
      "epoch": 0.7943085371942087,
      "grad_norm": 0.482455849647522,
      "learning_rate": 0.0004920569146280579,
      "loss": 0.009,
      "step": 1591
    },
    {
      "epoch": 0.7948077883175237,
      "grad_norm": 0.6505140662193298,
      "learning_rate": 0.0004920519221168247,
      "loss": 0.0092,
      "step": 1592
    },
    {
      "epoch": 0.7953070394408387,
      "grad_norm": 0.3283633887767792,
      "learning_rate": 0.0004920469296055916,
      "loss": 0.0179,
      "step": 1593
    },
    {
      "epoch": 0.7958062905641538,
      "grad_norm": 0.6283003687858582,
      "learning_rate": 0.0004920419370943584,
      "loss": 0.0238,
      "step": 1594
    },
    {
      "epoch": 0.7963055416874688,
      "grad_norm": 0.1716597080230713,
      "learning_rate": 0.0004920369445831253,
      "loss": 0.0029,
      "step": 1595
    },
    {
      "epoch": 0.7968047928107839,
      "grad_norm": 0.2210550755262375,
      "learning_rate": 0.0004920319520718921,
      "loss": 0.0094,
      "step": 1596
    },
    {
      "epoch": 0.7973040439340988,
      "grad_norm": 0.31675729155540466,
      "learning_rate": 0.000492026959560659,
      "loss": 0.0126,
      "step": 1597
    },
    {
      "epoch": 0.7978032950574139,
      "grad_norm": 1.3267632722854614,
      "learning_rate": 0.0004920219670494258,
      "loss": 0.0166,
      "step": 1598
    },
    {
      "epoch": 0.7983025461807289,
      "grad_norm": 0.5036844611167908,
      "learning_rate": 0.0004920169745381927,
      "loss": 0.0078,
      "step": 1599
    },
    {
      "epoch": 0.7988017973040439,
      "grad_norm": 0.04887102544307709,
      "learning_rate": 0.0004920119820269595,
      "loss": 0.0024,
      "step": 1600
    },
    {
      "epoch": 0.7993010484273589,
      "grad_norm": 1.647603154182434,
      "learning_rate": 0.0004920069895157264,
      "loss": 0.0176,
      "step": 1601
    },
    {
      "epoch": 0.799800299550674,
      "grad_norm": 0.1672368198633194,
      "learning_rate": 0.0004920019970044932,
      "loss": 0.0045,
      "step": 1602
    },
    {
      "epoch": 0.8002995506739891,
      "grad_norm": 0.05276023969054222,
      "learning_rate": 0.0004919970044932601,
      "loss": 0.0024,
      "step": 1603
    },
    {
      "epoch": 0.800798801797304,
      "grad_norm": 0.24494530260562897,
      "learning_rate": 0.0004919920119820269,
      "loss": 0.0084,
      "step": 1604
    },
    {
      "epoch": 0.8012980529206191,
      "grad_norm": 0.27270007133483887,
      "learning_rate": 0.0004919870194707938,
      "loss": 0.008,
      "step": 1605
    },
    {
      "epoch": 0.8017973040439341,
      "grad_norm": 0.5755274295806885,
      "learning_rate": 0.0004919820269595606,
      "loss": 0.0097,
      "step": 1606
    },
    {
      "epoch": 0.8022965551672492,
      "grad_norm": 0.9322693347930908,
      "learning_rate": 0.0004919770344483275,
      "loss": 0.0221,
      "step": 1607
    },
    {
      "epoch": 0.8027958062905641,
      "grad_norm": 2.134519100189209,
      "learning_rate": 0.0004919720419370943,
      "loss": 0.0518,
      "step": 1608
    },
    {
      "epoch": 0.8032950574138792,
      "grad_norm": 0.8630651831626892,
      "learning_rate": 0.0004919670494258612,
      "loss": 0.0064,
      "step": 1609
    },
    {
      "epoch": 0.8037943085371942,
      "grad_norm": 0.5306274890899658,
      "learning_rate": 0.000491962056914628,
      "loss": 0.0126,
      "step": 1610
    },
    {
      "epoch": 0.8042935596605092,
      "grad_norm": 1.2593188285827637,
      "learning_rate": 0.000491957064403395,
      "loss": 0.0197,
      "step": 1611
    },
    {
      "epoch": 0.8047928107838243,
      "grad_norm": 0.6866798400878906,
      "learning_rate": 0.0004919520718921617,
      "loss": 0.0085,
      "step": 1612
    },
    {
      "epoch": 0.8052920619071393,
      "grad_norm": 2.7363343238830566,
      "learning_rate": 0.0004919470793809287,
      "loss": 0.0177,
      "step": 1613
    },
    {
      "epoch": 0.8057913130304544,
      "grad_norm": 0.8105047941207886,
      "learning_rate": 0.0004919420868696955,
      "loss": 0.0265,
      "step": 1614
    },
    {
      "epoch": 0.8062905641537693,
      "grad_norm": 0.6075044274330139,
      "learning_rate": 0.0004919370943584622,
      "loss": 0.0087,
      "step": 1615
    },
    {
      "epoch": 0.8067898152770844,
      "grad_norm": 0.3053353428840637,
      "learning_rate": 0.0004919321018472292,
      "loss": 0.0101,
      "step": 1616
    },
    {
      "epoch": 0.8072890664003994,
      "grad_norm": 1.8174582719802856,
      "learning_rate": 0.000491927109335996,
      "loss": 0.0449,
      "step": 1617
    },
    {
      "epoch": 0.8077883175237144,
      "grad_norm": 0.8787373304367065,
      "learning_rate": 0.0004919221168247629,
      "loss": 0.0126,
      "step": 1618
    },
    {
      "epoch": 0.8082875686470294,
      "grad_norm": 0.5483649373054504,
      "learning_rate": 0.0004919171243135297,
      "loss": 0.0138,
      "step": 1619
    },
    {
      "epoch": 0.8087868197703445,
      "grad_norm": 1.1594312191009521,
      "learning_rate": 0.0004919121318022966,
      "loss": 0.0204,
      "step": 1620
    },
    {
      "epoch": 0.8092860708936596,
      "grad_norm": 0.23950685560703278,
      "learning_rate": 0.0004919071392910634,
      "loss": 0.0078,
      "step": 1621
    },
    {
      "epoch": 0.8097853220169745,
      "grad_norm": 0.5303860306739807,
      "learning_rate": 0.0004919021467798303,
      "loss": 0.0088,
      "step": 1622
    },
    {
      "epoch": 0.8102845731402896,
      "grad_norm": 0.2645626664161682,
      "learning_rate": 0.0004918971542685971,
      "loss": 0.0053,
      "step": 1623
    },
    {
      "epoch": 0.8107838242636046,
      "grad_norm": 0.09624987840652466,
      "learning_rate": 0.000491892161757364,
      "loss": 0.0038,
      "step": 1624
    },
    {
      "epoch": 0.8112830753869196,
      "grad_norm": 1.2378222942352295,
      "learning_rate": 0.0004918871692461308,
      "loss": 0.0468,
      "step": 1625
    },
    {
      "epoch": 0.8117823265102346,
      "grad_norm": 0.7218626737594604,
      "learning_rate": 0.0004918821767348977,
      "loss": 0.0131,
      "step": 1626
    },
    {
      "epoch": 0.8122815776335497,
      "grad_norm": 0.13092514872550964,
      "learning_rate": 0.0004918771842236645,
      "loss": 0.0057,
      "step": 1627
    },
    {
      "epoch": 0.8127808287568647,
      "grad_norm": 1.7029918432235718,
      "learning_rate": 0.0004918721917124314,
      "loss": 0.0131,
      "step": 1628
    },
    {
      "epoch": 0.8132800798801797,
      "grad_norm": 0.5723205804824829,
      "learning_rate": 0.0004918671992011982,
      "loss": 0.0108,
      "step": 1629
    },
    {
      "epoch": 0.8137793310034948,
      "grad_norm": 1.1914215087890625,
      "learning_rate": 0.0004918622066899651,
      "loss": 0.0192,
      "step": 1630
    },
    {
      "epoch": 0.8142785821268098,
      "grad_norm": 1.5227584838867188,
      "learning_rate": 0.0004918572141787319,
      "loss": 0.0349,
      "step": 1631
    },
    {
      "epoch": 0.8147778332501248,
      "grad_norm": 0.1870008409023285,
      "learning_rate": 0.0004918522216674988,
      "loss": 0.0053,
      "step": 1632
    },
    {
      "epoch": 0.8152770843734398,
      "grad_norm": 0.6808384656906128,
      "learning_rate": 0.0004918472291562656,
      "loss": 0.0158,
      "step": 1633
    },
    {
      "epoch": 0.8157763354967549,
      "grad_norm": 0.49073413014411926,
      "learning_rate": 0.0004918422366450325,
      "loss": 0.0159,
      "step": 1634
    },
    {
      "epoch": 0.8162755866200699,
      "grad_norm": 0.3029002249240875,
      "learning_rate": 0.0004918372441337993,
      "loss": 0.009,
      "step": 1635
    },
    {
      "epoch": 0.8167748377433849,
      "grad_norm": 0.29698020219802856,
      "learning_rate": 0.0004918322516225662,
      "loss": 0.006,
      "step": 1636
    },
    {
      "epoch": 0.8172740888666999,
      "grad_norm": 0.3106265962123871,
      "learning_rate": 0.000491827259111333,
      "loss": 0.0126,
      "step": 1637
    },
    {
      "epoch": 0.817773339990015,
      "grad_norm": 0.42066702246665955,
      "learning_rate": 0.0004918222666000999,
      "loss": 0.0092,
      "step": 1638
    },
    {
      "epoch": 0.81827259111333,
      "grad_norm": 0.32236433029174805,
      "learning_rate": 0.0004918172740888667,
      "loss": 0.0089,
      "step": 1639
    },
    {
      "epoch": 0.818771842236645,
      "grad_norm": 0.08501026034355164,
      "learning_rate": 0.0004918122815776336,
      "loss": 0.0027,
      "step": 1640
    },
    {
      "epoch": 0.8192710933599601,
      "grad_norm": 0.07999086380004883,
      "learning_rate": 0.0004918072890664004,
      "loss": 0.0025,
      "step": 1641
    },
    {
      "epoch": 0.8197703444832751,
      "grad_norm": 0.3285764753818512,
      "learning_rate": 0.0004918022965551673,
      "loss": 0.0067,
      "step": 1642
    },
    {
      "epoch": 0.8202695956065901,
      "grad_norm": 0.4456176161766052,
      "learning_rate": 0.0004917973040439341,
      "loss": 0.0105,
      "step": 1643
    },
    {
      "epoch": 0.8207688467299051,
      "grad_norm": 0.5270269513130188,
      "learning_rate": 0.0004917923115327009,
      "loss": 0.0058,
      "step": 1644
    },
    {
      "epoch": 0.8212680978532202,
      "grad_norm": 6.446621894836426,
      "learning_rate": 0.0004917873190214678,
      "loss": 0.0278,
      "step": 1645
    },
    {
      "epoch": 0.8217673489765353,
      "grad_norm": 0.35080409049987793,
      "learning_rate": 0.0004917823265102346,
      "loss": 0.0095,
      "step": 1646
    },
    {
      "epoch": 0.8222666000998502,
      "grad_norm": 0.09199792891740799,
      "learning_rate": 0.0004917773339990015,
      "loss": 0.0032,
      "step": 1647
    },
    {
      "epoch": 0.8227658512231653,
      "grad_norm": 0.9801152348518372,
      "learning_rate": 0.0004917723414877683,
      "loss": 0.0554,
      "step": 1648
    },
    {
      "epoch": 0.8232651023464803,
      "grad_norm": 1.1883814334869385,
      "learning_rate": 0.0004917673489765352,
      "loss": 0.0226,
      "step": 1649
    },
    {
      "epoch": 0.8237643534697953,
      "grad_norm": 0.6067398190498352,
      "learning_rate": 0.000491762356465302,
      "loss": 0.0103,
      "step": 1650
    },
    {
      "epoch": 0.8242636045931103,
      "grad_norm": 0.16352394223213196,
      "learning_rate": 0.0004917573639540689,
      "loss": 0.0037,
      "step": 1651
    },
    {
      "epoch": 0.8247628557164254,
      "grad_norm": 0.5848075151443481,
      "learning_rate": 0.0004917523714428357,
      "loss": 0.006,
      "step": 1652
    },
    {
      "epoch": 0.8252621068397404,
      "grad_norm": 0.1383104771375656,
      "learning_rate": 0.0004917473789316026,
      "loss": 0.0029,
      "step": 1653
    },
    {
      "epoch": 0.8257613579630554,
      "grad_norm": 0.1586102694272995,
      "learning_rate": 0.0004917423864203694,
      "loss": 0.0047,
      "step": 1654
    },
    {
      "epoch": 0.8262606090863704,
      "grad_norm": 0.4049507677555084,
      "learning_rate": 0.0004917373939091363,
      "loss": 0.0095,
      "step": 1655
    },
    {
      "epoch": 0.8267598602096855,
      "grad_norm": 1.9870877265930176,
      "learning_rate": 0.0004917324013979031,
      "loss": 0.0199,
      "step": 1656
    },
    {
      "epoch": 0.8272591113330005,
      "grad_norm": 0.1161513403058052,
      "learning_rate": 0.00049172740888667,
      "loss": 0.0037,
      "step": 1657
    },
    {
      "epoch": 0.8277583624563155,
      "grad_norm": 0.309747576713562,
      "learning_rate": 0.0004917224163754368,
      "loss": 0.0102,
      "step": 1658
    },
    {
      "epoch": 0.8282576135796306,
      "grad_norm": 0.1918814331293106,
      "learning_rate": 0.0004917174238642037,
      "loss": 0.0045,
      "step": 1659
    },
    {
      "epoch": 0.8287568647029456,
      "grad_norm": 0.6254539489746094,
      "learning_rate": 0.0004917124313529705,
      "loss": 0.0094,
      "step": 1660
    },
    {
      "epoch": 0.8292561158262606,
      "grad_norm": 0.9897757172584534,
      "learning_rate": 0.0004917074388417374,
      "loss": 0.0078,
      "step": 1661
    },
    {
      "epoch": 0.8297553669495756,
      "grad_norm": 0.5808922648429871,
      "learning_rate": 0.0004917024463305042,
      "loss": 0.0284,
      "step": 1662
    },
    {
      "epoch": 0.8302546180728907,
      "grad_norm": 0.5880704522132874,
      "learning_rate": 0.0004916974538192711,
      "loss": 0.0155,
      "step": 1663
    },
    {
      "epoch": 0.8307538691962056,
      "grad_norm": 1.414562463760376,
      "learning_rate": 0.000491692461308038,
      "loss": 0.0095,
      "step": 1664
    },
    {
      "epoch": 0.8312531203195207,
      "grad_norm": 0.839370608329773,
      "learning_rate": 0.0004916874687968049,
      "loss": 0.0122,
      "step": 1665
    },
    {
      "epoch": 0.8317523714428358,
      "grad_norm": 0.49213093519210815,
      "learning_rate": 0.0004916824762855717,
      "loss": 0.0124,
      "step": 1666
    },
    {
      "epoch": 0.8322516225661508,
      "grad_norm": 0.23953713476657867,
      "learning_rate": 0.0004916774837743386,
      "loss": 0.0061,
      "step": 1667
    },
    {
      "epoch": 0.8327508736894658,
      "grad_norm": 0.804486095905304,
      "learning_rate": 0.0004916724912631054,
      "loss": 0.0247,
      "step": 1668
    },
    {
      "epoch": 0.8332501248127808,
      "grad_norm": 0.45755472779273987,
      "learning_rate": 0.0004916674987518723,
      "loss": 0.0119,
      "step": 1669
    },
    {
      "epoch": 0.8337493759360959,
      "grad_norm": 0.7554193735122681,
      "learning_rate": 0.0004916625062406391,
      "loss": 0.0091,
      "step": 1670
    },
    {
      "epoch": 0.8342486270594108,
      "grad_norm": 0.7299069166183472,
      "learning_rate": 0.000491657513729406,
      "loss": 0.026,
      "step": 1671
    },
    {
      "epoch": 0.8347478781827259,
      "grad_norm": 0.9199533462524414,
      "learning_rate": 0.0004916525212181728,
      "loss": 0.0425,
      "step": 1672
    },
    {
      "epoch": 0.835247129306041,
      "grad_norm": 2.3333823680877686,
      "learning_rate": 0.0004916475287069397,
      "loss": 0.0479,
      "step": 1673
    },
    {
      "epoch": 0.835746380429356,
      "grad_norm": 0.08169230073690414,
      "learning_rate": 0.0004916425361957065,
      "loss": 0.0022,
      "step": 1674
    },
    {
      "epoch": 0.836245631552671,
      "grad_norm": 0.7512982487678528,
      "learning_rate": 0.0004916375436844733,
      "loss": 0.0424,
      "step": 1675
    },
    {
      "epoch": 0.836744882675986,
      "grad_norm": 0.610359251499176,
      "learning_rate": 0.0004916325511732402,
      "loss": 0.0163,
      "step": 1676
    },
    {
      "epoch": 0.8372441337993011,
      "grad_norm": 0.3783074915409088,
      "learning_rate": 0.000491627558662007,
      "loss": 0.0051,
      "step": 1677
    },
    {
      "epoch": 0.8377433849226161,
      "grad_norm": 0.2500644326210022,
      "learning_rate": 0.0004916225661507739,
      "loss": 0.0089,
      "step": 1678
    },
    {
      "epoch": 0.8382426360459311,
      "grad_norm": 0.18037976324558258,
      "learning_rate": 0.0004916175736395407,
      "loss": 0.0064,
      "step": 1679
    },
    {
      "epoch": 0.8387418871692461,
      "grad_norm": 0.3888242542743683,
      "learning_rate": 0.0004916125811283076,
      "loss": 0.0157,
      "step": 1680
    },
    {
      "epoch": 0.8392411382925612,
      "grad_norm": 0.34097060561180115,
      "learning_rate": 0.0004916075886170744,
      "loss": 0.0144,
      "step": 1681
    },
    {
      "epoch": 0.8397403894158761,
      "grad_norm": 0.09607994556427002,
      "learning_rate": 0.0004916025961058413,
      "loss": 0.0043,
      "step": 1682
    },
    {
      "epoch": 0.8402396405391912,
      "grad_norm": 0.3867245018482208,
      "learning_rate": 0.0004915976035946081,
      "loss": 0.0161,
      "step": 1683
    },
    {
      "epoch": 0.8407388916625063,
      "grad_norm": 0.43234774470329285,
      "learning_rate": 0.000491592611083375,
      "loss": 0.0174,
      "step": 1684
    },
    {
      "epoch": 0.8412381427858213,
      "grad_norm": 0.6490892767906189,
      "learning_rate": 0.0004915876185721418,
      "loss": 0.0369,
      "step": 1685
    },
    {
      "epoch": 0.8417373939091363,
      "grad_norm": 0.3568906784057617,
      "learning_rate": 0.0004915826260609087,
      "loss": 0.0078,
      "step": 1686
    },
    {
      "epoch": 0.8422366450324513,
      "grad_norm": 0.14922016859054565,
      "learning_rate": 0.0004915776335496755,
      "loss": 0.0046,
      "step": 1687
    },
    {
      "epoch": 0.8427358961557664,
      "grad_norm": 0.7179955840110779,
      "learning_rate": 0.0004915726410384424,
      "loss": 0.0132,
      "step": 1688
    },
    {
      "epoch": 0.8432351472790813,
      "grad_norm": 0.3532819449901581,
      "learning_rate": 0.0004915676485272092,
      "loss": 0.0117,
      "step": 1689
    },
    {
      "epoch": 0.8437343984023964,
      "grad_norm": 0.29906877875328064,
      "learning_rate": 0.0004915626560159761,
      "loss": 0.0055,
      "step": 1690
    },
    {
      "epoch": 0.8442336495257114,
      "grad_norm": 0.34759214520454407,
      "learning_rate": 0.0004915576635047429,
      "loss": 0.0063,
      "step": 1691
    },
    {
      "epoch": 0.8447329006490265,
      "grad_norm": 0.3003069758415222,
      "learning_rate": 0.0004915526709935098,
      "loss": 0.0056,
      "step": 1692
    },
    {
      "epoch": 0.8452321517723415,
      "grad_norm": 0.03581792116165161,
      "learning_rate": 0.0004915476784822766,
      "loss": 0.0024,
      "step": 1693
    },
    {
      "epoch": 0.8457314028956565,
      "grad_norm": 0.13151974976062775,
      "learning_rate": 0.0004915426859710435,
      "loss": 0.004,
      "step": 1694
    },
    {
      "epoch": 0.8462306540189716,
      "grad_norm": 0.559418261051178,
      "learning_rate": 0.0004915376934598103,
      "loss": 0.0108,
      "step": 1695
    },
    {
      "epoch": 0.8467299051422865,
      "grad_norm": 0.060435958206653595,
      "learning_rate": 0.0004915327009485772,
      "loss": 0.0024,
      "step": 1696
    },
    {
      "epoch": 0.8472291562656016,
      "grad_norm": 0.31406697630882263,
      "learning_rate": 0.000491527708437344,
      "loss": 0.0084,
      "step": 1697
    },
    {
      "epoch": 0.8477284073889166,
      "grad_norm": 0.7852768898010254,
      "learning_rate": 0.0004915227159261109,
      "loss": 0.0175,
      "step": 1698
    },
    {
      "epoch": 0.8482276585122317,
      "grad_norm": 0.789539098739624,
      "learning_rate": 0.0004915177234148777,
      "loss": 0.022,
      "step": 1699
    },
    {
      "epoch": 0.8487269096355466,
      "grad_norm": 0.43983688950538635,
      "learning_rate": 0.0004915127309036446,
      "loss": 0.0093,
      "step": 1700
    },
    {
      "epoch": 0.8492261607588617,
      "grad_norm": 0.22298093140125275,
      "learning_rate": 0.0004915077383924114,
      "loss": 0.0046,
      "step": 1701
    },
    {
      "epoch": 0.8497254118821768,
      "grad_norm": 0.24599769711494446,
      "learning_rate": 0.0004915027458811782,
      "loss": 0.0057,
      "step": 1702
    },
    {
      "epoch": 0.8502246630054917,
      "grad_norm": 0.6446468830108643,
      "learning_rate": 0.000491497753369945,
      "loss": 0.0135,
      "step": 1703
    },
    {
      "epoch": 0.8507239141288068,
      "grad_norm": 0.3385158181190491,
      "learning_rate": 0.0004914927608587119,
      "loss": 0.0087,
      "step": 1704
    },
    {
      "epoch": 0.8512231652521218,
      "grad_norm": 0.5611372590065002,
      "learning_rate": 0.0004914877683474787,
      "loss": 0.0135,
      "step": 1705
    },
    {
      "epoch": 0.8517224163754369,
      "grad_norm": 0.11208140850067139,
      "learning_rate": 0.0004914827758362456,
      "loss": 0.0027,
      "step": 1706
    },
    {
      "epoch": 0.8522216674987518,
      "grad_norm": 0.8186293244361877,
      "learning_rate": 0.0004914777833250124,
      "loss": 0.0173,
      "step": 1707
    },
    {
      "epoch": 0.8527209186220669,
      "grad_norm": 0.30246177315711975,
      "learning_rate": 0.0004914727908137793,
      "loss": 0.0051,
      "step": 1708
    },
    {
      "epoch": 0.853220169745382,
      "grad_norm": 0.609215497970581,
      "learning_rate": 0.0004914677983025461,
      "loss": 0.0106,
      "step": 1709
    },
    {
      "epoch": 0.853719420868697,
      "grad_norm": 0.660636842250824,
      "learning_rate": 0.000491462805791313,
      "loss": 0.0136,
      "step": 1710
    },
    {
      "epoch": 0.854218671992012,
      "grad_norm": 0.08068174123764038,
      "learning_rate": 0.0004914578132800798,
      "loss": 0.0025,
      "step": 1711
    },
    {
      "epoch": 0.854717923115327,
      "grad_norm": 0.47588589787483215,
      "learning_rate": 0.0004914528207688467,
      "loss": 0.0088,
      "step": 1712
    },
    {
      "epoch": 0.8552171742386421,
      "grad_norm": 0.13927437365055084,
      "learning_rate": 0.0004914478282576135,
      "loss": 0.0038,
      "step": 1713
    },
    {
      "epoch": 0.855716425361957,
      "grad_norm": 0.24439764022827148,
      "learning_rate": 0.0004914428357463804,
      "loss": 0.0076,
      "step": 1714
    },
    {
      "epoch": 0.8562156764852721,
      "grad_norm": 0.102310910820961,
      "learning_rate": 0.0004914378432351472,
      "loss": 0.0042,
      "step": 1715
    },
    {
      "epoch": 0.8567149276085871,
      "grad_norm": 2.252645969390869,
      "learning_rate": 0.0004914328507239141,
      "loss": 0.0225,
      "step": 1716
    },
    {
      "epoch": 0.8572141787319022,
      "grad_norm": 0.05702558159828186,
      "learning_rate": 0.0004914278582126809,
      "loss": 0.003,
      "step": 1717
    },
    {
      "epoch": 0.8577134298552171,
      "grad_norm": 0.8593430519104004,
      "learning_rate": 0.0004914228657014478,
      "loss": 0.0074,
      "step": 1718
    },
    {
      "epoch": 0.8582126809785322,
      "grad_norm": 0.5926996469497681,
      "learning_rate": 0.0004914178731902146,
      "loss": 0.0102,
      "step": 1719
    },
    {
      "epoch": 0.8587119321018473,
      "grad_norm": 1.914893388748169,
      "learning_rate": 0.0004914128806789816,
      "loss": 0.0246,
      "step": 1720
    },
    {
      "epoch": 0.8592111832251622,
      "grad_norm": 0.6692336201667786,
      "learning_rate": 0.0004914078881677484,
      "loss": 0.0053,
      "step": 1721
    },
    {
      "epoch": 0.8597104343484773,
      "grad_norm": 0.7278841137886047,
      "learning_rate": 0.0004914028956565153,
      "loss": 0.046,
      "step": 1722
    },
    {
      "epoch": 0.8602096854717923,
      "grad_norm": 0.3711601495742798,
      "learning_rate": 0.0004913979031452821,
      "loss": 0.0156,
      "step": 1723
    },
    {
      "epoch": 0.8607089365951074,
      "grad_norm": 0.1900373101234436,
      "learning_rate": 0.000491392910634049,
      "loss": 0.0068,
      "step": 1724
    },
    {
      "epoch": 0.8612081877184223,
      "grad_norm": 1.095702886581421,
      "learning_rate": 0.0004913879181228158,
      "loss": 0.0325,
      "step": 1725
    },
    {
      "epoch": 0.8617074388417374,
      "grad_norm": 0.5370578765869141,
      "learning_rate": 0.0004913829256115827,
      "loss": 0.0156,
      "step": 1726
    },
    {
      "epoch": 0.8622066899650525,
      "grad_norm": 0.17729870975017548,
      "learning_rate": 0.0004913779331003495,
      "loss": 0.0045,
      "step": 1727
    },
    {
      "epoch": 0.8627059410883674,
      "grad_norm": 0.36235281825065613,
      "learning_rate": 0.0004913729405891164,
      "loss": 0.007,
      "step": 1728
    },
    {
      "epoch": 0.8632051922116825,
      "grad_norm": 0.1198195070028305,
      "learning_rate": 0.0004913679480778832,
      "loss": 0.0043,
      "step": 1729
    },
    {
      "epoch": 0.8637044433349975,
      "grad_norm": 0.3804057240486145,
      "learning_rate": 0.0004913629555666501,
      "loss": 0.0203,
      "step": 1730
    },
    {
      "epoch": 0.8642036944583126,
      "grad_norm": 0.5871627926826477,
      "learning_rate": 0.0004913579630554169,
      "loss": 0.0228,
      "step": 1731
    },
    {
      "epoch": 0.8647029455816275,
      "grad_norm": 0.6341662406921387,
      "learning_rate": 0.0004913529705441837,
      "loss": 0.0267,
      "step": 1732
    },
    {
      "epoch": 0.8652021967049426,
      "grad_norm": 0.7289630770683289,
      "learning_rate": 0.0004913479780329506,
      "loss": 0.019,
      "step": 1733
    },
    {
      "epoch": 0.8657014478282576,
      "grad_norm": 0.2385701835155487,
      "learning_rate": 0.0004913429855217174,
      "loss": 0.0288,
      "step": 1734
    },
    {
      "epoch": 0.8662006989515726,
      "grad_norm": 0.3296016752719879,
      "learning_rate": 0.0004913379930104843,
      "loss": 0.0105,
      "step": 1735
    },
    {
      "epoch": 0.8666999500748876,
      "grad_norm": 0.33726072311401367,
      "learning_rate": 0.0004913330004992511,
      "loss": 0.0117,
      "step": 1736
    },
    {
      "epoch": 0.8671992011982027,
      "grad_norm": 0.18866600096225739,
      "learning_rate": 0.000491328007988018,
      "loss": 0.0047,
      "step": 1737
    },
    {
      "epoch": 0.8676984523215178,
      "grad_norm": 0.24341949820518494,
      "learning_rate": 0.0004913230154767848,
      "loss": 0.0228,
      "step": 1738
    },
    {
      "epoch": 0.8681977034448327,
      "grad_norm": 0.08985166996717453,
      "learning_rate": 0.0004913180229655517,
      "loss": 0.0033,
      "step": 1739
    },
    {
      "epoch": 0.8686969545681478,
      "grad_norm": 0.08897185325622559,
      "learning_rate": 0.0004913130304543185,
      "loss": 0.0036,
      "step": 1740
    },
    {
      "epoch": 0.8691962056914628,
      "grad_norm": 0.10581333935260773,
      "learning_rate": 0.0004913080379430854,
      "loss": 0.0028,
      "step": 1741
    },
    {
      "epoch": 0.8696954568147778,
      "grad_norm": 0.8187931776046753,
      "learning_rate": 0.0004913030454318522,
      "loss": 0.0389,
      "step": 1742
    },
    {
      "epoch": 0.8701947079380928,
      "grad_norm": 0.31605958938598633,
      "learning_rate": 0.0004912980529206191,
      "loss": 0.0085,
      "step": 1743
    },
    {
      "epoch": 0.8706939590614079,
      "grad_norm": 0.11608759313821793,
      "learning_rate": 0.0004912930604093859,
      "loss": 0.0033,
      "step": 1744
    },
    {
      "epoch": 0.871193210184723,
      "grad_norm": 0.16156132519245148,
      "learning_rate": 0.0004912880678981528,
      "loss": 0.0042,
      "step": 1745
    },
    {
      "epoch": 0.8716924613080379,
      "grad_norm": 0.17097237706184387,
      "learning_rate": 0.0004912830753869196,
      "loss": 0.0037,
      "step": 1746
    },
    {
      "epoch": 0.872191712431353,
      "grad_norm": 0.2156459391117096,
      "learning_rate": 0.0004912780828756865,
      "loss": 0.0071,
      "step": 1747
    },
    {
      "epoch": 0.872690963554668,
      "grad_norm": 0.26091882586479187,
      "learning_rate": 0.0004912730903644533,
      "loss": 0.0074,
      "step": 1748
    },
    {
      "epoch": 0.8731902146779831,
      "grad_norm": 0.18499204516410828,
      "learning_rate": 0.0004912680978532202,
      "loss": 0.006,
      "step": 1749
    },
    {
      "epoch": 0.873689465801298,
      "grad_norm": 0.2745234966278076,
      "learning_rate": 0.000491263105341987,
      "loss": 0.0066,
      "step": 1750
    },
    {
      "epoch": 0.8741887169246131,
      "grad_norm": 0.5290963053703308,
      "learning_rate": 0.0004912581128307539,
      "loss": 0.0065,
      "step": 1751
    },
    {
      "epoch": 0.8746879680479281,
      "grad_norm": 0.5142859220504761,
      "learning_rate": 0.0004912531203195207,
      "loss": 0.0035,
      "step": 1752
    },
    {
      "epoch": 0.8751872191712431,
      "grad_norm": 0.39838528633117676,
      "learning_rate": 0.0004912481278082876,
      "loss": 0.0067,
      "step": 1753
    },
    {
      "epoch": 0.8756864702945582,
      "grad_norm": 0.30233845114707947,
      "learning_rate": 0.0004912431352970544,
      "loss": 0.0041,
      "step": 1754
    },
    {
      "epoch": 0.8761857214178732,
      "grad_norm": 0.718966007232666,
      "learning_rate": 0.0004912381427858213,
      "loss": 0.0102,
      "step": 1755
    },
    {
      "epoch": 0.8766849725411883,
      "grad_norm": 0.4569658041000366,
      "learning_rate": 0.0004912331502745881,
      "loss": 0.0109,
      "step": 1756
    },
    {
      "epoch": 0.8771842236645032,
      "grad_norm": 0.07502958923578262,
      "learning_rate": 0.000491228157763355,
      "loss": 0.0025,
      "step": 1757
    },
    {
      "epoch": 0.8776834747878183,
      "grad_norm": 0.19191139936447144,
      "learning_rate": 0.0004912231652521218,
      "loss": 0.0074,
      "step": 1758
    },
    {
      "epoch": 0.8781827259111333,
      "grad_norm": 0.7315959930419922,
      "learning_rate": 0.0004912181727408887,
      "loss": 0.0139,
      "step": 1759
    },
    {
      "epoch": 0.8786819770344483,
      "grad_norm": 0.20701266825199127,
      "learning_rate": 0.0004912131802296555,
      "loss": 0.0038,
      "step": 1760
    },
    {
      "epoch": 0.8791812281577633,
      "grad_norm": 0.4186902642250061,
      "learning_rate": 0.0004912081877184223,
      "loss": 0.0221,
      "step": 1761
    },
    {
      "epoch": 0.8796804792810784,
      "grad_norm": 0.41005775332450867,
      "learning_rate": 0.0004912031952071892,
      "loss": 0.007,
      "step": 1762
    },
    {
      "epoch": 0.8801797304043935,
      "grad_norm": 1.2417004108428955,
      "learning_rate": 0.000491198202695956,
      "loss": 0.0278,
      "step": 1763
    },
    {
      "epoch": 0.8806789815277084,
      "grad_norm": 0.3011403977870941,
      "learning_rate": 0.0004911932101847229,
      "loss": 0.0058,
      "step": 1764
    },
    {
      "epoch": 0.8811782326510235,
      "grad_norm": 0.2293461114168167,
      "learning_rate": 0.0004911882176734897,
      "loss": 0.005,
      "step": 1765
    },
    {
      "epoch": 0.8816774837743385,
      "grad_norm": 0.3556213080883026,
      "learning_rate": 0.0004911832251622566,
      "loss": 0.0071,
      "step": 1766
    },
    {
      "epoch": 0.8821767348976535,
      "grad_norm": 0.22053536772727966,
      "learning_rate": 0.0004911782326510234,
      "loss": 0.0058,
      "step": 1767
    },
    {
      "epoch": 0.8826759860209685,
      "grad_norm": 0.2578815221786499,
      "learning_rate": 0.0004911732401397903,
      "loss": 0.0073,
      "step": 1768
    },
    {
      "epoch": 0.8831752371442836,
      "grad_norm": 0.5822685956954956,
      "learning_rate": 0.0004911682476285571,
      "loss": 0.0246,
      "step": 1769
    },
    {
      "epoch": 0.8836744882675986,
      "grad_norm": 0.36096230149269104,
      "learning_rate": 0.000491163255117324,
      "loss": 0.0077,
      "step": 1770
    },
    {
      "epoch": 0.8841737393909136,
      "grad_norm": 0.12798289954662323,
      "learning_rate": 0.0004911582626060908,
      "loss": 0.0034,
      "step": 1771
    },
    {
      "epoch": 0.8846729905142287,
      "grad_norm": 0.1578061580657959,
      "learning_rate": 0.0004911532700948578,
      "loss": 0.0036,
      "step": 1772
    },
    {
      "epoch": 0.8851722416375437,
      "grad_norm": 0.11442876607179642,
      "learning_rate": 0.0004911482775836246,
      "loss": 0.0033,
      "step": 1773
    },
    {
      "epoch": 0.8856714927608587,
      "grad_norm": 0.10191106796264648,
      "learning_rate": 0.0004911432850723915,
      "loss": 0.0031,
      "step": 1774
    },
    {
      "epoch": 0.8861707438841737,
      "grad_norm": 0.5221784114837646,
      "learning_rate": 0.0004911382925611583,
      "loss": 0.0165,
      "step": 1775
    },
    {
      "epoch": 0.8866699950074888,
      "grad_norm": 0.24491435289382935,
      "learning_rate": 0.0004911333000499252,
      "loss": 0.0054,
      "step": 1776
    },
    {
      "epoch": 0.8871692461308038,
      "grad_norm": 0.37889933586120605,
      "learning_rate": 0.000491128307538692,
      "loss": 0.0091,
      "step": 1777
    },
    {
      "epoch": 0.8876684972541188,
      "grad_norm": 0.31669238209724426,
      "learning_rate": 0.0004911233150274589,
      "loss": 0.0064,
      "step": 1778
    },
    {
      "epoch": 0.8881677483774338,
      "grad_norm": 0.48703664541244507,
      "learning_rate": 0.0004911183225162257,
      "loss": 0.0285,
      "step": 1779
    },
    {
      "epoch": 0.8886669995007489,
      "grad_norm": 0.23460890352725983,
      "learning_rate": 0.0004911133300049926,
      "loss": 0.0052,
      "step": 1780
    },
    {
      "epoch": 0.889166250624064,
      "grad_norm": 0.3180418908596039,
      "learning_rate": 0.0004911083374937594,
      "loss": 0.017,
      "step": 1781
    },
    {
      "epoch": 0.8896655017473789,
      "grad_norm": 0.26747775077819824,
      "learning_rate": 0.0004911033449825263,
      "loss": 0.0138,
      "step": 1782
    },
    {
      "epoch": 0.890164752870694,
      "grad_norm": 0.1611436903476715,
      "learning_rate": 0.0004910983524712931,
      "loss": 0.0043,
      "step": 1783
    },
    {
      "epoch": 0.890664003994009,
      "grad_norm": 0.5824131965637207,
      "learning_rate": 0.00049109335996006,
      "loss": 0.0135,
      "step": 1784
    },
    {
      "epoch": 0.891163255117324,
      "grad_norm": 0.1966933012008667,
      "learning_rate": 0.0004910883674488268,
      "loss": 0.029,
      "step": 1785
    },
    {
      "epoch": 0.891662506240639,
      "grad_norm": 0.3888514041900635,
      "learning_rate": 0.0004910833749375937,
      "loss": 0.0178,
      "step": 1786
    },
    {
      "epoch": 0.8921617573639541,
      "grad_norm": 0.065665103495121,
      "learning_rate": 0.0004910783824263605,
      "loss": 0.0021,
      "step": 1787
    },
    {
      "epoch": 0.8926610084872691,
      "grad_norm": 0.08705495297908783,
      "learning_rate": 0.0004910733899151274,
      "loss": 0.0028,
      "step": 1788
    },
    {
      "epoch": 0.8931602596105841,
      "grad_norm": 0.05306774005293846,
      "learning_rate": 0.0004910683974038942,
      "loss": 0.0022,
      "step": 1789
    },
    {
      "epoch": 0.8936595107338992,
      "grad_norm": 0.0495845302939415,
      "learning_rate": 0.000491063404892661,
      "loss": 0.0022,
      "step": 1790
    },
    {
      "epoch": 0.8941587618572142,
      "grad_norm": 0.1637798249721527,
      "learning_rate": 0.0004910584123814279,
      "loss": 0.0047,
      "step": 1791
    },
    {
      "epoch": 0.8946580129805292,
      "grad_norm": 0.27303066849708557,
      "learning_rate": 0.0004910534198701947,
      "loss": 0.0042,
      "step": 1792
    },
    {
      "epoch": 0.8951572641038442,
      "grad_norm": 0.7660371661186218,
      "learning_rate": 0.0004910484273589616,
      "loss": 0.0233,
      "step": 1793
    },
    {
      "epoch": 0.8956565152271593,
      "grad_norm": 0.8949258327484131,
      "learning_rate": 0.0004910434348477284,
      "loss": 0.0229,
      "step": 1794
    },
    {
      "epoch": 0.8961557663504743,
      "grad_norm": 0.2529241740703583,
      "learning_rate": 0.0004910384423364953,
      "loss": 0.0044,
      "step": 1795
    },
    {
      "epoch": 0.8966550174737893,
      "grad_norm": 0.21332120895385742,
      "learning_rate": 0.0004910334498252621,
      "loss": 0.0032,
      "step": 1796
    },
    {
      "epoch": 0.8971542685971043,
      "grad_norm": 0.06993020325899124,
      "learning_rate": 0.000491028457314029,
      "loss": 0.0023,
      "step": 1797
    },
    {
      "epoch": 0.8976535197204194,
      "grad_norm": 0.8391349911689758,
      "learning_rate": 0.0004910234648027958,
      "loss": 0.0181,
      "step": 1798
    },
    {
      "epoch": 0.8981527708437343,
      "grad_norm": 0.13114655017852783,
      "learning_rate": 0.0004910184722915627,
      "loss": 0.0027,
      "step": 1799
    },
    {
      "epoch": 0.8986520219670494,
      "grad_norm": 0.06775668263435364,
      "learning_rate": 0.0004910134797803295,
      "loss": 0.0025,
      "step": 1800
    },
    {
      "epoch": 0.8991512730903645,
      "grad_norm": 0.05308017507195473,
      "learning_rate": 0.0004910084872690964,
      "loss": 0.0022,
      "step": 1801
    },
    {
      "epoch": 0.8996505242136795,
      "grad_norm": 0.39183229207992554,
      "learning_rate": 0.0004910034947578632,
      "loss": 0.0066,
      "step": 1802
    },
    {
      "epoch": 0.9001497753369945,
      "grad_norm": 0.4078880846500397,
      "learning_rate": 0.0004909985022466301,
      "loss": 0.0093,
      "step": 1803
    },
    {
      "epoch": 0.9006490264603095,
      "grad_norm": 0.18324242532253265,
      "learning_rate": 0.0004909935097353969,
      "loss": 0.0038,
      "step": 1804
    },
    {
      "epoch": 0.9011482775836246,
      "grad_norm": 0.09311114996671677,
      "learning_rate": 0.0004909885172241638,
      "loss": 0.0024,
      "step": 1805
    },
    {
      "epoch": 0.9016475287069395,
      "grad_norm": 0.04629741981625557,
      "learning_rate": 0.0004909835247129306,
      "loss": 0.0022,
      "step": 1806
    },
    {
      "epoch": 0.9021467798302546,
      "grad_norm": 0.2761231064796448,
      "learning_rate": 0.0004909785322016975,
      "loss": 0.0047,
      "step": 1807
    },
    {
      "epoch": 0.9026460309535697,
      "grad_norm": 0.08282683789730072,
      "learning_rate": 0.0004909735396904643,
      "loss": 0.0023,
      "step": 1808
    },
    {
      "epoch": 0.9031452820768847,
      "grad_norm": 0.29147061705589294,
      "learning_rate": 0.0004909685471792312,
      "loss": 0.0056,
      "step": 1809
    },
    {
      "epoch": 0.9036445332001997,
      "grad_norm": 0.10995913296937943,
      "learning_rate": 0.000490963554667998,
      "loss": 0.0026,
      "step": 1810
    },
    {
      "epoch": 0.9041437843235147,
      "grad_norm": 0.44263944029808044,
      "learning_rate": 0.0004909585621567649,
      "loss": 0.0068,
      "step": 1811
    },
    {
      "epoch": 0.9046430354468298,
      "grad_norm": 0.41154661774635315,
      "learning_rate": 0.0004909535696455317,
      "loss": 0.0076,
      "step": 1812
    },
    {
      "epoch": 0.9051422865701447,
      "grad_norm": 2.0199990272521973,
      "learning_rate": 0.0004909485771342986,
      "loss": 0.0145,
      "step": 1813
    },
    {
      "epoch": 0.9056415376934598,
      "grad_norm": 1.2329310178756714,
      "learning_rate": 0.0004909435846230654,
      "loss": 0.0533,
      "step": 1814
    },
    {
      "epoch": 0.9061407888167748,
      "grad_norm": 0.4880448281764984,
      "learning_rate": 0.0004909385921118322,
      "loss": 0.0149,
      "step": 1815
    },
    {
      "epoch": 0.9066400399400899,
      "grad_norm": 0.3438448905944824,
      "learning_rate": 0.0004909335996005991,
      "loss": 0.0053,
      "step": 1816
    },
    {
      "epoch": 0.9071392910634049,
      "grad_norm": 0.475946307182312,
      "learning_rate": 0.0004909286070893659,
      "loss": 0.012,
      "step": 1817
    },
    {
      "epoch": 0.9076385421867199,
      "grad_norm": 0.03269112482666969,
      "learning_rate": 0.0004909236145781328,
      "loss": 0.0014,
      "step": 1818
    },
    {
      "epoch": 0.908137793310035,
      "grad_norm": 0.38502639532089233,
      "learning_rate": 0.0004909186220668996,
      "loss": 0.0053,
      "step": 1819
    },
    {
      "epoch": 0.90863704443335,
      "grad_norm": 0.6824812293052673,
      "learning_rate": 0.0004909136295556664,
      "loss": 0.0214,
      "step": 1820
    },
    {
      "epoch": 0.909136295556665,
      "grad_norm": 1.0691642761230469,
      "learning_rate": 0.0004909086370444333,
      "loss": 0.0143,
      "step": 1821
    },
    {
      "epoch": 0.90963554667998,
      "grad_norm": 0.5622383952140808,
      "learning_rate": 0.0004909036445332001,
      "loss": 0.0053,
      "step": 1822
    },
    {
      "epoch": 0.9101347978032951,
      "grad_norm": 0.1016005352139473,
      "learning_rate": 0.000490898652021967,
      "loss": 0.0022,
      "step": 1823
    },
    {
      "epoch": 0.91063404892661,
      "grad_norm": 0.05284959450364113,
      "learning_rate": 0.0004908936595107338,
      "loss": 0.0026,
      "step": 1824
    },
    {
      "epoch": 0.9111333000499251,
      "grad_norm": 0.19200633466243744,
      "learning_rate": 0.0004908886669995007,
      "loss": 0.0042,
      "step": 1825
    },
    {
      "epoch": 0.9116325511732402,
      "grad_norm": 0.030146392062306404,
      "learning_rate": 0.0004908836744882675,
      "loss": 0.0018,
      "step": 1826
    },
    {
      "epoch": 0.9121318022965552,
      "grad_norm": 0.5005760788917542,
      "learning_rate": 0.0004908786819770345,
      "loss": 0.0103,
      "step": 1827
    },
    {
      "epoch": 0.9126310534198702,
      "grad_norm": 0.7201240062713623,
      "learning_rate": 0.0004908736894658013,
      "loss": 0.0156,
      "step": 1828
    },
    {
      "epoch": 0.9131303045431852,
      "grad_norm": 1.2175252437591553,
      "learning_rate": 0.0004908686969545682,
      "loss": 0.0076,
      "step": 1829
    },
    {
      "epoch": 0.9136295556665003,
      "grad_norm": 0.15530630946159363,
      "learning_rate": 0.000490863704443335,
      "loss": 0.0037,
      "step": 1830
    },
    {
      "epoch": 0.9141288067898152,
      "grad_norm": 0.49155503511428833,
      "learning_rate": 0.0004908587119321019,
      "loss": 0.0151,
      "step": 1831
    },
    {
      "epoch": 0.9146280579131303,
      "grad_norm": 0.2578377425670624,
      "learning_rate": 0.0004908537194208687,
      "loss": 0.0065,
      "step": 1832
    },
    {
      "epoch": 0.9151273090364453,
      "grad_norm": 0.47113218903541565,
      "learning_rate": 0.0004908487269096356,
      "loss": 0.0235,
      "step": 1833
    },
    {
      "epoch": 0.9156265601597604,
      "grad_norm": 0.04899642989039421,
      "learning_rate": 0.0004908437343984024,
      "loss": 0.0021,
      "step": 1834
    },
    {
      "epoch": 0.9161258112830754,
      "grad_norm": 0.040711041539907455,
      "learning_rate": 0.0004908387418871693,
      "loss": 0.0015,
      "step": 1835
    },
    {
      "epoch": 0.9166250624063904,
      "grad_norm": 0.03883593529462814,
      "learning_rate": 0.0004908337493759361,
      "loss": 0.002,
      "step": 1836
    },
    {
      "epoch": 0.9171243135297055,
      "grad_norm": 0.09788066148757935,
      "learning_rate": 0.000490828756864703,
      "loss": 0.003,
      "step": 1837
    },
    {
      "epoch": 0.9176235646530204,
      "grad_norm": 0.3478958010673523,
      "learning_rate": 0.0004908237643534698,
      "loss": 0.0071,
      "step": 1838
    },
    {
      "epoch": 0.9181228157763355,
      "grad_norm": 1.3012990951538086,
      "learning_rate": 0.0004908187718422367,
      "loss": 0.0169,
      "step": 1839
    },
    {
      "epoch": 0.9186220668996505,
      "grad_norm": 0.5165566205978394,
      "learning_rate": 0.0004908137793310035,
      "loss": 0.0088,
      "step": 1840
    },
    {
      "epoch": 0.9191213180229656,
      "grad_norm": 0.30161577463150024,
      "learning_rate": 0.0004908087868197704,
      "loss": 0.0054,
      "step": 1841
    },
    {
      "epoch": 0.9196205691462805,
      "grad_norm": 0.05990372225642204,
      "learning_rate": 0.0004908037943085372,
      "loss": 0.0021,
      "step": 1842
    },
    {
      "epoch": 0.9201198202695956,
      "grad_norm": 0.1179819405078888,
      "learning_rate": 0.0004907988017973041,
      "loss": 0.0037,
      "step": 1843
    },
    {
      "epoch": 0.9206190713929107,
      "grad_norm": 0.15431751310825348,
      "learning_rate": 0.0004907938092860709,
      "loss": 0.0043,
      "step": 1844
    },
    {
      "epoch": 0.9211183225162256,
      "grad_norm": 0.41168755292892456,
      "learning_rate": 0.0004907888167748378,
      "loss": 0.0087,
      "step": 1845
    },
    {
      "epoch": 0.9216175736395407,
      "grad_norm": 0.6538278460502625,
      "learning_rate": 0.0004907838242636046,
      "loss": 0.0152,
      "step": 1846
    },
    {
      "epoch": 0.9221168247628557,
      "grad_norm": 0.16077834367752075,
      "learning_rate": 0.0004907788317523715,
      "loss": 0.0032,
      "step": 1847
    },
    {
      "epoch": 0.9226160758861708,
      "grad_norm": 0.31720495223999023,
      "learning_rate": 0.0004907738392411383,
      "loss": 0.013,
      "step": 1848
    },
    {
      "epoch": 0.9231153270094857,
      "grad_norm": 0.03998477756977081,
      "learning_rate": 0.0004907688467299051,
      "loss": 0.0018,
      "step": 1849
    },
    {
      "epoch": 0.9236145781328008,
      "grad_norm": 0.09644652903079987,
      "learning_rate": 0.000490763854218672,
      "loss": 0.0024,
      "step": 1850
    },
    {
      "epoch": 0.9241138292561158,
      "grad_norm": 0.6306845545768738,
      "learning_rate": 0.0004907588617074388,
      "loss": 0.005,
      "step": 1851
    },
    {
      "epoch": 0.9246130803794309,
      "grad_norm": 0.08603361248970032,
      "learning_rate": 0.0004907538691962057,
      "loss": 0.0022,
      "step": 1852
    },
    {
      "epoch": 0.9251123315027459,
      "grad_norm": 0.23847486078739166,
      "learning_rate": 0.0004907488766849725,
      "loss": 0.0042,
      "step": 1853
    },
    {
      "epoch": 0.9256115826260609,
      "grad_norm": 0.7447221875190735,
      "learning_rate": 0.0004907438841737394,
      "loss": 0.0261,
      "step": 1854
    },
    {
      "epoch": 0.926110833749376,
      "grad_norm": 0.3417981266975403,
      "learning_rate": 0.0004907388916625062,
      "loss": 0.0074,
      "step": 1855
    },
    {
      "epoch": 0.9266100848726909,
      "grad_norm": 0.3935095965862274,
      "learning_rate": 0.0004907338991512731,
      "loss": 0.0053,
      "step": 1856
    },
    {
      "epoch": 0.927109335996006,
      "grad_norm": 0.09522951394319534,
      "learning_rate": 0.0004907289066400399,
      "loss": 0.0029,
      "step": 1857
    },
    {
      "epoch": 0.927608587119321,
      "grad_norm": 0.5077064633369446,
      "learning_rate": 0.0004907239141288068,
      "loss": 0.0136,
      "step": 1858
    },
    {
      "epoch": 0.9281078382426361,
      "grad_norm": 1.9412810802459717,
      "learning_rate": 0.0004907189216175736,
      "loss": 0.0081,
      "step": 1859
    },
    {
      "epoch": 0.928607089365951,
      "grad_norm": 0.18832364678382874,
      "learning_rate": 0.0004907139291063405,
      "loss": 0.0039,
      "step": 1860
    },
    {
      "epoch": 0.9291063404892661,
      "grad_norm": 0.17864198982715607,
      "learning_rate": 0.0004907089365951073,
      "loss": 0.003,
      "step": 1861
    },
    {
      "epoch": 0.9296055916125812,
      "grad_norm": 0.28698837757110596,
      "learning_rate": 0.0004907039440838742,
      "loss": 0.0074,
      "step": 1862
    },
    {
      "epoch": 0.9301048427358961,
      "grad_norm": 0.6054616570472717,
      "learning_rate": 0.000490698951572641,
      "loss": 0.0215,
      "step": 1863
    },
    {
      "epoch": 0.9306040938592112,
      "grad_norm": 0.4987839460372925,
      "learning_rate": 0.0004906939590614079,
      "loss": 0.0091,
      "step": 1864
    },
    {
      "epoch": 0.9311033449825262,
      "grad_norm": 0.7027230858802795,
      "learning_rate": 0.0004906889665501747,
      "loss": 0.0083,
      "step": 1865
    },
    {
      "epoch": 0.9316025961058413,
      "grad_norm": 0.036239463835954666,
      "learning_rate": 0.0004906839740389416,
      "loss": 0.0015,
      "step": 1866
    },
    {
      "epoch": 0.9321018472291562,
      "grad_norm": 0.10977958142757416,
      "learning_rate": 0.0004906789815277084,
      "loss": 0.0025,
      "step": 1867
    },
    {
      "epoch": 0.9326010983524713,
      "grad_norm": 0.06890165060758591,
      "learning_rate": 0.0004906739890164753,
      "loss": 0.002,
      "step": 1868
    },
    {
      "epoch": 0.9331003494757864,
      "grad_norm": 1.3075129985809326,
      "learning_rate": 0.0004906689965052421,
      "loss": 0.0309,
      "step": 1869
    },
    {
      "epoch": 0.9335996005991013,
      "grad_norm": 0.4359966516494751,
      "learning_rate": 0.000490664003994009,
      "loss": 0.0073,
      "step": 1870
    },
    {
      "epoch": 0.9340988517224164,
      "grad_norm": 0.9140894412994385,
      "learning_rate": 0.0004906590114827758,
      "loss": 0.0118,
      "step": 1871
    },
    {
      "epoch": 0.9345981028457314,
      "grad_norm": 0.2501702606678009,
      "learning_rate": 0.0004906540189715427,
      "loss": 0.0106,
      "step": 1872
    },
    {
      "epoch": 0.9350973539690465,
      "grad_norm": 0.6034039258956909,
      "learning_rate": 0.0004906490264603095,
      "loss": 0.0137,
      "step": 1873
    },
    {
      "epoch": 0.9355966050923614,
      "grad_norm": 0.18246053159236908,
      "learning_rate": 0.0004906440339490764,
      "loss": 0.0042,
      "step": 1874
    },
    {
      "epoch": 0.9360958562156765,
      "grad_norm": 0.28211334347724915,
      "learning_rate": 0.0004906390414378432,
      "loss": 0.0061,
      "step": 1875
    },
    {
      "epoch": 0.9365951073389915,
      "grad_norm": 0.907296895980835,
      "learning_rate": 0.0004906340489266102,
      "loss": 0.017,
      "step": 1876
    },
    {
      "epoch": 0.9370943584623065,
      "grad_norm": 0.32526373863220215,
      "learning_rate": 0.000490629056415377,
      "loss": 0.0088,
      "step": 1877
    },
    {
      "epoch": 0.9375936095856215,
      "grad_norm": 0.4720379710197449,
      "learning_rate": 0.0004906240639041437,
      "loss": 0.0043,
      "step": 1878
    },
    {
      "epoch": 0.9380928607089366,
      "grad_norm": 0.22647948563098907,
      "learning_rate": 0.0004906190713929107,
      "loss": 0.0055,
      "step": 1879
    },
    {
      "epoch": 0.9385921118322517,
      "grad_norm": 0.14992259442806244,
      "learning_rate": 0.0004906140788816775,
      "loss": 0.003,
      "step": 1880
    },
    {
      "epoch": 0.9390913629555666,
      "grad_norm": 0.5159293413162231,
      "learning_rate": 0.0004906090863704444,
      "loss": 0.0076,
      "step": 1881
    },
    {
      "epoch": 0.9395906140788817,
      "grad_norm": 0.1992955356836319,
      "learning_rate": 0.0004906040938592112,
      "loss": 0.0019,
      "step": 1882
    },
    {
      "epoch": 0.9400898652021967,
      "grad_norm": 0.4655037224292755,
      "learning_rate": 0.0004905991013479781,
      "loss": 0.0195,
      "step": 1883
    },
    {
      "epoch": 0.9405891163255118,
      "grad_norm": 0.28760457038879395,
      "learning_rate": 0.0004905941088367449,
      "loss": 0.0285,
      "step": 1884
    },
    {
      "epoch": 0.9410883674488267,
      "grad_norm": 0.3818255066871643,
      "learning_rate": 0.0004905891163255118,
      "loss": 0.0063,
      "step": 1885
    },
    {
      "epoch": 0.9415876185721418,
      "grad_norm": 0.4681486189365387,
      "learning_rate": 0.0004905841238142786,
      "loss": 0.0094,
      "step": 1886
    },
    {
      "epoch": 0.9420868696954569,
      "grad_norm": 0.3441755473613739,
      "learning_rate": 0.0004905791313030455,
      "loss": 0.0095,
      "step": 1887
    },
    {
      "epoch": 0.9425861208187718,
      "grad_norm": 0.7255892753601074,
      "learning_rate": 0.0004905741387918123,
      "loss": 0.0145,
      "step": 1888
    },
    {
      "epoch": 0.9430853719420869,
      "grad_norm": 0.12368382513523102,
      "learning_rate": 0.0004905691462805792,
      "loss": 0.003,
      "step": 1889
    },
    {
      "epoch": 0.9435846230654019,
      "grad_norm": 0.1384870558977127,
      "learning_rate": 0.000490564153769346,
      "loss": 0.021,
      "step": 1890
    },
    {
      "epoch": 0.944083874188717,
      "grad_norm": 0.30500271916389465,
      "learning_rate": 0.0004905591612581129,
      "loss": 0.0079,
      "step": 1891
    },
    {
      "epoch": 0.9445831253120319,
      "grad_norm": 0.36040014028549194,
      "learning_rate": 0.0004905541687468797,
      "loss": 0.0082,
      "step": 1892
    },
    {
      "epoch": 0.945082376435347,
      "grad_norm": 0.07599783688783646,
      "learning_rate": 0.0004905491762356466,
      "loss": 0.0023,
      "step": 1893
    },
    {
      "epoch": 0.945581627558662,
      "grad_norm": 0.390070378780365,
      "learning_rate": 0.0004905441837244134,
      "loss": 0.0197,
      "step": 1894
    },
    {
      "epoch": 0.946080878681977,
      "grad_norm": 0.08574448525905609,
      "learning_rate": 0.0004905391912131803,
      "loss": 0.0029,
      "step": 1895
    },
    {
      "epoch": 0.946580129805292,
      "grad_norm": 0.04955640435218811,
      "learning_rate": 0.0004905341987019471,
      "loss": 0.002,
      "step": 1896
    },
    {
      "epoch": 0.9470793809286071,
      "grad_norm": 0.8764714598655701,
      "learning_rate": 0.000490529206190714,
      "loss": 0.0162,
      "step": 1897
    },
    {
      "epoch": 0.9475786320519222,
      "grad_norm": 0.389952689409256,
      "learning_rate": 0.0004905242136794808,
      "loss": 0.0071,
      "step": 1898
    },
    {
      "epoch": 0.9480778831752371,
      "grad_norm": 0.05526890605688095,
      "learning_rate": 0.0004905192211682477,
      "loss": 0.0018,
      "step": 1899
    },
    {
      "epoch": 0.9485771342985522,
      "grad_norm": 0.3462296426296234,
      "learning_rate": 0.0004905142286570145,
      "loss": 0.0082,
      "step": 1900
    },
    {
      "epoch": 0.9490763854218672,
      "grad_norm": 0.04277283698320389,
      "learning_rate": 0.0004905092361457814,
      "loss": 0.0014,
      "step": 1901
    },
    {
      "epoch": 0.9495756365451822,
      "grad_norm": 0.6104646325111389,
      "learning_rate": 0.0004905042436345482,
      "loss": 0.0052,
      "step": 1902
    },
    {
      "epoch": 0.9500748876684972,
      "grad_norm": 0.08362369239330292,
      "learning_rate": 0.0004904992511233151,
      "loss": 0.0021,
      "step": 1903
    },
    {
      "epoch": 0.9505741387918123,
      "grad_norm": 0.05087586119771004,
      "learning_rate": 0.0004904942586120819,
      "loss": 0.0018,
      "step": 1904
    },
    {
      "epoch": 0.9510733899151274,
      "grad_norm": 0.8784313201904297,
      "learning_rate": 0.0004904892661008488,
      "loss": 0.0417,
      "step": 1905
    },
    {
      "epoch": 0.9515726410384423,
      "grad_norm": 0.07581343501806259,
      "learning_rate": 0.0004904842735896156,
      "loss": 0.0024,
      "step": 1906
    },
    {
      "epoch": 0.9520718921617574,
      "grad_norm": 0.7331522703170776,
      "learning_rate": 0.0004904792810783824,
      "loss": 0.0353,
      "step": 1907
    },
    {
      "epoch": 0.9525711432850724,
      "grad_norm": 0.5442699193954468,
      "learning_rate": 0.0004904742885671493,
      "loss": 0.0063,
      "step": 1908
    },
    {
      "epoch": 0.9530703944083874,
      "grad_norm": 0.031406521797180176,
      "learning_rate": 0.0004904692960559161,
      "loss": 0.0015,
      "step": 1909
    },
    {
      "epoch": 0.9535696455317024,
      "grad_norm": 0.2895377576351166,
      "learning_rate": 0.000490464303544683,
      "loss": 0.0044,
      "step": 1910
    },
    {
      "epoch": 0.9540688966550175,
      "grad_norm": 0.3478536307811737,
      "learning_rate": 0.0004904593110334498,
      "loss": 0.0091,
      "step": 1911
    },
    {
      "epoch": 0.9545681477783325,
      "grad_norm": 0.1261494904756546,
      "learning_rate": 0.0004904543185222167,
      "loss": 0.0031,
      "step": 1912
    },
    {
      "epoch": 0.9550673989016475,
      "grad_norm": 0.12103721499443054,
      "learning_rate": 0.0004904493260109835,
      "loss": 0.003,
      "step": 1913
    },
    {
      "epoch": 0.9555666500249625,
      "grad_norm": 0.40215957164764404,
      "learning_rate": 0.0004904443334997504,
      "loss": 0.0445,
      "step": 1914
    },
    {
      "epoch": 0.9560659011482776,
      "grad_norm": 0.30413246154785156,
      "learning_rate": 0.0004904393409885172,
      "loss": 0.0087,
      "step": 1915
    },
    {
      "epoch": 0.9565651522715926,
      "grad_norm": 0.03314320370554924,
      "learning_rate": 0.0004904343484772841,
      "loss": 0.0016,
      "step": 1916
    },
    {
      "epoch": 0.9570644033949076,
      "grad_norm": 1.056736707687378,
      "learning_rate": 0.0004904293559660509,
      "loss": 0.0225,
      "step": 1917
    },
    {
      "epoch": 0.9575636545182227,
      "grad_norm": 0.3892633616924286,
      "learning_rate": 0.0004904243634548178,
      "loss": 0.0078,
      "step": 1918
    },
    {
      "epoch": 0.9580629056415377,
      "grad_norm": 2.6648590564727783,
      "learning_rate": 0.0004904193709435846,
      "loss": 0.0144,
      "step": 1919
    },
    {
      "epoch": 0.9585621567648527,
      "grad_norm": 0.3648150861263275,
      "learning_rate": 0.0004904143784323515,
      "loss": 0.0045,
      "step": 1920
    },
    {
      "epoch": 0.9590614078881677,
      "grad_norm": 0.31555283069610596,
      "learning_rate": 0.0004904093859211183,
      "loss": 0.0044,
      "step": 1921
    },
    {
      "epoch": 0.9595606590114828,
      "grad_norm": 0.125411719083786,
      "learning_rate": 0.0004904043934098852,
      "loss": 0.0035,
      "step": 1922
    },
    {
      "epoch": 0.9600599101347979,
      "grad_norm": 0.5876912474632263,
      "learning_rate": 0.000490399400898652,
      "loss": 0.0084,
      "step": 1923
    },
    {
      "epoch": 0.9605591612581128,
      "grad_norm": 0.47908589243888855,
      "learning_rate": 0.0004903944083874189,
      "loss": 0.0032,
      "step": 1924
    },
    {
      "epoch": 0.9610584123814279,
      "grad_norm": 0.21373870968818665,
      "learning_rate": 0.0004903894158761857,
      "loss": 0.0043,
      "step": 1925
    },
    {
      "epoch": 0.9615576635047429,
      "grad_norm": 0.044283267110586166,
      "learning_rate": 0.0004903844233649526,
      "loss": 0.0021,
      "step": 1926
    },
    {
      "epoch": 0.9620569146280579,
      "grad_norm": 0.025827130302786827,
      "learning_rate": 0.0004903794308537194,
      "loss": 0.0017,
      "step": 1927
    },
    {
      "epoch": 0.9625561657513729,
      "grad_norm": 0.512971043586731,
      "learning_rate": 0.0004903744383424862,
      "loss": 0.009,
      "step": 1928
    },
    {
      "epoch": 0.963055416874688,
      "grad_norm": 0.23580646514892578,
      "learning_rate": 0.0004903694458312531,
      "loss": 0.0141,
      "step": 1929
    },
    {
      "epoch": 0.963554667998003,
      "grad_norm": 0.5766978859901428,
      "learning_rate": 0.00049036445332002,
      "loss": 0.0231,
      "step": 1930
    },
    {
      "epoch": 0.964053919121318,
      "grad_norm": 0.04512758180499077,
      "learning_rate": 0.0004903594608087869,
      "loss": 0.0016,
      "step": 1931
    },
    {
      "epoch": 0.964553170244633,
      "grad_norm": 0.7654806971549988,
      "learning_rate": 0.0004903544682975537,
      "loss": 0.0149,
      "step": 1932
    },
    {
      "epoch": 0.9650524213679481,
      "grad_norm": 1.163628339767456,
      "learning_rate": 0.0004903494757863206,
      "loss": 0.0063,
      "step": 1933
    },
    {
      "epoch": 0.9655516724912631,
      "grad_norm": 0.15031343698501587,
      "learning_rate": 0.0004903444832750874,
      "loss": 0.0139,
      "step": 1934
    },
    {
      "epoch": 0.9660509236145781,
      "grad_norm": 0.3728110194206238,
      "learning_rate": 0.0004903394907638542,
      "loss": 0.0134,
      "step": 1935
    },
    {
      "epoch": 0.9665501747378932,
      "grad_norm": 0.15933148562908173,
      "learning_rate": 0.0004903344982526211,
      "loss": 0.0037,
      "step": 1936
    },
    {
      "epoch": 0.9670494258612082,
      "grad_norm": 0.17140978574752808,
      "learning_rate": 0.0004903295057413879,
      "loss": 0.005,
      "step": 1937
    },
    {
      "epoch": 0.9675486769845232,
      "grad_norm": 0.4847874939441681,
      "learning_rate": 0.0004903245132301548,
      "loss": 0.0096,
      "step": 1938
    },
    {
      "epoch": 0.9680479281078382,
      "grad_norm": 0.08847681432962418,
      "learning_rate": 0.0004903195207189216,
      "loss": 0.0025,
      "step": 1939
    },
    {
      "epoch": 0.9685471792311533,
      "grad_norm": 0.5937097072601318,
      "learning_rate": 0.0004903145282076885,
      "loss": 0.013,
      "step": 1940
    },
    {
      "epoch": 0.9690464303544682,
      "grad_norm": 0.3095840811729431,
      "learning_rate": 0.0004903095356964553,
      "loss": 0.0047,
      "step": 1941
    },
    {
      "epoch": 0.9695456814777833,
      "grad_norm": 0.43713217973709106,
      "learning_rate": 0.0004903045431852222,
      "loss": 0.0225,
      "step": 1942
    },
    {
      "epoch": 0.9700449326010984,
      "grad_norm": 0.6203356981277466,
      "learning_rate": 0.000490299550673989,
      "loss": 0.0452,
      "step": 1943
    },
    {
      "epoch": 0.9705441837244134,
      "grad_norm": 0.29367098212242126,
      "learning_rate": 0.0004902945581627559,
      "loss": 0.0047,
      "step": 1944
    },
    {
      "epoch": 0.9710434348477284,
      "grad_norm": 0.3643483817577362,
      "learning_rate": 0.0004902895656515227,
      "loss": 0.015,
      "step": 1945
    },
    {
      "epoch": 0.9715426859710434,
      "grad_norm": 1.0095630884170532,
      "learning_rate": 0.0004902845731402896,
      "loss": 0.019,
      "step": 1946
    },
    {
      "epoch": 0.9720419370943585,
      "grad_norm": 1.1074879169464111,
      "learning_rate": 0.0004902795806290564,
      "loss": 0.0116,
      "step": 1947
    },
    {
      "epoch": 0.9725411882176734,
      "grad_norm": 0.06589390337467194,
      "learning_rate": 0.0004902745881178233,
      "loss": 0.0023,
      "step": 1948
    },
    {
      "epoch": 0.9730404393409885,
      "grad_norm": 0.3076321482658386,
      "learning_rate": 0.0004902695956065901,
      "loss": 0.0064,
      "step": 1949
    },
    {
      "epoch": 0.9735396904643036,
      "grad_norm": 0.10115203261375427,
      "learning_rate": 0.000490264603095357,
      "loss": 0.0026,
      "step": 1950
    },
    {
      "epoch": 0.9740389415876186,
      "grad_norm": 0.5033968687057495,
      "learning_rate": 0.0004902596105841238,
      "loss": 0.0082,
      "step": 1951
    },
    {
      "epoch": 0.9745381927109336,
      "grad_norm": 0.25456544756889343,
      "learning_rate": 0.0004902546180728907,
      "loss": 0.0076,
      "step": 1952
    },
    {
      "epoch": 0.9750374438342486,
      "grad_norm": 0.464717298746109,
      "learning_rate": 0.0004902496255616575,
      "loss": 0.017,
      "step": 1953
    },
    {
      "epoch": 0.9755366949575637,
      "grad_norm": 0.761803388595581,
      "learning_rate": 0.0004902446330504244,
      "loss": 0.0183,
      "step": 1954
    },
    {
      "epoch": 0.9760359460808787,
      "grad_norm": 1.0685588121414185,
      "learning_rate": 0.0004902396405391912,
      "loss": 0.0096,
      "step": 1955
    },
    {
      "epoch": 0.9765351972041937,
      "grad_norm": 0.14957596361637115,
      "learning_rate": 0.0004902346480279581,
      "loss": 0.0032,
      "step": 1956
    },
    {
      "epoch": 0.9770344483275087,
      "grad_norm": 0.1661285012960434,
      "learning_rate": 0.0004902296555167249,
      "loss": 0.0042,
      "step": 1957
    },
    {
      "epoch": 0.9775336994508238,
      "grad_norm": 0.3207103908061981,
      "learning_rate": 0.0004902246630054918,
      "loss": 0.0057,
      "step": 1958
    },
    {
      "epoch": 0.9780329505741387,
      "grad_norm": 0.2708434760570526,
      "learning_rate": 0.0004902196704942586,
      "loss": 0.0047,
      "step": 1959
    },
    {
      "epoch": 0.9785322016974538,
      "grad_norm": 0.3350383937358856,
      "learning_rate": 0.0004902146779830255,
      "loss": 0.0094,
      "step": 1960
    },
    {
      "epoch": 0.9790314528207689,
      "grad_norm": 0.24694280326366425,
      "learning_rate": 0.0004902096854717923,
      "loss": 0.0026,
      "step": 1961
    },
    {
      "epoch": 0.9795307039440839,
      "grad_norm": 0.6324694156646729,
      "learning_rate": 0.0004902046929605592,
      "loss": 0.0042,
      "step": 1962
    },
    {
      "epoch": 0.9800299550673989,
      "grad_norm": 0.13312631845474243,
      "learning_rate": 0.000490199700449326,
      "loss": 0.0032,
      "step": 1963
    },
    {
      "epoch": 0.9805292061907139,
      "grad_norm": 0.18279872834682465,
      "learning_rate": 0.0004901947079380929,
      "loss": 0.0033,
      "step": 1964
    },
    {
      "epoch": 0.981028457314029,
      "grad_norm": 0.26552432775497437,
      "learning_rate": 0.0004901897154268597,
      "loss": 0.0082,
      "step": 1965
    },
    {
      "epoch": 0.9815277084373439,
      "grad_norm": 0.08250094205141068,
      "learning_rate": 0.0004901847229156265,
      "loss": 0.0026,
      "step": 1966
    },
    {
      "epoch": 0.982026959560659,
      "grad_norm": 0.07695217430591583,
      "learning_rate": 0.0004901797304043934,
      "loss": 0.0022,
      "step": 1967
    },
    {
      "epoch": 0.982526210683974,
      "grad_norm": 0.2835066616535187,
      "learning_rate": 0.0004901747378931602,
      "loss": 0.0062,
      "step": 1968
    },
    {
      "epoch": 0.9830254618072891,
      "grad_norm": 0.1960057020187378,
      "learning_rate": 0.0004901697453819271,
      "loss": 0.0033,
      "step": 1969
    },
    {
      "epoch": 0.9835247129306041,
      "grad_norm": 0.07217071205377579,
      "learning_rate": 0.0004901647528706939,
      "loss": 0.0024,
      "step": 1970
    },
    {
      "epoch": 0.9840239640539191,
      "grad_norm": 0.27038607001304626,
      "learning_rate": 0.0004901597603594608,
      "loss": 0.0054,
      "step": 1971
    },
    {
      "epoch": 0.9845232151772342,
      "grad_norm": 1.0863593816757202,
      "learning_rate": 0.0004901547678482276,
      "loss": 0.0234,
      "step": 1972
    },
    {
      "epoch": 0.9850224663005491,
      "grad_norm": 0.03152349218726158,
      "learning_rate": 0.0004901497753369945,
      "loss": 0.0015,
      "step": 1973
    },
    {
      "epoch": 0.9855217174238642,
      "grad_norm": 0.9055390357971191,
      "learning_rate": 0.0004901447828257613,
      "loss": 0.0145,
      "step": 1974
    },
    {
      "epoch": 0.9860209685471792,
      "grad_norm": 0.06255421787500381,
      "learning_rate": 0.0004901397903145282,
      "loss": 0.0015,
      "step": 1975
    },
    {
      "epoch": 0.9865202196704943,
      "grad_norm": 0.22973817586898804,
      "learning_rate": 0.000490134797803295,
      "loss": 0.0037,
      "step": 1976
    },
    {
      "epoch": 0.9870194707938093,
      "grad_norm": 0.1678740531206131,
      "learning_rate": 0.0004901298052920619,
      "loss": 0.003,
      "step": 1977
    },
    {
      "epoch": 0.9875187219171243,
      "grad_norm": 0.7868412733078003,
      "learning_rate": 0.0004901248127808287,
      "loss": 0.0164,
      "step": 1978
    },
    {
      "epoch": 0.9880179730404394,
      "grad_norm": 1.8338701725006104,
      "learning_rate": 0.0004901198202695956,
      "loss": 0.0464,
      "step": 1979
    },
    {
      "epoch": 0.9885172241637543,
      "grad_norm": 0.35037580132484436,
      "learning_rate": 0.0004901148277583624,
      "loss": 0.013,
      "step": 1980
    },
    {
      "epoch": 0.9890164752870694,
      "grad_norm": 0.42062243819236755,
      "learning_rate": 0.0004901098352471293,
      "loss": 0.0072,
      "step": 1981
    },
    {
      "epoch": 0.9895157264103844,
      "grad_norm": 1.4198867082595825,
      "learning_rate": 0.0004901048427358961,
      "loss": 0.0081,
      "step": 1982
    },
    {
      "epoch": 0.9900149775336995,
      "grad_norm": 1.5246655941009521,
      "learning_rate": 0.000490099850224663,
      "loss": 0.0048,
      "step": 1983
    },
    {
      "epoch": 0.9905142286570144,
      "grad_norm": 0.1464819312095642,
      "learning_rate": 0.0004900948577134298,
      "loss": 0.0033,
      "step": 1984
    },
    {
      "epoch": 0.9910134797803295,
      "grad_norm": 0.21235540509223938,
      "learning_rate": 0.0004900898652021968,
      "loss": 0.0062,
      "step": 1985
    },
    {
      "epoch": 0.9915127309036446,
      "grad_norm": 0.40827980637550354,
      "learning_rate": 0.0004900848726909636,
      "loss": 0.008,
      "step": 1986
    },
    {
      "epoch": 0.9920119820269596,
      "grad_norm": 0.10810571163892746,
      "learning_rate": 0.0004900798801797305,
      "loss": 0.0022,
      "step": 1987
    },
    {
      "epoch": 0.9925112331502746,
      "grad_norm": 0.242573544383049,
      "learning_rate": 0.0004900748876684973,
      "loss": 0.004,
      "step": 1988
    },
    {
      "epoch": 0.9930104842735896,
      "grad_norm": 0.04924822226166725,
      "learning_rate": 0.0004900698951572642,
      "loss": 0.002,
      "step": 1989
    },
    {
      "epoch": 0.9935097353969047,
      "grad_norm": 0.17115291953086853,
      "learning_rate": 0.000490064902646031,
      "loss": 0.0049,
      "step": 1990
    },
    {
      "epoch": 0.9940089865202196,
      "grad_norm": 0.3313237130641937,
      "learning_rate": 0.0004900599101347979,
      "loss": 0.0113,
      "step": 1991
    },
    {
      "epoch": 0.9945082376435347,
      "grad_norm": 0.6779391169548035,
      "learning_rate": 0.0004900549176235647,
      "loss": 0.0168,
      "step": 1992
    },
    {
      "epoch": 0.9950074887668497,
      "grad_norm": 0.5572754144668579,
      "learning_rate": 0.0004900499251123316,
      "loss": 0.0114,
      "step": 1993
    },
    {
      "epoch": 0.9955067398901648,
      "grad_norm": 0.397670179605484,
      "learning_rate": 0.0004900449326010984,
      "loss": 0.0151,
      "step": 1994
    },
    {
      "epoch": 0.9960059910134798,
      "grad_norm": 0.04480176419019699,
      "learning_rate": 0.0004900399400898652,
      "loss": 0.0019,
      "step": 1995
    },
    {
      "epoch": 0.9965052421367948,
      "grad_norm": 0.42403262853622437,
      "learning_rate": 0.0004900349475786321,
      "loss": 0.024,
      "step": 1996
    },
    {
      "epoch": 0.9970044932601099,
      "grad_norm": 0.2338208258152008,
      "learning_rate": 0.0004900299550673989,
      "loss": 0.0094,
      "step": 1997
    },
    {
      "epoch": 0.9975037443834248,
      "grad_norm": 0.03878636658191681,
      "learning_rate": 0.0004900249625561658,
      "loss": 0.0014,
      "step": 1998
    },
    {
      "epoch": 0.9980029955067399,
      "grad_norm": 0.3549731373786926,
      "learning_rate": 0.0004900199700449326,
      "loss": 0.011,
      "step": 1999
    },
    {
      "epoch": 0.9985022466300549,
      "grad_norm": 1.2737237215042114,
      "learning_rate": 0.0004900149775336995,
      "loss": 0.017,
      "step": 2000
    },
    {
      "epoch": 0.99900149775337,
      "grad_norm": 0.6422103047370911,
      "learning_rate": 0.0004900099850224663,
      "loss": 0.0045,
      "step": 2001
    },
    {
      "epoch": 0.9995007488766849,
      "grad_norm": 0.36410072445869446,
      "learning_rate": 0.0004900049925112332,
      "loss": 0.0142,
      "step": 2002
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.3087785542011261,
      "learning_rate": 0.00049,
      "loss": 0.0125,
      "step": 2003
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.12974469363689423,
      "eval_runtime": 2267.8525,
      "eval_samples_per_second": 2.584,
      "eval_steps_per_second": 0.216,
      "step": 2003
    },
    {
      "epoch": 1.000499251123315,
      "grad_norm": 0.2066185027360916,
      "learning_rate": 0.0004899950074887669,
      "loss": 0.0045,
      "step": 2004
    },
    {
      "epoch": 1.0009985022466301,
      "grad_norm": 0.0897141620516777,
      "learning_rate": 0.0004899900149775337,
      "loss": 0.0028,
      "step": 2005
    },
    {
      "epoch": 1.0014977533699452,
      "grad_norm": 0.21218307316303253,
      "learning_rate": 0.0004899850224663006,
      "loss": 0.0058,
      "step": 2006
    },
    {
      "epoch": 1.00199700449326,
      "grad_norm": 0.06107938289642334,
      "learning_rate": 0.0004899800299550674,
      "loss": 0.0022,
      "step": 2007
    },
    {
      "epoch": 1.002496255616575,
      "grad_norm": 0.3440806269645691,
      "learning_rate": 0.0004899750374438343,
      "loss": 0.0273,
      "step": 2008
    },
    {
      "epoch": 1.0029955067398901,
      "grad_norm": 0.3990103304386139,
      "learning_rate": 0.0004899700449326011,
      "loss": 0.014,
      "step": 2009
    },
    {
      "epoch": 1.0034947578632052,
      "grad_norm": 0.12448987364768982,
      "learning_rate": 0.000489965052421368,
      "loss": 0.0027,
      "step": 2010
    },
    {
      "epoch": 1.0039940089865202,
      "grad_norm": 0.11596666276454926,
      "learning_rate": 0.0004899600599101348,
      "loss": 0.0032,
      "step": 2011
    },
    {
      "epoch": 1.0044932601098353,
      "grad_norm": 0.2653769850730896,
      "learning_rate": 0.0004899550673989017,
      "loss": 0.005,
      "step": 2012
    },
    {
      "epoch": 1.0049925112331504,
      "grad_norm": 0.12872664630413055,
      "learning_rate": 0.0004899500748876685,
      "loss": 0.0029,
      "step": 2013
    },
    {
      "epoch": 1.0054917623564652,
      "grad_norm": 0.39663851261138916,
      "learning_rate": 0.0004899450823764354,
      "loss": 0.0071,
      "step": 2014
    },
    {
      "epoch": 1.0059910134797803,
      "grad_norm": 0.43306735157966614,
      "learning_rate": 0.0004899400898652022,
      "loss": 0.0144,
      "step": 2015
    },
    {
      "epoch": 1.0064902646030953,
      "grad_norm": 0.40370893478393555,
      "learning_rate": 0.0004899350973539691,
      "loss": 0.0095,
      "step": 2016
    },
    {
      "epoch": 1.0069895157264104,
      "grad_norm": 0.127635657787323,
      "learning_rate": 0.0004899301048427359,
      "loss": 0.0021,
      "step": 2017
    },
    {
      "epoch": 1.0074887668497254,
      "grad_norm": 0.7016258835792542,
      "learning_rate": 0.0004899251123315028,
      "loss": 0.0085,
      "step": 2018
    },
    {
      "epoch": 1.0079880179730405,
      "grad_norm": 1.5285649299621582,
      "learning_rate": 0.0004899201198202696,
      "loss": 0.0211,
      "step": 2019
    },
    {
      "epoch": 1.0084872690963556,
      "grad_norm": 0.07073266059160233,
      "learning_rate": 0.0004899151273090365,
      "loss": 0.0026,
      "step": 2020
    },
    {
      "epoch": 1.0089865202196704,
      "grad_norm": 0.16910864412784576,
      "learning_rate": 0.0004899101347978033,
      "loss": 0.003,
      "step": 2021
    },
    {
      "epoch": 1.0094857713429854,
      "grad_norm": 0.253683477640152,
      "learning_rate": 0.0004899051422865702,
      "loss": 0.0069,
      "step": 2022
    },
    {
      "epoch": 1.0099850224663005,
      "grad_norm": 0.41217952966690063,
      "learning_rate": 0.000489900149775337,
      "loss": 0.0052,
      "step": 2023
    },
    {
      "epoch": 1.0104842735896156,
      "grad_norm": 1.1334983110427856,
      "learning_rate": 0.0004898951572641038,
      "loss": 0.0062,
      "step": 2024
    },
    {
      "epoch": 1.0109835247129306,
      "grad_norm": 0.07266679406166077,
      "learning_rate": 0.0004898901647528707,
      "loss": 0.0022,
      "step": 2025
    },
    {
      "epoch": 1.0114827758362457,
      "grad_norm": 0.4400433599948883,
      "learning_rate": 0.0004898851722416375,
      "loss": 0.006,
      "step": 2026
    },
    {
      "epoch": 1.0119820269595607,
      "grad_norm": 0.21279509365558624,
      "learning_rate": 0.0004898801797304044,
      "loss": 0.0065,
      "step": 2027
    },
    {
      "epoch": 1.0124812780828756,
      "grad_norm": 0.17204593122005463,
      "learning_rate": 0.0004898751872191712,
      "loss": 0.0035,
      "step": 2028
    },
    {
      "epoch": 1.0129805292061906,
      "grad_norm": 0.8737779855728149,
      "learning_rate": 0.0004898701947079381,
      "loss": 0.006,
      "step": 2029
    },
    {
      "epoch": 1.0134797803295057,
      "grad_norm": 0.12419338524341583,
      "learning_rate": 0.0004898652021967049,
      "loss": 0.0026,
      "step": 2030
    },
    {
      "epoch": 1.0139790314528208,
      "grad_norm": 0.21716992557048798,
      "learning_rate": 0.0004898602096854718,
      "loss": 0.0035,
      "step": 2031
    },
    {
      "epoch": 1.0144782825761358,
      "grad_norm": 0.4538830816745758,
      "learning_rate": 0.0004898552171742386,
      "loss": 0.0227,
      "step": 2032
    },
    {
      "epoch": 1.0149775336994509,
      "grad_norm": 0.4744125306606293,
      "learning_rate": 0.0004898502246630055,
      "loss": 0.0065,
      "step": 2033
    },
    {
      "epoch": 1.015476784822766,
      "grad_norm": 0.1071794182062149,
      "learning_rate": 0.0004898452321517723,
      "loss": 0.0022,
      "step": 2034
    },
    {
      "epoch": 1.015976035946081,
      "grad_norm": 0.3160560131072998,
      "learning_rate": 0.0004898402396405393,
      "loss": 0.0056,
      "step": 2035
    },
    {
      "epoch": 1.0164752870693958,
      "grad_norm": 0.08790404349565506,
      "learning_rate": 0.000489835247129306,
      "loss": 0.0013,
      "step": 2036
    },
    {
      "epoch": 1.0169745381927109,
      "grad_norm": 0.11892145127058029,
      "learning_rate": 0.000489830254618073,
      "loss": 0.0025,
      "step": 2037
    },
    {
      "epoch": 1.017473789316026,
      "grad_norm": 0.24893628060817719,
      "learning_rate": 0.0004898252621068398,
      "loss": 0.026,
      "step": 2038
    },
    {
      "epoch": 1.017973040439341,
      "grad_norm": 0.6630588173866272,
      "learning_rate": 0.0004898202695956067,
      "loss": 0.0595,
      "step": 2039
    },
    {
      "epoch": 1.018472291562656,
      "grad_norm": 0.37244513630867004,
      "learning_rate": 0.0004898152770843735,
      "loss": 0.0038,
      "step": 2040
    },
    {
      "epoch": 1.0189715426859711,
      "grad_norm": 1.6321197748184204,
      "learning_rate": 0.0004898102845731403,
      "loss": 0.0216,
      "step": 2041
    },
    {
      "epoch": 1.0194707938092862,
      "grad_norm": 0.33170104026794434,
      "learning_rate": 0.0004898052920619072,
      "loss": 0.0034,
      "step": 2042
    },
    {
      "epoch": 1.019970044932601,
      "grad_norm": 0.5598453283309937,
      "learning_rate": 0.000489800299550674,
      "loss": 0.0152,
      "step": 2043
    },
    {
      "epoch": 1.020469296055916,
      "grad_norm": 0.207366943359375,
      "learning_rate": 0.0004897953070394409,
      "loss": 0.0084,
      "step": 2044
    },
    {
      "epoch": 1.0209685471792311,
      "grad_norm": 0.37824034690856934,
      "learning_rate": 0.0004897903145282077,
      "loss": 0.0041,
      "step": 2045
    },
    {
      "epoch": 1.0214677983025462,
      "grad_norm": 0.045047976076602936,
      "learning_rate": 0.0004897853220169746,
      "loss": 0.0018,
      "step": 2046
    },
    {
      "epoch": 1.0219670494258613,
      "grad_norm": 0.043170128017663956,
      "learning_rate": 0.0004897803295057414,
      "loss": 0.0018,
      "step": 2047
    },
    {
      "epoch": 1.0224663005491763,
      "grad_norm": 0.1131131500005722,
      "learning_rate": 0.0004897753369945083,
      "loss": 0.0023,
      "step": 2048
    },
    {
      "epoch": 1.0229655516724914,
      "grad_norm": 0.18132105469703674,
      "learning_rate": 0.0004897703444832751,
      "loss": 0.0045,
      "step": 2049
    },
    {
      "epoch": 1.0234648027958062,
      "grad_norm": 0.09033764153718948,
      "learning_rate": 0.000489765351972042,
      "loss": 0.0024,
      "step": 2050
    },
    {
      "epoch": 1.0239640539191213,
      "grad_norm": 1.1361408233642578,
      "learning_rate": 0.0004897603594608088,
      "loss": 0.0157,
      "step": 2051
    },
    {
      "epoch": 1.0244633050424363,
      "grad_norm": 0.0989636704325676,
      "learning_rate": 0.0004897553669495756,
      "loss": 0.004,
      "step": 2052
    },
    {
      "epoch": 1.0249625561657514,
      "grad_norm": 0.28380560874938965,
      "learning_rate": 0.0004897503744383425,
      "loss": 0.0074,
      "step": 2053
    },
    {
      "epoch": 1.0254618072890664,
      "grad_norm": 0.058545686304569244,
      "learning_rate": 0.0004897453819271093,
      "loss": 0.0018,
      "step": 2054
    },
    {
      "epoch": 1.0259610584123815,
      "grad_norm": 0.04424445331096649,
      "learning_rate": 0.0004897403894158762,
      "loss": 0.002,
      "step": 2055
    },
    {
      "epoch": 1.0264603095356966,
      "grad_norm": 0.2352495789527893,
      "learning_rate": 0.000489735396904643,
      "loss": 0.0049,
      "step": 2056
    },
    {
      "epoch": 1.0269595606590114,
      "grad_norm": 0.12814709544181824,
      "learning_rate": 0.0004897304043934099,
      "loss": 0.0031,
      "step": 2057
    },
    {
      "epoch": 1.0274588117823265,
      "grad_norm": 0.07669726759195328,
      "learning_rate": 0.0004897254118821767,
      "loss": 0.0022,
      "step": 2058
    },
    {
      "epoch": 1.0279580629056415,
      "grad_norm": 0.06326141208410263,
      "learning_rate": 0.0004897204193709436,
      "loss": 0.0021,
      "step": 2059
    },
    {
      "epoch": 1.0284573140289566,
      "grad_norm": 0.6017916202545166,
      "learning_rate": 0.0004897154268597104,
      "loss": 0.0219,
      "step": 2060
    },
    {
      "epoch": 1.0289565651522716,
      "grad_norm": 0.06981588155031204,
      "learning_rate": 0.0004897104343484773,
      "loss": 0.0024,
      "step": 2061
    },
    {
      "epoch": 1.0294558162755867,
      "grad_norm": 0.04486255720257759,
      "learning_rate": 0.0004897054418372441,
      "loss": 0.0015,
      "step": 2062
    },
    {
      "epoch": 1.0299550673989017,
      "grad_norm": 0.263258695602417,
      "learning_rate": 0.000489700449326011,
      "loss": 0.0054,
      "step": 2063
    },
    {
      "epoch": 1.0304543185222166,
      "grad_norm": 0.1660955846309662,
      "learning_rate": 0.0004896954568147778,
      "loss": 0.0031,
      "step": 2064
    },
    {
      "epoch": 1.0309535696455316,
      "grad_norm": 0.12182999402284622,
      "learning_rate": 0.0004896904643035447,
      "loss": 0.002,
      "step": 2065
    },
    {
      "epoch": 1.0314528207688467,
      "grad_norm": 0.0670236349105835,
      "learning_rate": 0.0004896854717923115,
      "loss": 0.0012,
      "step": 2066
    },
    {
      "epoch": 1.0319520718921618,
      "grad_norm": 0.16027848422527313,
      "learning_rate": 0.0004896804792810784,
      "loss": 0.0025,
      "step": 2067
    },
    {
      "epoch": 1.0324513230154768,
      "grad_norm": 0.3258454203605652,
      "learning_rate": 0.0004896754867698452,
      "loss": 0.0091,
      "step": 2068
    },
    {
      "epoch": 1.0329505741387919,
      "grad_norm": 0.13242964446544647,
      "learning_rate": 0.0004896704942586121,
      "loss": 0.0021,
      "step": 2069
    },
    {
      "epoch": 1.033449825262107,
      "grad_norm": 0.1028941348195076,
      "learning_rate": 0.0004896655017473789,
      "loss": 0.0025,
      "step": 2070
    },
    {
      "epoch": 1.0339490763854218,
      "grad_norm": 0.6299347877502441,
      "learning_rate": 0.0004896605092361458,
      "loss": 0.0041,
      "step": 2071
    },
    {
      "epoch": 1.0344483275087368,
      "grad_norm": 0.21191707253456116,
      "learning_rate": 0.0004896555167249126,
      "loss": 0.0057,
      "step": 2072
    },
    {
      "epoch": 1.034947578632052,
      "grad_norm": 0.04063164442777634,
      "learning_rate": 0.0004896505242136795,
      "loss": 0.0016,
      "step": 2073
    },
    {
      "epoch": 1.035446829755367,
      "grad_norm": 0.161103293299675,
      "learning_rate": 0.0004896455317024463,
      "loss": 0.0031,
      "step": 2074
    },
    {
      "epoch": 1.035946080878682,
      "grad_norm": 0.06764453649520874,
      "learning_rate": 0.0004896405391912132,
      "loss": 0.0021,
      "step": 2075
    },
    {
      "epoch": 1.036445332001997,
      "grad_norm": 0.4427827298641205,
      "learning_rate": 0.00048963554667998,
      "loss": 0.0115,
      "step": 2076
    },
    {
      "epoch": 1.0369445831253121,
      "grad_norm": 0.4976519048213959,
      "learning_rate": 0.0004896305541687469,
      "loss": 0.0085,
      "step": 2077
    },
    {
      "epoch": 1.037443834248627,
      "grad_norm": 0.09854447841644287,
      "learning_rate": 0.0004896255616575137,
      "loss": 0.0018,
      "step": 2078
    },
    {
      "epoch": 1.037943085371942,
      "grad_norm": 0.08864152431488037,
      "learning_rate": 0.0004896205691462806,
      "loss": 0.0018,
      "step": 2079
    },
    {
      "epoch": 1.038442336495257,
      "grad_norm": 0.3387799561023712,
      "learning_rate": 0.0004896155766350474,
      "loss": 0.0031,
      "step": 2080
    },
    {
      "epoch": 1.0389415876185721,
      "grad_norm": 0.05730396509170532,
      "learning_rate": 0.0004896105841238142,
      "loss": 0.0018,
      "step": 2081
    },
    {
      "epoch": 1.0394408387418872,
      "grad_norm": 0.1815425455570221,
      "learning_rate": 0.0004896055916125811,
      "loss": 0.0042,
      "step": 2082
    },
    {
      "epoch": 1.0399400898652023,
      "grad_norm": 0.01850331947207451,
      "learning_rate": 0.0004896005991013479,
      "loss": 0.0011,
      "step": 2083
    },
    {
      "epoch": 1.0404393409885173,
      "grad_norm": 0.2624652683734894,
      "learning_rate": 0.0004895956065901148,
      "loss": 0.0053,
      "step": 2084
    },
    {
      "epoch": 1.0409385921118322,
      "grad_norm": 0.18389280140399933,
      "learning_rate": 0.0004895906140788816,
      "loss": 0.0034,
      "step": 2085
    },
    {
      "epoch": 1.0414378432351472,
      "grad_norm": 0.46388763189315796,
      "learning_rate": 0.0004895856215676485,
      "loss": 0.0101,
      "step": 2086
    },
    {
      "epoch": 1.0419370943584623,
      "grad_norm": 0.1344447135925293,
      "learning_rate": 0.0004895806290564153,
      "loss": 0.0026,
      "step": 2087
    },
    {
      "epoch": 1.0424363454817773,
      "grad_norm": 0.3078923225402832,
      "learning_rate": 0.0004895756365451822,
      "loss": 0.0099,
      "step": 2088
    },
    {
      "epoch": 1.0429355966050924,
      "grad_norm": 0.015547762624919415,
      "learning_rate": 0.000489570644033949,
      "loss": 0.001,
      "step": 2089
    },
    {
      "epoch": 1.0434348477284074,
      "grad_norm": 0.1773965209722519,
      "learning_rate": 0.000489565651522716,
      "loss": 0.0035,
      "step": 2090
    },
    {
      "epoch": 1.0439340988517225,
      "grad_norm": 0.03570764511823654,
      "learning_rate": 0.0004895606590114827,
      "loss": 0.0011,
      "step": 2091
    },
    {
      "epoch": 1.0444333499750373,
      "grad_norm": 0.026747548952698708,
      "learning_rate": 0.0004895556665002497,
      "loss": 0.0011,
      "step": 2092
    },
    {
      "epoch": 1.0449326010983524,
      "grad_norm": 0.07388930022716522,
      "learning_rate": 0.0004895506739890165,
      "loss": 0.0015,
      "step": 2093
    },
    {
      "epoch": 1.0454318522216675,
      "grad_norm": 0.09328030049800873,
      "learning_rate": 0.0004895456814777834,
      "loss": 0.0016,
      "step": 2094
    },
    {
      "epoch": 1.0459311033449825,
      "grad_norm": 0.29583603143692017,
      "learning_rate": 0.0004895406889665502,
      "loss": 0.0059,
      "step": 2095
    },
    {
      "epoch": 1.0464303544682976,
      "grad_norm": 0.08509964495897293,
      "learning_rate": 0.0004895356964553171,
      "loss": 0.0017,
      "step": 2096
    },
    {
      "epoch": 1.0469296055916126,
      "grad_norm": 0.03682109713554382,
      "learning_rate": 0.0004895307039440839,
      "loss": 0.001,
      "step": 2097
    },
    {
      "epoch": 1.0474288567149277,
      "grad_norm": 0.048005882650613785,
      "learning_rate": 0.0004895257114328508,
      "loss": 0.0013,
      "step": 2098
    },
    {
      "epoch": 1.0479281078382425,
      "grad_norm": 0.09874967485666275,
      "learning_rate": 0.0004895207189216176,
      "loss": 0.0024,
      "step": 2099
    },
    {
      "epoch": 1.0484273589615576,
      "grad_norm": 0.030774779617786407,
      "learning_rate": 0.0004895157264103845,
      "loss": 0.0008,
      "step": 2100
    },
    {
      "epoch": 1.0489266100848726,
      "grad_norm": 0.040918365120887756,
      "learning_rate": 0.0004895107338991513,
      "loss": 0.001,
      "step": 2101
    },
    {
      "epoch": 1.0494258612081877,
      "grad_norm": 0.16434288024902344,
      "learning_rate": 0.0004895057413879182,
      "loss": 0.0017,
      "step": 2102
    },
    {
      "epoch": 1.0499251123315028,
      "grad_norm": 0.011861352249979973,
      "learning_rate": 0.000489500748876685,
      "loss": 0.0008,
      "step": 2103
    },
    {
      "epoch": 1.0504243634548178,
      "grad_norm": 0.07560987770557404,
      "learning_rate": 0.0004894957563654519,
      "loss": 0.0016,
      "step": 2104
    },
    {
      "epoch": 1.0509236145781329,
      "grad_norm": 0.8417962193489075,
      "learning_rate": 0.0004894907638542187,
      "loss": 0.0162,
      "step": 2105
    },
    {
      "epoch": 1.051422865701448,
      "grad_norm": 0.1917361319065094,
      "learning_rate": 0.0004894857713429856,
      "loss": 0.0024,
      "step": 2106
    },
    {
      "epoch": 1.0519221168247628,
      "grad_norm": 0.021312711760401726,
      "learning_rate": 0.0004894807788317524,
      "loss": 0.001,
      "step": 2107
    },
    {
      "epoch": 1.0524213679480778,
      "grad_norm": 0.03599468246102333,
      "learning_rate": 0.0004894757863205193,
      "loss": 0.0011,
      "step": 2108
    },
    {
      "epoch": 1.052920619071393,
      "grad_norm": 0.03181013837456703,
      "learning_rate": 0.0004894707938092861,
      "loss": 0.001,
      "step": 2109
    },
    {
      "epoch": 1.053419870194708,
      "grad_norm": 0.029543843120336533,
      "learning_rate": 0.0004894658012980529,
      "loss": 0.001,
      "step": 2110
    },
    {
      "epoch": 1.053919121318023,
      "grad_norm": 0.40300634503364563,
      "learning_rate": 0.0004894608087868198,
      "loss": 0.0057,
      "step": 2111
    },
    {
      "epoch": 1.054418372441338,
      "grad_norm": 0.05447869002819061,
      "learning_rate": 0.0004894558162755866,
      "loss": 0.0015,
      "step": 2112
    },
    {
      "epoch": 1.054917623564653,
      "grad_norm": 0.4427923560142517,
      "learning_rate": 0.0004894508237643535,
      "loss": 0.0161,
      "step": 2113
    },
    {
      "epoch": 1.055416874687968,
      "grad_norm": 0.7168645858764648,
      "learning_rate": 0.0004894458312531203,
      "loss": 0.0075,
      "step": 2114
    },
    {
      "epoch": 1.055916125811283,
      "grad_norm": 0.23788800835609436,
      "learning_rate": 0.0004894408387418872,
      "loss": 0.0041,
      "step": 2115
    },
    {
      "epoch": 1.056415376934598,
      "grad_norm": 0.5305008888244629,
      "learning_rate": 0.000489435846230654,
      "loss": 0.0085,
      "step": 2116
    },
    {
      "epoch": 1.0569146280579131,
      "grad_norm": 0.34571748971939087,
      "learning_rate": 0.0004894308537194209,
      "loss": 0.0059,
      "step": 2117
    },
    {
      "epoch": 1.0574138791812282,
      "grad_norm": 0.38437315821647644,
      "learning_rate": 0.0004894258612081877,
      "loss": 0.0048,
      "step": 2118
    },
    {
      "epoch": 1.0579131303045433,
      "grad_norm": 0.010811745189130306,
      "learning_rate": 0.0004894208686969546,
      "loss": 0.0006,
      "step": 2119
    },
    {
      "epoch": 1.0584123814278583,
      "grad_norm": 0.09970958530902863,
      "learning_rate": 0.0004894158761857214,
      "loss": 0.0022,
      "step": 2120
    },
    {
      "epoch": 1.0589116325511732,
      "grad_norm": 0.03454634174704552,
      "learning_rate": 0.0004894108836744883,
      "loss": 0.0012,
      "step": 2121
    },
    {
      "epoch": 1.0594108836744882,
      "grad_norm": 0.4273810386657715,
      "learning_rate": 0.0004894058911632551,
      "loss": 0.0485,
      "step": 2122
    },
    {
      "epoch": 1.0599101347978033,
      "grad_norm": 0.028179466724395752,
      "learning_rate": 0.000489400898652022,
      "loss": 0.0009,
      "step": 2123
    },
    {
      "epoch": 1.0604093859211183,
      "grad_norm": 0.1640303134918213,
      "learning_rate": 0.0004893959061407888,
      "loss": 0.0038,
      "step": 2124
    },
    {
      "epoch": 1.0609086370444334,
      "grad_norm": 0.25881338119506836,
      "learning_rate": 0.0004893909136295557,
      "loss": 0.0039,
      "step": 2125
    },
    {
      "epoch": 1.0614078881677484,
      "grad_norm": 0.11668124049901962,
      "learning_rate": 0.0004893859211183225,
      "loss": 0.0025,
      "step": 2126
    },
    {
      "epoch": 1.0619071392910635,
      "grad_norm": 0.016885880380868912,
      "learning_rate": 0.0004893809286070894,
      "loss": 0.0007,
      "step": 2127
    },
    {
      "epoch": 1.0624063904143783,
      "grad_norm": 1.6870672702789307,
      "learning_rate": 0.0004893759360958562,
      "loss": 0.0199,
      "step": 2128
    },
    {
      "epoch": 1.0629056415376934,
      "grad_norm": 0.29725152254104614,
      "learning_rate": 0.0004893709435846231,
      "loss": 0.004,
      "step": 2129
    },
    {
      "epoch": 1.0634048926610085,
      "grad_norm": 0.00931502878665924,
      "learning_rate": 0.0004893659510733899,
      "loss": 0.0008,
      "step": 2130
    },
    {
      "epoch": 1.0639041437843235,
      "grad_norm": 0.1264919936656952,
      "learning_rate": 0.0004893609585621568,
      "loss": 0.002,
      "step": 2131
    },
    {
      "epoch": 1.0644033949076386,
      "grad_norm": 0.024735258892178535,
      "learning_rate": 0.0004893559660509236,
      "loss": 0.0009,
      "step": 2132
    },
    {
      "epoch": 1.0649026460309536,
      "grad_norm": 0.18554943799972534,
      "learning_rate": 0.0004893509735396905,
      "loss": 0.0022,
      "step": 2133
    },
    {
      "epoch": 1.0654018971542687,
      "grad_norm": 0.025447236374020576,
      "learning_rate": 0.0004893459810284573,
      "loss": 0.0012,
      "step": 2134
    },
    {
      "epoch": 1.0659011482775835,
      "grad_norm": 0.2698434889316559,
      "learning_rate": 0.0004893409885172242,
      "loss": 0.0102,
      "step": 2135
    },
    {
      "epoch": 1.0664003994008986,
      "grad_norm": 0.04213780164718628,
      "learning_rate": 0.000489335996005991,
      "loss": 0.0017,
      "step": 2136
    },
    {
      "epoch": 1.0668996505242136,
      "grad_norm": 0.1622568517923355,
      "learning_rate": 0.0004893310034947579,
      "loss": 0.0053,
      "step": 2137
    },
    {
      "epoch": 1.0673989016475287,
      "grad_norm": 0.16170348227024078,
      "learning_rate": 0.0004893260109835247,
      "loss": 0.0039,
      "step": 2138
    },
    {
      "epoch": 1.0678981527708438,
      "grad_norm": 0.6057206988334656,
      "learning_rate": 0.0004893210184722915,
      "loss": 0.0258,
      "step": 2139
    },
    {
      "epoch": 1.0683974038941588,
      "grad_norm": 0.08010169863700867,
      "learning_rate": 0.0004893160259610584,
      "loss": 0.0013,
      "step": 2140
    },
    {
      "epoch": 1.0688966550174739,
      "grad_norm": 0.0250544510781765,
      "learning_rate": 0.0004893110334498252,
      "loss": 0.0009,
      "step": 2141
    },
    {
      "epoch": 1.0693959061407887,
      "grad_norm": 1.423120141029358,
      "learning_rate": 0.0004893060409385922,
      "loss": 0.035,
      "step": 2142
    },
    {
      "epoch": 1.0698951572641038,
      "grad_norm": 0.06462370604276657,
      "learning_rate": 0.000489301048427359,
      "loss": 0.0014,
      "step": 2143
    },
    {
      "epoch": 1.0703944083874188,
      "grad_norm": 0.023556696251034737,
      "learning_rate": 0.0004892960559161259,
      "loss": 0.001,
      "step": 2144
    },
    {
      "epoch": 1.070893659510734,
      "grad_norm": 0.22663980722427368,
      "learning_rate": 0.0004892910634048927,
      "loss": 0.0021,
      "step": 2145
    },
    {
      "epoch": 1.071392910634049,
      "grad_norm": 0.041093699634075165,
      "learning_rate": 0.0004892860708936596,
      "loss": 0.0013,
      "step": 2146
    },
    {
      "epoch": 1.071892161757364,
      "grad_norm": 0.06400071829557419,
      "learning_rate": 0.0004892810783824264,
      "loss": 0.0015,
      "step": 2147
    },
    {
      "epoch": 1.072391412880679,
      "grad_norm": 0.0883876383304596,
      "learning_rate": 0.0004892760858711933,
      "loss": 0.0016,
      "step": 2148
    },
    {
      "epoch": 1.072890664003994,
      "grad_norm": 0.17423607409000397,
      "learning_rate": 0.0004892710933599601,
      "loss": 0.0046,
      "step": 2149
    },
    {
      "epoch": 1.073389915127309,
      "grad_norm": 1.4149805307388306,
      "learning_rate": 0.000489266100848727,
      "loss": 0.0285,
      "step": 2150
    },
    {
      "epoch": 1.073889166250624,
      "grad_norm": 1.2238109111785889,
      "learning_rate": 0.0004892611083374938,
      "loss": 0.0188,
      "step": 2151
    },
    {
      "epoch": 1.074388417373939,
      "grad_norm": 0.06801720708608627,
      "learning_rate": 0.0004892561158262607,
      "loss": 0.0016,
      "step": 2152
    },
    {
      "epoch": 1.0748876684972541,
      "grad_norm": 0.08617277443408966,
      "learning_rate": 0.0004892511233150275,
      "loss": 0.0018,
      "step": 2153
    },
    {
      "epoch": 1.0753869196205692,
      "grad_norm": 0.24823716282844543,
      "learning_rate": 0.0004892461308037943,
      "loss": 0.004,
      "step": 2154
    },
    {
      "epoch": 1.0758861707438843,
      "grad_norm": 0.02214200608432293,
      "learning_rate": 0.0004892411382925612,
      "loss": 0.001,
      "step": 2155
    },
    {
      "epoch": 1.076385421867199,
      "grad_norm": 0.04990145564079285,
      "learning_rate": 0.000489236145781328,
      "loss": 0.0015,
      "step": 2156
    },
    {
      "epoch": 1.0768846729905142,
      "grad_norm": 0.23540718853473663,
      "learning_rate": 0.0004892311532700949,
      "loss": 0.0032,
      "step": 2157
    },
    {
      "epoch": 1.0773839241138292,
      "grad_norm": 0.29650232195854187,
      "learning_rate": 0.0004892261607588617,
      "loss": 0.0046,
      "step": 2158
    },
    {
      "epoch": 1.0778831752371443,
      "grad_norm": 0.41083768010139465,
      "learning_rate": 0.0004892211682476286,
      "loss": 0.0168,
      "step": 2159
    },
    {
      "epoch": 1.0783824263604593,
      "grad_norm": 1.738964319229126,
      "learning_rate": 0.0004892161757363954,
      "loss": 0.0252,
      "step": 2160
    },
    {
      "epoch": 1.0788816774837744,
      "grad_norm": 0.4175751507282257,
      "learning_rate": 0.0004892111832251623,
      "loss": 0.0065,
      "step": 2161
    },
    {
      "epoch": 1.0793809286070895,
      "grad_norm": 0.10389905422925949,
      "learning_rate": 0.0004892061907139291,
      "loss": 0.0022,
      "step": 2162
    },
    {
      "epoch": 1.0798801797304045,
      "grad_norm": 0.06417590379714966,
      "learning_rate": 0.000489201198202696,
      "loss": 0.0021,
      "step": 2163
    },
    {
      "epoch": 1.0803794308537193,
      "grad_norm": 0.16317394375801086,
      "learning_rate": 0.0004891962056914628,
      "loss": 0.002,
      "step": 2164
    },
    {
      "epoch": 1.0808786819770344,
      "grad_norm": 0.08773721009492874,
      "learning_rate": 0.0004891912131802297,
      "loss": 0.0021,
      "step": 2165
    },
    {
      "epoch": 1.0813779331003495,
      "grad_norm": 0.15359656512737274,
      "learning_rate": 0.0004891862206689965,
      "loss": 0.0038,
      "step": 2166
    },
    {
      "epoch": 1.0818771842236645,
      "grad_norm": 0.07308275252580643,
      "learning_rate": 0.0004891812281577634,
      "loss": 0.0021,
      "step": 2167
    },
    {
      "epoch": 1.0823764353469796,
      "grad_norm": 0.5097276568412781,
      "learning_rate": 0.0004891762356465302,
      "loss": 0.0104,
      "step": 2168
    },
    {
      "epoch": 1.0828756864702946,
      "grad_norm": 0.40245699882507324,
      "learning_rate": 0.000489171243135297,
      "loss": 0.0079,
      "step": 2169
    },
    {
      "epoch": 1.0833749375936095,
      "grad_norm": 0.07522007822990417,
      "learning_rate": 0.0004891662506240639,
      "loss": 0.0021,
      "step": 2170
    },
    {
      "epoch": 1.0838741887169245,
      "grad_norm": 0.10150931030511856,
      "learning_rate": 0.0004891612581128307,
      "loss": 0.0018,
      "step": 2171
    },
    {
      "epoch": 1.0843734398402396,
      "grad_norm": 0.5825700163841248,
      "learning_rate": 0.0004891562656015976,
      "loss": 0.0159,
      "step": 2172
    },
    {
      "epoch": 1.0848726909635547,
      "grad_norm": 0.14733368158340454,
      "learning_rate": 0.0004891512730903644,
      "loss": 0.0028,
      "step": 2173
    },
    {
      "epoch": 1.0853719420868697,
      "grad_norm": 0.05925637111067772,
      "learning_rate": 0.0004891462805791313,
      "loss": 0.0021,
      "step": 2174
    },
    {
      "epoch": 1.0858711932101848,
      "grad_norm": 0.25236645340919495,
      "learning_rate": 0.0004891412880678981,
      "loss": 0.0049,
      "step": 2175
    },
    {
      "epoch": 1.0863704443334998,
      "grad_norm": 0.19419756531715393,
      "learning_rate": 0.000489136295556665,
      "loss": 0.0025,
      "step": 2176
    },
    {
      "epoch": 1.0868696954568149,
      "grad_norm": 0.06914845108985901,
      "learning_rate": 0.0004891313030454318,
      "loss": 0.0024,
      "step": 2177
    },
    {
      "epoch": 1.0873689465801297,
      "grad_norm": 0.2410006821155548,
      "learning_rate": 0.0004891263105341987,
      "loss": 0.0048,
      "step": 2178
    },
    {
      "epoch": 1.0878681977034448,
      "grad_norm": 0.09557324647903442,
      "learning_rate": 0.0004891213180229655,
      "loss": 0.0022,
      "step": 2179
    },
    {
      "epoch": 1.0883674488267598,
      "grad_norm": 0.2690802216529846,
      "learning_rate": 0.0004891163255117324,
      "loss": 0.0059,
      "step": 2180
    },
    {
      "epoch": 1.088866699950075,
      "grad_norm": 0.10328686237335205,
      "learning_rate": 0.0004891113330004992,
      "loss": 0.0017,
      "step": 2181
    },
    {
      "epoch": 1.08936595107339,
      "grad_norm": 0.2286919802427292,
      "learning_rate": 0.0004891063404892661,
      "loss": 0.0154,
      "step": 2182
    },
    {
      "epoch": 1.089865202196705,
      "grad_norm": 0.3756570518016815,
      "learning_rate": 0.0004891013479780329,
      "loss": 0.0104,
      "step": 2183
    },
    {
      "epoch": 1.0903644533200199,
      "grad_norm": 0.02532457374036312,
      "learning_rate": 0.0004890963554667998,
      "loss": 0.0008,
      "step": 2184
    },
    {
      "epoch": 1.090863704443335,
      "grad_norm": 0.41889023780822754,
      "learning_rate": 0.0004890913629555666,
      "loss": 0.0098,
      "step": 2185
    },
    {
      "epoch": 1.09136295556665,
      "grad_norm": 0.04896140098571777,
      "learning_rate": 0.0004890863704443335,
      "loss": 0.001,
      "step": 2186
    },
    {
      "epoch": 1.091862206689965,
      "grad_norm": 0.00837443396449089,
      "learning_rate": 0.0004890813779331003,
      "loss": 0.0006,
      "step": 2187
    },
    {
      "epoch": 1.09236145781328,
      "grad_norm": 0.011844109743833542,
      "learning_rate": 0.0004890763854218672,
      "loss": 0.0009,
      "step": 2188
    },
    {
      "epoch": 1.0928607089365951,
      "grad_norm": 0.042992331087589264,
      "learning_rate": 0.000489071392910634,
      "loss": 0.0012,
      "step": 2189
    },
    {
      "epoch": 1.0933599600599102,
      "grad_norm": 0.32009467482566833,
      "learning_rate": 0.0004890664003994009,
      "loss": 0.0034,
      "step": 2190
    },
    {
      "epoch": 1.0938592111832253,
      "grad_norm": 0.04980374500155449,
      "learning_rate": 0.0004890614078881677,
      "loss": 0.0013,
      "step": 2191
    },
    {
      "epoch": 1.09435846230654,
      "grad_norm": 0.056923236697912216,
      "learning_rate": 0.0004890564153769346,
      "loss": 0.0015,
      "step": 2192
    },
    {
      "epoch": 1.0948577134298552,
      "grad_norm": 0.01677635870873928,
      "learning_rate": 0.0004890514228657014,
      "loss": 0.001,
      "step": 2193
    },
    {
      "epoch": 1.0953569645531702,
      "grad_norm": 0.45322200655937195,
      "learning_rate": 0.0004890464303544683,
      "loss": 0.0037,
      "step": 2194
    },
    {
      "epoch": 1.0958562156764853,
      "grad_norm": 0.10970836877822876,
      "learning_rate": 0.0004890414378432351,
      "loss": 0.001,
      "step": 2195
    },
    {
      "epoch": 1.0963554667998003,
      "grad_norm": 0.03672608733177185,
      "learning_rate": 0.000489036445332002,
      "loss": 0.0013,
      "step": 2196
    },
    {
      "epoch": 1.0968547179231154,
      "grad_norm": 0.024736933410167694,
      "learning_rate": 0.0004890314528207689,
      "loss": 0.001,
      "step": 2197
    },
    {
      "epoch": 1.0973539690464305,
      "grad_norm": 0.1906348317861557,
      "learning_rate": 0.0004890264603095356,
      "loss": 0.0021,
      "step": 2198
    },
    {
      "epoch": 1.0978532201697453,
      "grad_norm": 0.5402529239654541,
      "learning_rate": 0.0004890214677983026,
      "loss": 0.0074,
      "step": 2199
    },
    {
      "epoch": 1.0983524712930604,
      "grad_norm": 0.6785167455673218,
      "learning_rate": 0.0004890164752870694,
      "loss": 0.0022,
      "step": 2200
    },
    {
      "epoch": 1.0988517224163754,
      "grad_norm": 0.29454126954078674,
      "learning_rate": 0.0004890114827758363,
      "loss": 0.0036,
      "step": 2201
    },
    {
      "epoch": 1.0993509735396905,
      "grad_norm": 0.09805696457624435,
      "learning_rate": 0.0004890064902646031,
      "loss": 0.0014,
      "step": 2202
    },
    {
      "epoch": 1.0998502246630055,
      "grad_norm": 0.29119008779525757,
      "learning_rate": 0.00048900149775337,
      "loss": 0.0055,
      "step": 2203
    },
    {
      "epoch": 1.1003494757863206,
      "grad_norm": 0.0116581404581666,
      "learning_rate": 0.0004889965052421368,
      "loss": 0.0008,
      "step": 2204
    },
    {
      "epoch": 1.1008487269096356,
      "grad_norm": 0.04403958097100258,
      "learning_rate": 0.0004889915127309037,
      "loss": 0.0015,
      "step": 2205
    },
    {
      "epoch": 1.1013479780329505,
      "grad_norm": 0.02005649358034134,
      "learning_rate": 0.0004889865202196705,
      "loss": 0.0008,
      "step": 2206
    },
    {
      "epoch": 1.1018472291562655,
      "grad_norm": 0.4580632448196411,
      "learning_rate": 0.0004889815277084374,
      "loss": 0.0109,
      "step": 2207
    },
    {
      "epoch": 1.1023464802795806,
      "grad_norm": 0.059242796152830124,
      "learning_rate": 0.0004889765351972042,
      "loss": 0.0013,
      "step": 2208
    },
    {
      "epoch": 1.1028457314028957,
      "grad_norm": 0.062394749373197556,
      "learning_rate": 0.0004889715426859711,
      "loss": 0.0015,
      "step": 2209
    },
    {
      "epoch": 1.1033449825262107,
      "grad_norm": 0.07510124146938324,
      "learning_rate": 0.0004889665501747379,
      "loss": 0.002,
      "step": 2210
    },
    {
      "epoch": 1.1038442336495258,
      "grad_norm": 0.18278314173221588,
      "learning_rate": 0.0004889615576635048,
      "loss": 0.0029,
      "step": 2211
    },
    {
      "epoch": 1.1043434847728408,
      "grad_norm": 0.16297024488449097,
      "learning_rate": 0.0004889565651522716,
      "loss": 0.0036,
      "step": 2212
    },
    {
      "epoch": 1.1048427358961557,
      "grad_norm": 0.16222411394119263,
      "learning_rate": 0.0004889515726410385,
      "loss": 0.0028,
      "step": 2213
    },
    {
      "epoch": 1.1053419870194707,
      "grad_norm": 0.6923023462295532,
      "learning_rate": 0.0004889465801298053,
      "loss": 0.0177,
      "step": 2214
    },
    {
      "epoch": 1.1058412381427858,
      "grad_norm": 0.02999378927052021,
      "learning_rate": 0.0004889415876185722,
      "loss": 0.0011,
      "step": 2215
    },
    {
      "epoch": 1.1063404892661008,
      "grad_norm": 0.02652619406580925,
      "learning_rate": 0.000488936595107339,
      "loss": 0.001,
      "step": 2216
    },
    {
      "epoch": 1.106839740389416,
      "grad_norm": 0.19100762903690338,
      "learning_rate": 0.0004889316025961059,
      "loss": 0.0022,
      "step": 2217
    },
    {
      "epoch": 1.107338991512731,
      "grad_norm": 0.1462838500738144,
      "learning_rate": 0.0004889266100848727,
      "loss": 0.0017,
      "step": 2218
    },
    {
      "epoch": 1.107838242636046,
      "grad_norm": 0.30194738507270813,
      "learning_rate": 0.0004889216175736396,
      "loss": 0.0041,
      "step": 2219
    },
    {
      "epoch": 1.1083374937593609,
      "grad_norm": 0.02765638567507267,
      "learning_rate": 0.0004889166250624064,
      "loss": 0.001,
      "step": 2220
    },
    {
      "epoch": 1.108836744882676,
      "grad_norm": 0.027200259268283844,
      "learning_rate": 0.0004889116325511733,
      "loss": 0.0011,
      "step": 2221
    },
    {
      "epoch": 1.109335996005991,
      "grad_norm": 0.024530211463570595,
      "learning_rate": 0.0004889066400399401,
      "loss": 0.0011,
      "step": 2222
    },
    {
      "epoch": 1.109835247129306,
      "grad_norm": 0.17737628519535065,
      "learning_rate": 0.000488901647528707,
      "loss": 0.002,
      "step": 2223
    },
    {
      "epoch": 1.110334498252621,
      "grad_norm": 0.1267695277929306,
      "learning_rate": 0.0004888966550174738,
      "loss": 0.0022,
      "step": 2224
    },
    {
      "epoch": 1.1108337493759362,
      "grad_norm": 0.21900314092636108,
      "learning_rate": 0.0004888916625062407,
      "loss": 0.0022,
      "step": 2225
    },
    {
      "epoch": 1.1113330004992512,
      "grad_norm": 0.036235589534044266,
      "learning_rate": 0.0004888866699950075,
      "loss": 0.0011,
      "step": 2226
    },
    {
      "epoch": 1.111832251622566,
      "grad_norm": 0.08774963766336441,
      "learning_rate": 0.0004888816774837743,
      "loss": 0.0021,
      "step": 2227
    },
    {
      "epoch": 1.112331502745881,
      "grad_norm": 0.04415416345000267,
      "learning_rate": 0.0004888766849725412,
      "loss": 0.0013,
      "step": 2228
    },
    {
      "epoch": 1.1128307538691962,
      "grad_norm": 0.3691870868206024,
      "learning_rate": 0.000488871692461308,
      "loss": 0.0034,
      "step": 2229
    },
    {
      "epoch": 1.1133300049925112,
      "grad_norm": 0.1229805052280426,
      "learning_rate": 0.0004888666999500749,
      "loss": 0.0022,
      "step": 2230
    },
    {
      "epoch": 1.1138292561158263,
      "grad_norm": 0.3243542015552521,
      "learning_rate": 0.0004888617074388417,
      "loss": 0.0061,
      "step": 2231
    },
    {
      "epoch": 1.1143285072391413,
      "grad_norm": 0.1124475970864296,
      "learning_rate": 0.0004888567149276086,
      "loss": 0.0018,
      "step": 2232
    },
    {
      "epoch": 1.1148277583624564,
      "grad_norm": 0.20368219912052155,
      "learning_rate": 0.0004888517224163754,
      "loss": 0.0022,
      "step": 2233
    },
    {
      "epoch": 1.1153270094857715,
      "grad_norm": 0.2658284604549408,
      "learning_rate": 0.0004888467299051423,
      "loss": 0.0029,
      "step": 2234
    },
    {
      "epoch": 1.1158262606090863,
      "grad_norm": 0.14340458810329437,
      "learning_rate": 0.0004888417373939091,
      "loss": 0.0013,
      "step": 2235
    },
    {
      "epoch": 1.1163255117324014,
      "grad_norm": 0.01928509771823883,
      "learning_rate": 0.000488836744882676,
      "loss": 0.0007,
      "step": 2236
    },
    {
      "epoch": 1.1168247628557164,
      "grad_norm": 0.03762960806488991,
      "learning_rate": 0.0004888317523714428,
      "loss": 0.0012,
      "step": 2237
    },
    {
      "epoch": 1.1173240139790315,
      "grad_norm": 0.1281462460756302,
      "learning_rate": 0.0004888267598602097,
      "loss": 0.0023,
      "step": 2238
    },
    {
      "epoch": 1.1178232651023465,
      "grad_norm": 0.009487929753959179,
      "learning_rate": 0.0004888217673489765,
      "loss": 0.0007,
      "step": 2239
    },
    {
      "epoch": 1.1183225162256616,
      "grad_norm": 0.18479257822036743,
      "learning_rate": 0.0004888167748377434,
      "loss": 0.0051,
      "step": 2240
    },
    {
      "epoch": 1.1188217673489764,
      "grad_norm": 0.06048062443733215,
      "learning_rate": 0.0004888117823265102,
      "loss": 0.0013,
      "step": 2241
    },
    {
      "epoch": 1.1193210184722915,
      "grad_norm": 0.40469256043434143,
      "learning_rate": 0.0004888067898152771,
      "loss": 0.008,
      "step": 2242
    },
    {
      "epoch": 1.1198202695956065,
      "grad_norm": 0.08897323906421661,
      "learning_rate": 0.0004888017973040439,
      "loss": 0.0019,
      "step": 2243
    },
    {
      "epoch": 1.1203195207189216,
      "grad_norm": 0.02084122970700264,
      "learning_rate": 0.0004887968047928108,
      "loss": 0.0009,
      "step": 2244
    },
    {
      "epoch": 1.1208187718422367,
      "grad_norm": 0.02681279554963112,
      "learning_rate": 0.0004887918122815776,
      "loss": 0.0008,
      "step": 2245
    },
    {
      "epoch": 1.1213180229655517,
      "grad_norm": 0.2734111249446869,
      "learning_rate": 0.0004887868197703445,
      "loss": 0.0027,
      "step": 2246
    },
    {
      "epoch": 1.1218172740888668,
      "grad_norm": 0.0441623292863369,
      "learning_rate": 0.0004887818272591113,
      "loss": 0.0011,
      "step": 2247
    },
    {
      "epoch": 1.1223165252121818,
      "grad_norm": 0.5100741982460022,
      "learning_rate": 0.0004887768347478783,
      "loss": 0.0075,
      "step": 2248
    },
    {
      "epoch": 1.1228157763354967,
      "grad_norm": 0.06513617932796478,
      "learning_rate": 0.000488771842236645,
      "loss": 0.0016,
      "step": 2249
    },
    {
      "epoch": 1.1233150274588117,
      "grad_norm": 0.2578756511211395,
      "learning_rate": 0.000488766849725412,
      "loss": 0.0032,
      "step": 2250
    },
    {
      "epoch": 1.1238142785821268,
      "grad_norm": 0.1584518402814865,
      "learning_rate": 0.0004887618572141788,
      "loss": 0.0027,
      "step": 2251
    },
    {
      "epoch": 1.1243135297054418,
      "grad_norm": 0.010486200451850891,
      "learning_rate": 0.0004887568647029457,
      "loss": 0.0006,
      "step": 2252
    },
    {
      "epoch": 1.124812780828757,
      "grad_norm": 0.016700763255357742,
      "learning_rate": 0.0004887518721917125,
      "loss": 0.0008,
      "step": 2253
    },
    {
      "epoch": 1.125312031952072,
      "grad_norm": 0.10881691426038742,
      "learning_rate": 0.0004887468796804794,
      "loss": 0.0016,
      "step": 2254
    },
    {
      "epoch": 1.1258112830753868,
      "grad_norm": 0.020714184269309044,
      "learning_rate": 0.0004887418871692462,
      "loss": 0.0009,
      "step": 2255
    },
    {
      "epoch": 1.1263105341987019,
      "grad_norm": 0.013905389234423637,
      "learning_rate": 0.000488736894658013,
      "loss": 0.0008,
      "step": 2256
    },
    {
      "epoch": 1.126809785322017,
      "grad_norm": 0.2657482624053955,
      "learning_rate": 0.0004887319021467799,
      "loss": 0.0025,
      "step": 2257
    },
    {
      "epoch": 1.127309036445332,
      "grad_norm": 0.48048925399780273,
      "learning_rate": 0.0004887269096355467,
      "loss": 0.0116,
      "step": 2258
    },
    {
      "epoch": 1.127808287568647,
      "grad_norm": 0.1598169356584549,
      "learning_rate": 0.0004887219171243136,
      "loss": 0.0023,
      "step": 2259
    },
    {
      "epoch": 1.128307538691962,
      "grad_norm": 0.2942071259021759,
      "learning_rate": 0.0004887169246130804,
      "loss": 0.027,
      "step": 2260
    },
    {
      "epoch": 1.1288067898152772,
      "grad_norm": 0.2162381112575531,
      "learning_rate": 0.0004887119321018473,
      "loss": 0.0026,
      "step": 2261
    },
    {
      "epoch": 1.1293060409385922,
      "grad_norm": 0.18951651453971863,
      "learning_rate": 0.0004887069395906141,
      "loss": 0.0036,
      "step": 2262
    },
    {
      "epoch": 1.129805292061907,
      "grad_norm": 0.02341386303305626,
      "learning_rate": 0.000488701947079381,
      "loss": 0.0009,
      "step": 2263
    },
    {
      "epoch": 1.130304543185222,
      "grad_norm": 0.01613326370716095,
      "learning_rate": 0.0004886969545681478,
      "loss": 0.0008,
      "step": 2264
    },
    {
      "epoch": 1.1308037943085372,
      "grad_norm": 0.025682419538497925,
      "learning_rate": 0.0004886919620569147,
      "loss": 0.0009,
      "step": 2265
    },
    {
      "epoch": 1.1313030454318522,
      "grad_norm": 0.11664660274982452,
      "learning_rate": 0.0004886869695456815,
      "loss": 0.0017,
      "step": 2266
    },
    {
      "epoch": 1.1318022965551673,
      "grad_norm": 0.0557510107755661,
      "learning_rate": 0.0004886819770344483,
      "loss": 0.001,
      "step": 2267
    },
    {
      "epoch": 1.1323015476784823,
      "grad_norm": 0.062109656631946564,
      "learning_rate": 0.0004886769845232152,
      "loss": 0.0013,
      "step": 2268
    },
    {
      "epoch": 1.1328007988017972,
      "grad_norm": 0.11270971596240997,
      "learning_rate": 0.000488671992011982,
      "loss": 0.0015,
      "step": 2269
    },
    {
      "epoch": 1.1333000499251122,
      "grad_norm": 0.01880008541047573,
      "learning_rate": 0.0004886669995007489,
      "loss": 0.0007,
      "step": 2270
    },
    {
      "epoch": 1.1337993010484273,
      "grad_norm": 0.11264380067586899,
      "learning_rate": 0.0004886620069895157,
      "loss": 0.0016,
      "step": 2271
    },
    {
      "epoch": 1.1342985521717424,
      "grad_norm": 0.07267612963914871,
      "learning_rate": 0.0004886570144782826,
      "loss": 0.0012,
      "step": 2272
    },
    {
      "epoch": 1.1347978032950574,
      "grad_norm": 0.10022679716348648,
      "learning_rate": 0.0004886520219670494,
      "loss": 0.0018,
      "step": 2273
    },
    {
      "epoch": 1.1352970544183725,
      "grad_norm": 0.0564408041536808,
      "learning_rate": 0.0004886470294558163,
      "loss": 0.0012,
      "step": 2274
    },
    {
      "epoch": 1.1357963055416875,
      "grad_norm": 0.10024374723434448,
      "learning_rate": 0.0004886420369445831,
      "loss": 0.0012,
      "step": 2275
    },
    {
      "epoch": 1.1362955566650026,
      "grad_norm": 0.14182880520820618,
      "learning_rate": 0.00048863704443335,
      "loss": 0.0024,
      "step": 2276
    },
    {
      "epoch": 1.1367948077883174,
      "grad_norm": 0.029036259278655052,
      "learning_rate": 0.0004886320519221168,
      "loss": 0.001,
      "step": 2277
    },
    {
      "epoch": 1.1372940589116325,
      "grad_norm": 0.08777888864278793,
      "learning_rate": 0.0004886270594108837,
      "loss": 0.0011,
      "step": 2278
    },
    {
      "epoch": 1.1377933100349475,
      "grad_norm": 0.045640796422958374,
      "learning_rate": 0.0004886220668996505,
      "loss": 0.0009,
      "step": 2279
    },
    {
      "epoch": 1.1382925611582626,
      "grad_norm": 0.004231778439134359,
      "learning_rate": 0.0004886170743884174,
      "loss": 0.0004,
      "step": 2280
    },
    {
      "epoch": 1.1387918122815777,
      "grad_norm": 0.04071462154388428,
      "learning_rate": 0.0004886120818771842,
      "loss": 0.001,
      "step": 2281
    },
    {
      "epoch": 1.1392910634048927,
      "grad_norm": 0.22648854553699493,
      "learning_rate": 0.0004886070893659511,
      "loss": 0.0024,
      "step": 2282
    },
    {
      "epoch": 1.1397903145282078,
      "grad_norm": 0.01129978708922863,
      "learning_rate": 0.0004886020968547179,
      "loss": 0.0006,
      "step": 2283
    },
    {
      "epoch": 1.1402895656515226,
      "grad_norm": 0.011732938699424267,
      "learning_rate": 0.0004885971043434848,
      "loss": 0.0007,
      "step": 2284
    },
    {
      "epoch": 1.1407888167748377,
      "grad_norm": 0.06434867531061172,
      "learning_rate": 0.0004885921118322516,
      "loss": 0.001,
      "step": 2285
    },
    {
      "epoch": 1.1412880678981527,
      "grad_norm": 0.011763598769903183,
      "learning_rate": 0.0004885871193210184,
      "loss": 0.0005,
      "step": 2286
    },
    {
      "epoch": 1.1417873190214678,
      "grad_norm": 0.529004693031311,
      "learning_rate": 0.0004885821268097853,
      "loss": 0.0047,
      "step": 2287
    },
    {
      "epoch": 1.1422865701447829,
      "grad_norm": 0.007646641694009304,
      "learning_rate": 0.0004885771342985521,
      "loss": 0.0005,
      "step": 2288
    },
    {
      "epoch": 1.142785821268098,
      "grad_norm": 0.40676823258399963,
      "learning_rate": 0.000488572141787319,
      "loss": 0.0049,
      "step": 2289
    },
    {
      "epoch": 1.143285072391413,
      "grad_norm": 0.033416274935007095,
      "learning_rate": 0.0004885671492760858,
      "loss": 0.0008,
      "step": 2290
    },
    {
      "epoch": 1.143784323514728,
      "grad_norm": 0.0042159100994467735,
      "learning_rate": 0.0004885621567648527,
      "loss": 0.0004,
      "step": 2291
    },
    {
      "epoch": 1.1442835746380429,
      "grad_norm": 0.12912137806415558,
      "learning_rate": 0.0004885571642536195,
      "loss": 0.0015,
      "step": 2292
    },
    {
      "epoch": 1.144782825761358,
      "grad_norm": 1.0782369375228882,
      "learning_rate": 0.0004885521717423864,
      "loss": 0.0231,
      "step": 2293
    },
    {
      "epoch": 1.145282076884673,
      "grad_norm": 0.03586358577013016,
      "learning_rate": 0.0004885471792311532,
      "loss": 0.0009,
      "step": 2294
    },
    {
      "epoch": 1.145781328007988,
      "grad_norm": 0.026886139065027237,
      "learning_rate": 0.0004885421867199201,
      "loss": 0.0007,
      "step": 2295
    },
    {
      "epoch": 1.146280579131303,
      "grad_norm": 0.015978535637259483,
      "learning_rate": 0.0004885371942086869,
      "loss": 0.0006,
      "step": 2296
    },
    {
      "epoch": 1.1467798302546182,
      "grad_norm": 0.00715499185025692,
      "learning_rate": 0.0004885322016974538,
      "loss": 0.0005,
      "step": 2297
    },
    {
      "epoch": 1.147279081377933,
      "grad_norm": 0.4277864992618561,
      "learning_rate": 0.0004885272091862206,
      "loss": 0.003,
      "step": 2298
    },
    {
      "epoch": 1.147778332501248,
      "grad_norm": 0.39719927310943604,
      "learning_rate": 0.0004885222166749875,
      "loss": 0.0043,
      "step": 2299
    },
    {
      "epoch": 1.1482775836245631,
      "grad_norm": 0.16825732588768005,
      "learning_rate": 0.0004885172241637543,
      "loss": 0.002,
      "step": 2300
    },
    {
      "epoch": 1.1487768347478782,
      "grad_norm": 0.5241127610206604,
      "learning_rate": 0.0004885122316525212,
      "loss": 0.0147,
      "step": 2301
    },
    {
      "epoch": 1.1492760858711932,
      "grad_norm": 0.025482673197984695,
      "learning_rate": 0.000488507239141288,
      "loss": 0.0009,
      "step": 2302
    },
    {
      "epoch": 1.1497753369945083,
      "grad_norm": 0.4464840590953827,
      "learning_rate": 0.000488502246630055,
      "loss": 0.0052,
      "step": 2303
    },
    {
      "epoch": 1.1502745881178233,
      "grad_norm": 0.38510453701019287,
      "learning_rate": 0.0004884972541188218,
      "loss": 0.0045,
      "step": 2304
    },
    {
      "epoch": 1.1507738392411384,
      "grad_norm": 0.03367559611797333,
      "learning_rate": 0.0004884922616075887,
      "loss": 0.001,
      "step": 2305
    },
    {
      "epoch": 1.1512730903644532,
      "grad_norm": 0.6299560070037842,
      "learning_rate": 0.0004884872690963555,
      "loss": 0.0143,
      "step": 2306
    },
    {
      "epoch": 1.1517723414877683,
      "grad_norm": 0.7140355110168457,
      "learning_rate": 0.0004884822765851224,
      "loss": 0.0115,
      "step": 2307
    },
    {
      "epoch": 1.1522715926110834,
      "grad_norm": 0.012942789122462273,
      "learning_rate": 0.0004884772840738892,
      "loss": 0.0008,
      "step": 2308
    },
    {
      "epoch": 1.1527708437343984,
      "grad_norm": 0.8454263210296631,
      "learning_rate": 0.0004884722915626561,
      "loss": 0.0048,
      "step": 2309
    },
    {
      "epoch": 1.1532700948577135,
      "grad_norm": 0.17720450460910797,
      "learning_rate": 0.0004884672990514229,
      "loss": 0.0015,
      "step": 2310
    },
    {
      "epoch": 1.1537693459810285,
      "grad_norm": 0.031156262382864952,
      "learning_rate": 0.0004884623065401898,
      "loss": 0.0013,
      "step": 2311
    },
    {
      "epoch": 1.1542685971043434,
      "grad_norm": 0.5871880054473877,
      "learning_rate": 0.0004884573140289566,
      "loss": 0.0047,
      "step": 2312
    },
    {
      "epoch": 1.1547678482276584,
      "grad_norm": 0.17361067235469818,
      "learning_rate": 0.0004884523215177235,
      "loss": 0.0022,
      "step": 2313
    },
    {
      "epoch": 1.1552670993509735,
      "grad_norm": 0.57769376039505,
      "learning_rate": 0.0004884473290064903,
      "loss": 0.0095,
      "step": 2314
    },
    {
      "epoch": 1.1557663504742886,
      "grad_norm": 0.6035335659980774,
      "learning_rate": 0.0004884423364952571,
      "loss": 0.0134,
      "step": 2315
    },
    {
      "epoch": 1.1562656015976036,
      "grad_norm": 0.5764729976654053,
      "learning_rate": 0.000488437343984024,
      "loss": 0.0049,
      "step": 2316
    },
    {
      "epoch": 1.1567648527209187,
      "grad_norm": 0.47358688712120056,
      "learning_rate": 0.0004884323514727908,
      "loss": 0.0065,
      "step": 2317
    },
    {
      "epoch": 1.1572641038442337,
      "grad_norm": 0.025052059441804886,
      "learning_rate": 0.0004884273589615577,
      "loss": 0.001,
      "step": 2318
    },
    {
      "epoch": 1.1577633549675488,
      "grad_norm": 0.043243732303380966,
      "learning_rate": 0.0004884223664503245,
      "loss": 0.0013,
      "step": 2319
    },
    {
      "epoch": 1.1582626060908636,
      "grad_norm": 0.08679419755935669,
      "learning_rate": 0.0004884173739390914,
      "loss": 0.003,
      "step": 2320
    },
    {
      "epoch": 1.1587618572141787,
      "grad_norm": 0.20185929536819458,
      "learning_rate": 0.0004884123814278582,
      "loss": 0.0032,
      "step": 2321
    },
    {
      "epoch": 1.1592611083374937,
      "grad_norm": 0.06639906018972397,
      "learning_rate": 0.0004884073889166251,
      "loss": 0.0024,
      "step": 2322
    },
    {
      "epoch": 1.1597603594608088,
      "grad_norm": 0.800094485282898,
      "learning_rate": 0.0004884023964053919,
      "loss": 0.0094,
      "step": 2323
    },
    {
      "epoch": 1.1602596105841239,
      "grad_norm": 0.3537923991680145,
      "learning_rate": 0.0004883974038941588,
      "loss": 0.004,
      "step": 2324
    },
    {
      "epoch": 1.160758861707439,
      "grad_norm": 0.06625133007764816,
      "learning_rate": 0.0004883924113829256,
      "loss": 0.0017,
      "step": 2325
    },
    {
      "epoch": 1.1612581128307538,
      "grad_norm": 0.49498239159584045,
      "learning_rate": 0.0004883874188716925,
      "loss": 0.0091,
      "step": 2326
    },
    {
      "epoch": 1.1617573639540688,
      "grad_norm": 0.23624633252620697,
      "learning_rate": 0.0004883824263604593,
      "loss": 0.0111,
      "step": 2327
    },
    {
      "epoch": 1.1622566150773839,
      "grad_norm": 0.0682380348443985,
      "learning_rate": 0.0004883774338492262,
      "loss": 0.0019,
      "step": 2328
    },
    {
      "epoch": 1.162755866200699,
      "grad_norm": 0.0936393290758133,
      "learning_rate": 0.000488372441337993,
      "loss": 0.0016,
      "step": 2329
    },
    {
      "epoch": 1.163255117324014,
      "grad_norm": 0.08662658929824829,
      "learning_rate": 0.0004883674488267599,
      "loss": 0.0017,
      "step": 2330
    },
    {
      "epoch": 1.163754368447329,
      "grad_norm": 0.05724119395017624,
      "learning_rate": 0.0004883624563155267,
      "loss": 0.0012,
      "step": 2331
    },
    {
      "epoch": 1.164253619570644,
      "grad_norm": 0.1305316686630249,
      "learning_rate": 0.0004883574638042936,
      "loss": 0.002,
      "step": 2332
    },
    {
      "epoch": 1.1647528706939592,
      "grad_norm": 0.173130065202713,
      "learning_rate": 0.0004883524712930604,
      "loss": 0.002,
      "step": 2333
    },
    {
      "epoch": 1.165252121817274,
      "grad_norm": 0.4808441400527954,
      "learning_rate": 0.0004883474787818273,
      "loss": 0.0115,
      "step": 2334
    },
    {
      "epoch": 1.165751372940589,
      "grad_norm": 0.05866262689232826,
      "learning_rate": 0.0004883424862705941,
      "loss": 0.0013,
      "step": 2335
    },
    {
      "epoch": 1.1662506240639041,
      "grad_norm": 0.39890068769454956,
      "learning_rate": 0.000488337493759361,
      "loss": 0.0031,
      "step": 2336
    },
    {
      "epoch": 1.1667498751872192,
      "grad_norm": 1.6633543968200684,
      "learning_rate": 0.0004883325012481278,
      "loss": 0.0162,
      "step": 2337
    },
    {
      "epoch": 1.1672491263105342,
      "grad_norm": 0.6293022036552429,
      "learning_rate": 0.0004883275087368947,
      "loss": 0.004,
      "step": 2338
    },
    {
      "epoch": 1.1677483774338493,
      "grad_norm": 0.016545768827199936,
      "learning_rate": 0.0004883225162256615,
      "loss": 0.0009,
      "step": 2339
    },
    {
      "epoch": 1.1682476285571641,
      "grad_norm": 0.22365625202655792,
      "learning_rate": 0.0004883175237144284,
      "loss": 0.0035,
      "step": 2340
    },
    {
      "epoch": 1.1687468796804792,
      "grad_norm": 0.06615865975618362,
      "learning_rate": 0.0004883125312031952,
      "loss": 0.0017,
      "step": 2341
    },
    {
      "epoch": 1.1692461308037942,
      "grad_norm": 0.11169296503067017,
      "learning_rate": 0.0004883075386919621,
      "loss": 0.0021,
      "step": 2342
    },
    {
      "epoch": 1.1697453819271093,
      "grad_norm": 0.09818930923938751,
      "learning_rate": 0.0004883025461807289,
      "loss": 0.0017,
      "step": 2343
    },
    {
      "epoch": 1.1702446330504244,
      "grad_norm": 0.32705387473106384,
      "learning_rate": 0.0004882975536694957,
      "loss": 0.0419,
      "step": 2344
    },
    {
      "epoch": 1.1707438841737394,
      "grad_norm": 0.06935469061136246,
      "learning_rate": 0.0004882925611582626,
      "loss": 0.0015,
      "step": 2345
    },
    {
      "epoch": 1.1712431352970545,
      "grad_norm": 0.6282246112823486,
      "learning_rate": 0.0004882875686470294,
      "loss": 0.0036,
      "step": 2346
    },
    {
      "epoch": 1.1717423864203695,
      "grad_norm": 0.5409647822380066,
      "learning_rate": 0.0004882825761357963,
      "loss": 0.0169,
      "step": 2347
    },
    {
      "epoch": 1.1722416375436844,
      "grad_norm": 0.06299286335706711,
      "learning_rate": 0.00048827758362456313,
      "loss": 0.0009,
      "step": 2348
    },
    {
      "epoch": 1.1727408886669994,
      "grad_norm": 0.0543191134929657,
      "learning_rate": 0.00048827259111333,
      "loss": 0.0011,
      "step": 2349
    },
    {
      "epoch": 1.1732401397903145,
      "grad_norm": 0.8990772366523743,
      "learning_rate": 0.00048826759860209684,
      "loss": 0.0113,
      "step": 2350
    },
    {
      "epoch": 1.1737393909136296,
      "grad_norm": 0.6635754704475403,
      "learning_rate": 0.0004882626060908637,
      "loss": 0.0068,
      "step": 2351
    },
    {
      "epoch": 1.1742386420369446,
      "grad_norm": 0.023646648973226547,
      "learning_rate": 0.00048825761357963054,
      "loss": 0.0008,
      "step": 2352
    },
    {
      "epoch": 1.1747378931602597,
      "grad_norm": 0.17669835686683655,
      "learning_rate": 0.0004882526210683974,
      "loss": 0.0024,
      "step": 2353
    },
    {
      "epoch": 1.1752371442835747,
      "grad_norm": 0.39551597833633423,
      "learning_rate": 0.00048824762855716425,
      "loss": 0.0067,
      "step": 2354
    },
    {
      "epoch": 1.1757363954068896,
      "grad_norm": 0.4203104078769684,
      "learning_rate": 0.0004882426360459311,
      "loss": 0.0119,
      "step": 2355
    },
    {
      "epoch": 1.1762356465302046,
      "grad_norm": 0.5202176570892334,
      "learning_rate": 0.00048823764353469795,
      "loss": 0.0069,
      "step": 2356
    },
    {
      "epoch": 1.1767348976535197,
      "grad_norm": 0.08113536238670349,
      "learning_rate": 0.0004882326510234648,
      "loss": 0.0018,
      "step": 2357
    },
    {
      "epoch": 1.1772341487768347,
      "grad_norm": 0.6525851488113403,
      "learning_rate": 0.00048822765851223165,
      "loss": 0.0163,
      "step": 2358
    },
    {
      "epoch": 1.1777333999001498,
      "grad_norm": 0.1726967692375183,
      "learning_rate": 0.0004882226660009985,
      "loss": 0.0023,
      "step": 2359
    },
    {
      "epoch": 1.1782326510234649,
      "grad_norm": 0.4497997462749481,
      "learning_rate": 0.00048821767348976536,
      "loss": 0.0041,
      "step": 2360
    },
    {
      "epoch": 1.17873190214678,
      "grad_norm": 0.3233499526977539,
      "learning_rate": 0.0004882126809785322,
      "loss": 0.028,
      "step": 2361
    },
    {
      "epoch": 1.179231153270095,
      "grad_norm": 0.0878799557685852,
      "learning_rate": 0.00048820768846729906,
      "loss": 0.0021,
      "step": 2362
    },
    {
      "epoch": 1.1797304043934098,
      "grad_norm": 0.3244490325450897,
      "learning_rate": 0.0004882026959560659,
      "loss": 0.0205,
      "step": 2363
    },
    {
      "epoch": 1.1802296555167249,
      "grad_norm": 0.017356129363179207,
      "learning_rate": 0.00048819770344483277,
      "loss": 0.0009,
      "step": 2364
    },
    {
      "epoch": 1.18072890664004,
      "grad_norm": 0.13016346096992493,
      "learning_rate": 0.0004881927109335996,
      "loss": 0.002,
      "step": 2365
    },
    {
      "epoch": 1.181228157763355,
      "grad_norm": 0.2692471742630005,
      "learning_rate": 0.00048818771842236647,
      "loss": 0.0033,
      "step": 2366
    },
    {
      "epoch": 1.18172740888667,
      "grad_norm": 0.04437202960252762,
      "learning_rate": 0.0004881827259111333,
      "loss": 0.0015,
      "step": 2367
    },
    {
      "epoch": 1.182226660009985,
      "grad_norm": 0.373127281665802,
      "learning_rate": 0.0004881777333999002,
      "loss": 0.0123,
      "step": 2368
    },
    {
      "epoch": 1.1827259111333,
      "grad_norm": 0.10776910930871964,
      "learning_rate": 0.00048817274088866703,
      "loss": 0.0019,
      "step": 2369
    },
    {
      "epoch": 1.183225162256615,
      "grad_norm": 0.02961268089711666,
      "learning_rate": 0.0004881677483774339,
      "loss": 0.0013,
      "step": 2370
    },
    {
      "epoch": 1.18372441337993,
      "grad_norm": 0.28752031922340393,
      "learning_rate": 0.00048816275586620073,
      "loss": 0.005,
      "step": 2371
    },
    {
      "epoch": 1.1842236645032451,
      "grad_norm": 0.47194361686706543,
      "learning_rate": 0.00048815776335496753,
      "loss": 0.0069,
      "step": 2372
    },
    {
      "epoch": 1.1847229156265602,
      "grad_norm": 0.29707831144332886,
      "learning_rate": 0.0004881527708437344,
      "loss": 0.0047,
      "step": 2373
    },
    {
      "epoch": 1.1852221667498752,
      "grad_norm": 0.008784891106188297,
      "learning_rate": 0.00048814777833250123,
      "loss": 0.0007,
      "step": 2374
    },
    {
      "epoch": 1.1857214178731903,
      "grad_norm": 0.02561444602906704,
      "learning_rate": 0.0004881427858212681,
      "loss": 0.0009,
      "step": 2375
    },
    {
      "epoch": 1.1862206689965054,
      "grad_norm": 0.04315022751688957,
      "learning_rate": 0.00048813779331003494,
      "loss": 0.001,
      "step": 2376
    },
    {
      "epoch": 1.1867199201198202,
      "grad_norm": 0.017634838819503784,
      "learning_rate": 0.0004881328007988018,
      "loss": 0.0008,
      "step": 2377
    },
    {
      "epoch": 1.1872191712431353,
      "grad_norm": 0.023129841312766075,
      "learning_rate": 0.00048812780828756864,
      "loss": 0.0008,
      "step": 2378
    },
    {
      "epoch": 1.1877184223664503,
      "grad_norm": 0.951586902141571,
      "learning_rate": 0.0004881228157763355,
      "loss": 0.0124,
      "step": 2379
    },
    {
      "epoch": 1.1882176734897654,
      "grad_norm": 0.28350234031677246,
      "learning_rate": 0.00048811782326510235,
      "loss": 0.005,
      "step": 2380
    },
    {
      "epoch": 1.1887169246130804,
      "grad_norm": 0.5675345063209534,
      "learning_rate": 0.0004881128307538692,
      "loss": 0.0106,
      "step": 2381
    },
    {
      "epoch": 1.1892161757363955,
      "grad_norm": 0.024017909541726112,
      "learning_rate": 0.00048810783824263605,
      "loss": 0.0008,
      "step": 2382
    },
    {
      "epoch": 1.1897154268597103,
      "grad_norm": 0.2691856920719147,
      "learning_rate": 0.0004881028457314029,
      "loss": 0.0031,
      "step": 2383
    },
    {
      "epoch": 1.1902146779830254,
      "grad_norm": 0.014395068399608135,
      "learning_rate": 0.00048809785322016975,
      "loss": 0.0007,
      "step": 2384
    },
    {
      "epoch": 1.1907139291063404,
      "grad_norm": 0.31097590923309326,
      "learning_rate": 0.0004880928607089366,
      "loss": 0.0051,
      "step": 2385
    },
    {
      "epoch": 1.1912131802296555,
      "grad_norm": 0.32612231373786926,
      "learning_rate": 0.00048808786819770346,
      "loss": 0.0133,
      "step": 2386
    },
    {
      "epoch": 1.1917124313529706,
      "grad_norm": 0.14207638800144196,
      "learning_rate": 0.0004880828756864703,
      "loss": 0.0023,
      "step": 2387
    },
    {
      "epoch": 1.1922116824762856,
      "grad_norm": 0.19338001310825348,
      "learning_rate": 0.00048807788317523716,
      "loss": 0.0025,
      "step": 2388
    },
    {
      "epoch": 1.1927109335996007,
      "grad_norm": 0.11061175912618637,
      "learning_rate": 0.000488072890664004,
      "loss": 0.0025,
      "step": 2389
    },
    {
      "epoch": 1.1932101847229157,
      "grad_norm": 0.4104427993297577,
      "learning_rate": 0.00048806789815277087,
      "loss": 0.0112,
      "step": 2390
    },
    {
      "epoch": 1.1937094358462306,
      "grad_norm": 0.11395139247179031,
      "learning_rate": 0.0004880629056415377,
      "loss": 0.0021,
      "step": 2391
    },
    {
      "epoch": 1.1942086869695456,
      "grad_norm": 0.021077129989862442,
      "learning_rate": 0.00048805791313030457,
      "loss": 0.001,
      "step": 2392
    },
    {
      "epoch": 1.1947079380928607,
      "grad_norm": 0.043181322515010834,
      "learning_rate": 0.0004880529206190714,
      "loss": 0.0011,
      "step": 2393
    },
    {
      "epoch": 1.1952071892161757,
      "grad_norm": 0.057566940784454346,
      "learning_rate": 0.0004880479281078383,
      "loss": 0.0014,
      "step": 2394
    },
    {
      "epoch": 1.1957064403394908,
      "grad_norm": 0.04657038301229477,
      "learning_rate": 0.0004880429355966051,
      "loss": 0.0011,
      "step": 2395
    },
    {
      "epoch": 1.1962056914628059,
      "grad_norm": 0.08841092139482498,
      "learning_rate": 0.000488037943085372,
      "loss": 0.0014,
      "step": 2396
    },
    {
      "epoch": 1.1967049425861207,
      "grad_norm": 0.4720470607280731,
      "learning_rate": 0.00048803295057413883,
      "loss": 0.0033,
      "step": 2397
    },
    {
      "epoch": 1.1972041937094358,
      "grad_norm": 0.01845584250986576,
      "learning_rate": 0.0004880279580629057,
      "loss": 0.001,
      "step": 2398
    },
    {
      "epoch": 1.1977034448327508,
      "grad_norm": 0.28534775972366333,
      "learning_rate": 0.00048802296555167254,
      "loss": 0.0157,
      "step": 2399
    },
    {
      "epoch": 1.1982026959560659,
      "grad_norm": 0.1010608822107315,
      "learning_rate": 0.0004880179730404394,
      "loss": 0.0021,
      "step": 2400
    },
    {
      "epoch": 1.198701947079381,
      "grad_norm": 0.2145642191171646,
      "learning_rate": 0.0004880129805292062,
      "loss": 0.0055,
      "step": 2401
    },
    {
      "epoch": 1.199201198202696,
      "grad_norm": 0.066231869161129,
      "learning_rate": 0.00048800798801797304,
      "loss": 0.0013,
      "step": 2402
    },
    {
      "epoch": 1.199700449326011,
      "grad_norm": 0.06005711480975151,
      "learning_rate": 0.0004880029955067399,
      "loss": 0.0014,
      "step": 2403
    },
    {
      "epoch": 1.2001997004493261,
      "grad_norm": 0.009955336339771748,
      "learning_rate": 0.00048799800299550674,
      "loss": 0.0008,
      "step": 2404
    },
    {
      "epoch": 1.200698951572641,
      "grad_norm": 0.6013410091400146,
      "learning_rate": 0.0004879930104842736,
      "loss": 0.0054,
      "step": 2405
    },
    {
      "epoch": 1.201198202695956,
      "grad_norm": 0.1600722074508667,
      "learning_rate": 0.00048798801797304045,
      "loss": 0.0037,
      "step": 2406
    },
    {
      "epoch": 1.201697453819271,
      "grad_norm": 0.6442285776138306,
      "learning_rate": 0.0004879830254618073,
      "loss": 0.0158,
      "step": 2407
    },
    {
      "epoch": 1.2021967049425861,
      "grad_norm": 0.17917510867118835,
      "learning_rate": 0.00048797803295057415,
      "loss": 0.0021,
      "step": 2408
    },
    {
      "epoch": 1.2026959560659012,
      "grad_norm": 0.4622408151626587,
      "learning_rate": 0.000487973040439341,
      "loss": 0.0071,
      "step": 2409
    },
    {
      "epoch": 1.2031952071892162,
      "grad_norm": 0.2816579341888428,
      "learning_rate": 0.00048796804792810785,
      "loss": 0.0119,
      "step": 2410
    },
    {
      "epoch": 1.203694458312531,
      "grad_norm": 0.20602838695049286,
      "learning_rate": 0.0004879630554168747,
      "loss": 0.0015,
      "step": 2411
    },
    {
      "epoch": 1.2041937094358461,
      "grad_norm": 0.22114348411560059,
      "learning_rate": 0.00048795806290564156,
      "loss": 0.0029,
      "step": 2412
    },
    {
      "epoch": 1.2046929605591612,
      "grad_norm": 0.23838919401168823,
      "learning_rate": 0.0004879530703944084,
      "loss": 0.004,
      "step": 2413
    },
    {
      "epoch": 1.2051922116824763,
      "grad_norm": 0.18658797442913055,
      "learning_rate": 0.00048794807788317526,
      "loss": 0.0033,
      "step": 2414
    },
    {
      "epoch": 1.2056914628057913,
      "grad_norm": 0.1616593599319458,
      "learning_rate": 0.0004879430853719421,
      "loss": 0.0017,
      "step": 2415
    },
    {
      "epoch": 1.2061907139291064,
      "grad_norm": 0.5474050641059875,
      "learning_rate": 0.00048793809286070897,
      "loss": 0.0205,
      "step": 2416
    },
    {
      "epoch": 1.2066899650524214,
      "grad_norm": 0.034338973462581635,
      "learning_rate": 0.0004879331003494758,
      "loss": 0.0014,
      "step": 2417
    },
    {
      "epoch": 1.2071892161757365,
      "grad_norm": 0.14388787746429443,
      "learning_rate": 0.00048792810783824267,
      "loss": 0.0031,
      "step": 2418
    },
    {
      "epoch": 1.2076884672990513,
      "grad_norm": 0.2020435482263565,
      "learning_rate": 0.0004879231153270095,
      "loss": 0.0024,
      "step": 2419
    },
    {
      "epoch": 1.2081877184223664,
      "grad_norm": 0.13248372077941895,
      "learning_rate": 0.0004879181228157764,
      "loss": 0.0032,
      "step": 2420
    },
    {
      "epoch": 1.2086869695456814,
      "grad_norm": 0.014413614757359028,
      "learning_rate": 0.0004879131303045432,
      "loss": 0.0008,
      "step": 2421
    },
    {
      "epoch": 1.2091862206689965,
      "grad_norm": 0.20811915397644043,
      "learning_rate": 0.0004879081377933101,
      "loss": 0.0029,
      "step": 2422
    },
    {
      "epoch": 1.2096854717923116,
      "grad_norm": 0.4143373370170593,
      "learning_rate": 0.00048790314528207693,
      "loss": 0.0132,
      "step": 2423
    },
    {
      "epoch": 1.2101847229156266,
      "grad_norm": 0.05013761669397354,
      "learning_rate": 0.0004878981527708438,
      "loss": 0.0016,
      "step": 2424
    },
    {
      "epoch": 1.2106839740389417,
      "grad_norm": 0.11684656143188477,
      "learning_rate": 0.00048789316025961063,
      "loss": 0.0024,
      "step": 2425
    },
    {
      "epoch": 1.2111832251622565,
      "grad_norm": 0.10921639949083328,
      "learning_rate": 0.0004878881677483775,
      "loss": 0.0019,
      "step": 2426
    },
    {
      "epoch": 1.2116824762855716,
      "grad_norm": 0.040937066078186035,
      "learning_rate": 0.00048788317523714434,
      "loss": 0.0017,
      "step": 2427
    },
    {
      "epoch": 1.2121817274088866,
      "grad_norm": 0.5344093441963196,
      "learning_rate": 0.0004878781827259112,
      "loss": 0.009,
      "step": 2428
    },
    {
      "epoch": 1.2126809785322017,
      "grad_norm": 0.04858030006289482,
      "learning_rate": 0.00048787319021467804,
      "loss": 0.0016,
      "step": 2429
    },
    {
      "epoch": 1.2131802296555168,
      "grad_norm": 0.22338241338729858,
      "learning_rate": 0.00048786819770344484,
      "loss": 0.003,
      "step": 2430
    },
    {
      "epoch": 1.2136794807788318,
      "grad_norm": 0.34715041518211365,
      "learning_rate": 0.0004878632051922117,
      "loss": 0.006,
      "step": 2431
    },
    {
      "epoch": 1.2141787319021469,
      "grad_norm": 0.5270247459411621,
      "learning_rate": 0.00048785821268097855,
      "loss": 0.0053,
      "step": 2432
    },
    {
      "epoch": 1.214677983025462,
      "grad_norm": 0.18308891355991364,
      "learning_rate": 0.0004878532201697454,
      "loss": 0.0027,
      "step": 2433
    },
    {
      "epoch": 1.2151772341487768,
      "grad_norm": 0.4332311451435089,
      "learning_rate": 0.00048784822765851225,
      "loss": 0.0032,
      "step": 2434
    },
    {
      "epoch": 1.2156764852720918,
      "grad_norm": 0.15082749724388123,
      "learning_rate": 0.00048784323514727905,
      "loss": 0.0032,
      "step": 2435
    },
    {
      "epoch": 1.2161757363954069,
      "grad_norm": 0.016438616439700127,
      "learning_rate": 0.0004878382426360459,
      "loss": 0.0008,
      "step": 2436
    },
    {
      "epoch": 1.216674987518722,
      "grad_norm": 1.770842432975769,
      "learning_rate": 0.00048783325012481275,
      "loss": 0.003,
      "step": 2437
    },
    {
      "epoch": 1.217174238642037,
      "grad_norm": 0.1293468326330185,
      "learning_rate": 0.0004878282576135796,
      "loss": 0.0018,
      "step": 2438
    },
    {
      "epoch": 1.217673489765352,
      "grad_norm": 0.28377822041511536,
      "learning_rate": 0.00048782326510234646,
      "loss": 0.0034,
      "step": 2439
    },
    {
      "epoch": 1.218172740888667,
      "grad_norm": 0.24544887244701385,
      "learning_rate": 0.0004878182725911133,
      "loss": 0.0032,
      "step": 2440
    },
    {
      "epoch": 1.218671992011982,
      "grad_norm": 1.175071358680725,
      "learning_rate": 0.00048781328007988016,
      "loss": 0.0267,
      "step": 2441
    },
    {
      "epoch": 1.219171243135297,
      "grad_norm": 0.40397971868515015,
      "learning_rate": 0.000487808287568647,
      "loss": 0.0096,
      "step": 2442
    },
    {
      "epoch": 1.219670494258612,
      "grad_norm": 0.1245126873254776,
      "learning_rate": 0.00048780329505741386,
      "loss": 0.0021,
      "step": 2443
    },
    {
      "epoch": 1.2201697453819271,
      "grad_norm": 0.16408023238182068,
      "learning_rate": 0.0004877983025461807,
      "loss": 0.002,
      "step": 2444
    },
    {
      "epoch": 1.2206689965052422,
      "grad_norm": 1.0802313089370728,
      "learning_rate": 0.00048779331003494757,
      "loss": 0.008,
      "step": 2445
    },
    {
      "epoch": 1.2211682476285572,
      "grad_norm": 0.7702080011367798,
      "learning_rate": 0.0004877883175237144,
      "loss": 0.0119,
      "step": 2446
    },
    {
      "epoch": 1.2216674987518723,
      "grad_norm": 0.09236845374107361,
      "learning_rate": 0.00048778332501248127,
      "loss": 0.0017,
      "step": 2447
    },
    {
      "epoch": 1.2221667498751871,
      "grad_norm": 0.026936562731862068,
      "learning_rate": 0.0004877783325012481,
      "loss": 0.0012,
      "step": 2448
    },
    {
      "epoch": 1.2226660009985022,
      "grad_norm": 0.22756002843379974,
      "learning_rate": 0.000487773339990015,
      "loss": 0.004,
      "step": 2449
    },
    {
      "epoch": 1.2231652521218173,
      "grad_norm": 0.050057005137205124,
      "learning_rate": 0.00048776834747878183,
      "loss": 0.0015,
      "step": 2450
    },
    {
      "epoch": 1.2236645032451323,
      "grad_norm": 0.06294393539428711,
      "learning_rate": 0.0004877633549675487,
      "loss": 0.0018,
      "step": 2451
    },
    {
      "epoch": 1.2241637543684474,
      "grad_norm": 0.07542585581541061,
      "learning_rate": 0.00048775836245631553,
      "loss": 0.0023,
      "step": 2452
    },
    {
      "epoch": 1.2246630054917624,
      "grad_norm": 0.13446326553821564,
      "learning_rate": 0.0004877533699450824,
      "loss": 0.0031,
      "step": 2453
    },
    {
      "epoch": 1.2251622566150773,
      "grad_norm": 0.4713457226753235,
      "learning_rate": 0.00048774837743384924,
      "loss": 0.0324,
      "step": 2454
    },
    {
      "epoch": 1.2256615077383923,
      "grad_norm": 0.39443400502204895,
      "learning_rate": 0.0004877433849226161,
      "loss": 0.0083,
      "step": 2455
    },
    {
      "epoch": 1.2261607588617074,
      "grad_norm": 0.49686315655708313,
      "learning_rate": 0.00048773839241138294,
      "loss": 0.0021,
      "step": 2456
    },
    {
      "epoch": 1.2266600099850224,
      "grad_norm": 0.22608527541160583,
      "learning_rate": 0.0004877333999001498,
      "loss": 0.0017,
      "step": 2457
    },
    {
      "epoch": 1.2271592611083375,
      "grad_norm": 0.4557802975177765,
      "learning_rate": 0.00048772840738891664,
      "loss": 0.0076,
      "step": 2458
    },
    {
      "epoch": 1.2276585122316526,
      "grad_norm": 0.4040985703468323,
      "learning_rate": 0.00048772341487768344,
      "loss": 0.0131,
      "step": 2459
    },
    {
      "epoch": 1.2281577633549676,
      "grad_norm": 0.2654607594013214,
      "learning_rate": 0.0004877184223664503,
      "loss": 0.0035,
      "step": 2460
    },
    {
      "epoch": 1.2286570144782827,
      "grad_norm": 0.4022940695285797,
      "learning_rate": 0.00048771342985521715,
      "loss": 0.0193,
      "step": 2461
    },
    {
      "epoch": 1.2291562656015975,
      "grad_norm": 0.46259912848472595,
      "learning_rate": 0.000487708437343984,
      "loss": 0.0205,
      "step": 2462
    },
    {
      "epoch": 1.2296555167249126,
      "grad_norm": 0.4559084475040436,
      "learning_rate": 0.00048770344483275085,
      "loss": 0.0141,
      "step": 2463
    },
    {
      "epoch": 1.2301547678482276,
      "grad_norm": 0.5082015991210938,
      "learning_rate": 0.0004876984523215177,
      "loss": 0.0313,
      "step": 2464
    },
    {
      "epoch": 1.2306540189715427,
      "grad_norm": 0.16138982772827148,
      "learning_rate": 0.00048769345981028455,
      "loss": 0.0038,
      "step": 2465
    },
    {
      "epoch": 1.2311532700948578,
      "grad_norm": 0.06712261587381363,
      "learning_rate": 0.0004876884672990514,
      "loss": 0.0022,
      "step": 2466
    },
    {
      "epoch": 1.2316525212181728,
      "grad_norm": 0.7768600583076477,
      "learning_rate": 0.00048768347478781826,
      "loss": 0.0109,
      "step": 2467
    },
    {
      "epoch": 1.2321517723414876,
      "grad_norm": 0.1747196912765503,
      "learning_rate": 0.0004876784822765851,
      "loss": 0.0029,
      "step": 2468
    },
    {
      "epoch": 1.2326510234648027,
      "grad_norm": 0.14029645919799805,
      "learning_rate": 0.00048767348976535196,
      "loss": 0.0024,
      "step": 2469
    },
    {
      "epoch": 1.2331502745881178,
      "grad_norm": 0.32469427585601807,
      "learning_rate": 0.0004876684972541188,
      "loss": 0.0073,
      "step": 2470
    },
    {
      "epoch": 1.2336495257114328,
      "grad_norm": 0.1379002183675766,
      "learning_rate": 0.00048766350474288567,
      "loss": 0.0037,
      "step": 2471
    },
    {
      "epoch": 1.2341487768347479,
      "grad_norm": 0.29204726219177246,
      "learning_rate": 0.0004876585122316525,
      "loss": 0.0058,
      "step": 2472
    },
    {
      "epoch": 1.234648027958063,
      "grad_norm": 0.44220948219299316,
      "learning_rate": 0.00048765351972041937,
      "loss": 0.0158,
      "step": 2473
    },
    {
      "epoch": 1.235147279081378,
      "grad_norm": 0.04161466285586357,
      "learning_rate": 0.0004876485272091862,
      "loss": 0.0015,
      "step": 2474
    },
    {
      "epoch": 1.235646530204693,
      "grad_norm": 0.030252184718847275,
      "learning_rate": 0.0004876435346979531,
      "loss": 0.0011,
      "step": 2475
    },
    {
      "epoch": 1.236145781328008,
      "grad_norm": 0.21526053547859192,
      "learning_rate": 0.00048763854218671993,
      "loss": 0.0054,
      "step": 2476
    },
    {
      "epoch": 1.236645032451323,
      "grad_norm": 0.2923896312713623,
      "learning_rate": 0.0004876335496754868,
      "loss": 0.0047,
      "step": 2477
    },
    {
      "epoch": 1.237144283574638,
      "grad_norm": 0.038500864058732986,
      "learning_rate": 0.00048762855716425363,
      "loss": 0.0018,
      "step": 2478
    },
    {
      "epoch": 1.237643534697953,
      "grad_norm": 0.6981975436210632,
      "learning_rate": 0.0004876235646530205,
      "loss": 0.0159,
      "step": 2479
    },
    {
      "epoch": 1.2381427858212681,
      "grad_norm": 0.018485626205801964,
      "learning_rate": 0.00048761857214178734,
      "loss": 0.0009,
      "step": 2480
    },
    {
      "epoch": 1.2386420369445832,
      "grad_norm": 0.12967610359191895,
      "learning_rate": 0.0004876135796305542,
      "loss": 0.0028,
      "step": 2481
    },
    {
      "epoch": 1.239141288067898,
      "grad_norm": 0.2222386598587036,
      "learning_rate": 0.00048760858711932104,
      "loss": 0.0043,
      "step": 2482
    },
    {
      "epoch": 1.239640539191213,
      "grad_norm": 0.09860629588365555,
      "learning_rate": 0.0004876035946080879,
      "loss": 0.0022,
      "step": 2483
    },
    {
      "epoch": 1.2401397903145281,
      "grad_norm": 0.08979736268520355,
      "learning_rate": 0.00048759860209685474,
      "loss": 0.0016,
      "step": 2484
    },
    {
      "epoch": 1.2406390414378432,
      "grad_norm": 0.10203518718481064,
      "learning_rate": 0.0004875936095856216,
      "loss": 0.0021,
      "step": 2485
    },
    {
      "epoch": 1.2411382925611583,
      "grad_norm": 0.12686346471309662,
      "learning_rate": 0.00048758861707438845,
      "loss": 0.0023,
      "step": 2486
    },
    {
      "epoch": 1.2416375436844733,
      "grad_norm": 0.03280661627650261,
      "learning_rate": 0.0004875836245631553,
      "loss": 0.0012,
      "step": 2487
    },
    {
      "epoch": 1.2421367948077884,
      "grad_norm": 0.0687054768204689,
      "learning_rate": 0.0004875786320519221,
      "loss": 0.0016,
      "step": 2488
    },
    {
      "epoch": 1.2426360459311034,
      "grad_norm": 0.044470276683568954,
      "learning_rate": 0.00048757363954068895,
      "loss": 0.0013,
      "step": 2489
    },
    {
      "epoch": 1.2431352970544183,
      "grad_norm": 0.032529521733522415,
      "learning_rate": 0.0004875686470294558,
      "loss": 0.0011,
      "step": 2490
    },
    {
      "epoch": 1.2436345481777333,
      "grad_norm": 0.046957481652498245,
      "learning_rate": 0.00048756365451822265,
      "loss": 0.0012,
      "step": 2491
    },
    {
      "epoch": 1.2441337993010484,
      "grad_norm": 0.18638920783996582,
      "learning_rate": 0.0004875586620069895,
      "loss": 0.0025,
      "step": 2492
    },
    {
      "epoch": 1.2446330504243635,
      "grad_norm": 0.3333408236503601,
      "learning_rate": 0.00048755366949575636,
      "loss": 0.0043,
      "step": 2493
    },
    {
      "epoch": 1.2451323015476785,
      "grad_norm": 0.05541180446743965,
      "learning_rate": 0.0004875486769845232,
      "loss": 0.0012,
      "step": 2494
    },
    {
      "epoch": 1.2456315526709936,
      "grad_norm": 0.31954121589660645,
      "learning_rate": 0.00048754368447329006,
      "loss": 0.0026,
      "step": 2495
    },
    {
      "epoch": 1.2461308037943086,
      "grad_norm": 0.16018632054328918,
      "learning_rate": 0.0004875386919620569,
      "loss": 0.0032,
      "step": 2496
    },
    {
      "epoch": 1.2466300549176235,
      "grad_norm": 0.02182638831436634,
      "learning_rate": 0.00048753369945082377,
      "loss": 0.001,
      "step": 2497
    },
    {
      "epoch": 1.2471293060409385,
      "grad_norm": 0.033358383923769,
      "learning_rate": 0.0004875287069395906,
      "loss": 0.0009,
      "step": 2498
    },
    {
      "epoch": 1.2476285571642536,
      "grad_norm": 0.18837031722068787,
      "learning_rate": 0.00048752371442835747,
      "loss": 0.0022,
      "step": 2499
    },
    {
      "epoch": 1.2481278082875686,
      "grad_norm": 0.2132118195295334,
      "learning_rate": 0.0004875187219171243,
      "loss": 0.0022,
      "step": 2500
    },
    {
      "epoch": 1.2486270594108837,
      "grad_norm": 0.02176341786980629,
      "learning_rate": 0.0004875137294058912,
      "loss": 0.0007,
      "step": 2501
    },
    {
      "epoch": 1.2491263105341988,
      "grad_norm": 0.14668060839176178,
      "learning_rate": 0.00048750873689465803,
      "loss": 0.0024,
      "step": 2502
    },
    {
      "epoch": 1.2496255616575138,
      "grad_norm": 0.21103371679782867,
      "learning_rate": 0.0004875037443834249,
      "loss": 0.0031,
      "step": 2503
    },
    {
      "epoch": 1.2501248127808289,
      "grad_norm": 0.005613397341221571,
      "learning_rate": 0.00048749875187219173,
      "loss": 0.0005,
      "step": 2504
    },
    {
      "epoch": 1.2506240639041437,
      "grad_norm": 0.011484679765999317,
      "learning_rate": 0.0004874937593609586,
      "loss": 0.0006,
      "step": 2505
    },
    {
      "epoch": 1.2511233150274588,
      "grad_norm": 1.1525136232376099,
      "learning_rate": 0.00048748876684972544,
      "loss": 0.0086,
      "step": 2506
    },
    {
      "epoch": 1.2516225661507738,
      "grad_norm": 0.015152369625866413,
      "learning_rate": 0.0004874837743384923,
      "loss": 0.0008,
      "step": 2507
    },
    {
      "epoch": 1.2521218172740889,
      "grad_norm": 0.39573490619659424,
      "learning_rate": 0.00048747878182725914,
      "loss": 0.004,
      "step": 2508
    },
    {
      "epoch": 1.252621068397404,
      "grad_norm": 0.02115468494594097,
      "learning_rate": 0.000487473789316026,
      "loss": 0.0008,
      "step": 2509
    },
    {
      "epoch": 1.2531203195207188,
      "grad_norm": 0.4994850754737854,
      "learning_rate": 0.00048746879680479284,
      "loss": 0.0155,
      "step": 2510
    },
    {
      "epoch": 1.2536195706440338,
      "grad_norm": 0.5452350378036499,
      "learning_rate": 0.0004874638042935597,
      "loss": 0.0099,
      "step": 2511
    },
    {
      "epoch": 1.254118821767349,
      "grad_norm": 0.5991985201835632,
      "learning_rate": 0.00048745881178232655,
      "loss": 0.0107,
      "step": 2512
    },
    {
      "epoch": 1.254618072890664,
      "grad_norm": 0.3382919728755951,
      "learning_rate": 0.0004874538192710934,
      "loss": 0.0027,
      "step": 2513
    },
    {
      "epoch": 1.255117324013979,
      "grad_norm": 0.045585568994283676,
      "learning_rate": 0.00048744882675986025,
      "loss": 0.0013,
      "step": 2514
    },
    {
      "epoch": 1.255616575137294,
      "grad_norm": 0.057072434574365616,
      "learning_rate": 0.0004874438342486271,
      "loss": 0.0014,
      "step": 2515
    },
    {
      "epoch": 1.2561158262606091,
      "grad_norm": 0.11598760634660721,
      "learning_rate": 0.00048743884173739396,
      "loss": 0.0033,
      "step": 2516
    },
    {
      "epoch": 1.2566150773839242,
      "grad_norm": 0.2734789550304413,
      "learning_rate": 0.00048743384922616075,
      "loss": 0.0055,
      "step": 2517
    },
    {
      "epoch": 1.2571143285072393,
      "grad_norm": 0.1317008137702942,
      "learning_rate": 0.0004874288567149276,
      "loss": 0.0023,
      "step": 2518
    },
    {
      "epoch": 1.257613579630554,
      "grad_norm": 0.3948538899421692,
      "learning_rate": 0.00048742386420369446,
      "loss": 0.0032,
      "step": 2519
    },
    {
      "epoch": 1.2581128307538691,
      "grad_norm": 0.04089463874697685,
      "learning_rate": 0.0004874188716924613,
      "loss": 0.0014,
      "step": 2520
    },
    {
      "epoch": 1.2586120818771842,
      "grad_norm": 0.4294220507144928,
      "learning_rate": 0.00048741387918122816,
      "loss": 0.0151,
      "step": 2521
    },
    {
      "epoch": 1.2591113330004993,
      "grad_norm": 0.9297868609428406,
      "learning_rate": 0.000487408886669995,
      "loss": 0.0526,
      "step": 2522
    },
    {
      "epoch": 1.2596105841238143,
      "grad_norm": 0.35488173365592957,
      "learning_rate": 0.00048740389415876187,
      "loss": 0.0106,
      "step": 2523
    },
    {
      "epoch": 1.2601098352471294,
      "grad_norm": 0.17704534530639648,
      "learning_rate": 0.0004873989016475287,
      "loss": 0.0019,
      "step": 2524
    },
    {
      "epoch": 1.2606090863704442,
      "grad_norm": 0.2463091015815735,
      "learning_rate": 0.00048739390913629557,
      "loss": 0.0043,
      "step": 2525
    },
    {
      "epoch": 1.2611083374937593,
      "grad_norm": 0.2516127824783325,
      "learning_rate": 0.0004873889166250624,
      "loss": 0.005,
      "step": 2526
    },
    {
      "epoch": 1.2616075886170743,
      "grad_norm": 0.032411713153123856,
      "learning_rate": 0.0004873839241138293,
      "loss": 0.0013,
      "step": 2527
    },
    {
      "epoch": 1.2621068397403894,
      "grad_norm": 0.29693490266799927,
      "learning_rate": 0.0004873789316025961,
      "loss": 0.0136,
      "step": 2528
    },
    {
      "epoch": 1.2626060908637045,
      "grad_norm": 0.5499058961868286,
      "learning_rate": 0.000487373939091363,
      "loss": 0.0119,
      "step": 2529
    },
    {
      "epoch": 1.2631053419870195,
      "grad_norm": 0.1370798945426941,
      "learning_rate": 0.00048736894658012983,
      "loss": 0.0031,
      "step": 2530
    },
    {
      "epoch": 1.2636045931103346,
      "grad_norm": 0.1599661409854889,
      "learning_rate": 0.0004873639540688967,
      "loss": 0.0245,
      "step": 2531
    },
    {
      "epoch": 1.2641038442336496,
      "grad_norm": 0.25442183017730713,
      "learning_rate": 0.00048735896155766353,
      "loss": 0.0043,
      "step": 2532
    },
    {
      "epoch": 1.2646030953569647,
      "grad_norm": 1.0750494003295898,
      "learning_rate": 0.0004873539690464304,
      "loss": 0.0303,
      "step": 2533
    },
    {
      "epoch": 1.2651023464802795,
      "grad_norm": 0.41846635937690735,
      "learning_rate": 0.00048734897653519724,
      "loss": 0.0109,
      "step": 2534
    },
    {
      "epoch": 1.2656015976035946,
      "grad_norm": 0.04541102424263954,
      "learning_rate": 0.0004873439840239641,
      "loss": 0.0018,
      "step": 2535
    },
    {
      "epoch": 1.2661008487269096,
      "grad_norm": 0.627119779586792,
      "learning_rate": 0.00048733899151273094,
      "loss": 0.0201,
      "step": 2536
    },
    {
      "epoch": 1.2666000998502247,
      "grad_norm": 0.34566935896873474,
      "learning_rate": 0.0004873339990014978,
      "loss": 0.0088,
      "step": 2537
    },
    {
      "epoch": 1.2670993509735398,
      "grad_norm": 0.33657073974609375,
      "learning_rate": 0.00048732900649026465,
      "loss": 0.0066,
      "step": 2538
    },
    {
      "epoch": 1.2675986020968546,
      "grad_norm": 0.1397445797920227,
      "learning_rate": 0.0004873240139790315,
      "loss": 0.0024,
      "step": 2539
    },
    {
      "epoch": 1.2680978532201697,
      "grad_norm": 0.44971758127212524,
      "learning_rate": 0.00048731902146779835,
      "loss": 0.0056,
      "step": 2540
    },
    {
      "epoch": 1.2685971043434847,
      "grad_norm": 0.2702922821044922,
      "learning_rate": 0.0004873140289565652,
      "loss": 0.009,
      "step": 2541
    },
    {
      "epoch": 1.2690963554667998,
      "grad_norm": 0.05904832109808922,
      "learning_rate": 0.00048730903644533206,
      "loss": 0.0026,
      "step": 2542
    },
    {
      "epoch": 1.2695956065901148,
      "grad_norm": 0.039743147790431976,
      "learning_rate": 0.0004873040439340989,
      "loss": 0.0017,
      "step": 2543
    },
    {
      "epoch": 1.27009485771343,
      "grad_norm": 0.1548171192407608,
      "learning_rate": 0.00048729905142286576,
      "loss": 0.0028,
      "step": 2544
    },
    {
      "epoch": 1.270594108836745,
      "grad_norm": 0.030238822102546692,
      "learning_rate": 0.0004872940589116326,
      "loss": 0.0019,
      "step": 2545
    },
    {
      "epoch": 1.27109335996006,
      "grad_norm": 0.2775281071662903,
      "learning_rate": 0.0004872890664003994,
      "loss": 0.0066,
      "step": 2546
    },
    {
      "epoch": 1.271592611083375,
      "grad_norm": 0.12966834008693695,
      "learning_rate": 0.00048728407388916626,
      "loss": 0.0032,
      "step": 2547
    },
    {
      "epoch": 1.27209186220669,
      "grad_norm": 0.22920632362365723,
      "learning_rate": 0.00048727908137793306,
      "loss": 0.0068,
      "step": 2548
    },
    {
      "epoch": 1.272591113330005,
      "grad_norm": 0.08580953627824783,
      "learning_rate": 0.0004872740888666999,
      "loss": 0.0029,
      "step": 2549
    },
    {
      "epoch": 1.27309036445332,
      "grad_norm": 0.1582862287759781,
      "learning_rate": 0.00048726909635546676,
      "loss": 0.0038,
      "step": 2550
    },
    {
      "epoch": 1.273589615576635,
      "grad_norm": 0.5086723566055298,
      "learning_rate": 0.0004872641038442336,
      "loss": 0.0148,
      "step": 2551
    },
    {
      "epoch": 1.2740888666999501,
      "grad_norm": 0.5401518940925598,
      "learning_rate": 0.00048725911133300047,
      "loss": 0.0062,
      "step": 2552
    },
    {
      "epoch": 1.274588117823265,
      "grad_norm": 0.024759935215115547,
      "learning_rate": 0.0004872541188217673,
      "loss": 0.0013,
      "step": 2553
    },
    {
      "epoch": 1.27508736894658,
      "grad_norm": 0.07664089649915695,
      "learning_rate": 0.00048724912631053417,
      "loss": 0.0021,
      "step": 2554
    },
    {
      "epoch": 1.275586620069895,
      "grad_norm": 0.39127397537231445,
      "learning_rate": 0.000487244133799301,
      "loss": 0.0071,
      "step": 2555
    },
    {
      "epoch": 1.2760858711932102,
      "grad_norm": 24.894908905029297,
      "learning_rate": 0.0004872391412880679,
      "loss": 0.0433,
      "step": 2556
    },
    {
      "epoch": 1.2765851223165252,
      "grad_norm": 0.36737242341041565,
      "learning_rate": 0.00048723414877683473,
      "loss": 0.005,
      "step": 2557
    },
    {
      "epoch": 1.2770843734398403,
      "grad_norm": 0.029835296794772148,
      "learning_rate": 0.0004872291562656016,
      "loss": 0.0013,
      "step": 2558
    },
    {
      "epoch": 1.2775836245631553,
      "grad_norm": 0.29389408230781555,
      "learning_rate": 0.00048722416375436843,
      "loss": 0.0149,
      "step": 2559
    },
    {
      "epoch": 1.2780828756864704,
      "grad_norm": 0.5182276368141174,
      "learning_rate": 0.0004872191712431353,
      "loss": 0.0062,
      "step": 2560
    },
    {
      "epoch": 1.2785821268097854,
      "grad_norm": 0.13000452518463135,
      "learning_rate": 0.00048721417873190214,
      "loss": 0.0029,
      "step": 2561
    },
    {
      "epoch": 1.2790813779331003,
      "grad_norm": 0.037042420357465744,
      "learning_rate": 0.000487209186220669,
      "loss": 0.0016,
      "step": 2562
    },
    {
      "epoch": 1.2795806290564153,
      "grad_norm": 0.4059123396873474,
      "learning_rate": 0.00048720419370943584,
      "loss": 0.0294,
      "step": 2563
    },
    {
      "epoch": 1.2800798801797304,
      "grad_norm": 2.5337491035461426,
      "learning_rate": 0.0004871992011982027,
      "loss": 0.0184,
      "step": 2564
    },
    {
      "epoch": 1.2805791313030455,
      "grad_norm": 0.11584354937076569,
      "learning_rate": 0.00048719420868696954,
      "loss": 0.0033,
      "step": 2565
    },
    {
      "epoch": 1.2810783824263605,
      "grad_norm": 0.10850795358419418,
      "learning_rate": 0.0004871892161757364,
      "loss": 0.0026,
      "step": 2566
    },
    {
      "epoch": 1.2815776335496754,
      "grad_norm": 16.373830795288086,
      "learning_rate": 0.00048718422366450325,
      "loss": 0.1142,
      "step": 2567
    },
    {
      "epoch": 1.2820768846729904,
      "grad_norm": 0.19627916812896729,
      "learning_rate": 0.0004871792311532701,
      "loss": 0.0028,
      "step": 2568
    },
    {
      "epoch": 1.2825761357963055,
      "grad_norm": 0.054988421499729156,
      "learning_rate": 0.00048717423864203695,
      "loss": 0.0022,
      "step": 2569
    },
    {
      "epoch": 1.2830753869196205,
      "grad_norm": 0.0570392869412899,
      "learning_rate": 0.0004871692461308038,
      "loss": 0.0018,
      "step": 2570
    },
    {
      "epoch": 1.2835746380429356,
      "grad_norm": 0.19415146112442017,
      "learning_rate": 0.00048716425361957066,
      "loss": 0.0045,
      "step": 2571
    },
    {
      "epoch": 1.2840738891662506,
      "grad_norm": 0.08622808009386063,
      "learning_rate": 0.0004871592611083375,
      "loss": 0.0022,
      "step": 2572
    },
    {
      "epoch": 1.2845731402895657,
      "grad_norm": 0.06573616713285446,
      "learning_rate": 0.00048715426859710436,
      "loss": 0.0018,
      "step": 2573
    },
    {
      "epoch": 1.2850723914128808,
      "grad_norm": 0.028242366388440132,
      "learning_rate": 0.0004871492760858712,
      "loss": 0.0017,
      "step": 2574
    },
    {
      "epoch": 1.2855716425361958,
      "grad_norm": 0.5034346580505371,
      "learning_rate": 0.00048714428357463807,
      "loss": 0.0067,
      "step": 2575
    },
    {
      "epoch": 1.2860708936595107,
      "grad_norm": 0.04959108680486679,
      "learning_rate": 0.00048713929106340486,
      "loss": 0.0018,
      "step": 2576
    },
    {
      "epoch": 1.2865701447828257,
      "grad_norm": 0.22715334594249725,
      "learning_rate": 0.0004871342985521717,
      "loss": 0.0051,
      "step": 2577
    },
    {
      "epoch": 1.2870693959061408,
      "grad_norm": 0.09425953775644302,
      "learning_rate": 0.00048712930604093857,
      "loss": 0.0014,
      "step": 2578
    },
    {
      "epoch": 1.2875686470294558,
      "grad_norm": 0.11124908179044724,
      "learning_rate": 0.0004871243135297054,
      "loss": 0.0021,
      "step": 2579
    },
    {
      "epoch": 1.288067898152771,
      "grad_norm": 0.30403217673301697,
      "learning_rate": 0.00048711932101847227,
      "loss": 0.0066,
      "step": 2580
    },
    {
      "epoch": 1.2885671492760857,
      "grad_norm": 0.01409071497619152,
      "learning_rate": 0.0004871143285072391,
      "loss": 0.0009,
      "step": 2581
    },
    {
      "epoch": 1.2890664003994008,
      "grad_norm": 2.0993123054504395,
      "learning_rate": 0.000487109335996006,
      "loss": 0.0283,
      "step": 2582
    },
    {
      "epoch": 1.2895656515227159,
      "grad_norm": 0.13346479833126068,
      "learning_rate": 0.00048710434348477283,
      "loss": 0.0017,
      "step": 2583
    },
    {
      "epoch": 1.290064902646031,
      "grad_norm": 0.06060096621513367,
      "learning_rate": 0.0004870993509735397,
      "loss": 0.0015,
      "step": 2584
    },
    {
      "epoch": 1.290564153769346,
      "grad_norm": 0.18426257371902466,
      "learning_rate": 0.00048709435846230653,
      "loss": 0.0026,
      "step": 2585
    },
    {
      "epoch": 1.291063404892661,
      "grad_norm": 0.35260888934135437,
      "learning_rate": 0.0004870893659510734,
      "loss": 0.0017,
      "step": 2586
    },
    {
      "epoch": 1.291562656015976,
      "grad_norm": 0.06720195710659027,
      "learning_rate": 0.00048708437343984024,
      "loss": 0.0014,
      "step": 2587
    },
    {
      "epoch": 1.2920619071392911,
      "grad_norm": 0.16156995296478271,
      "learning_rate": 0.0004870793809286071,
      "loss": 0.0026,
      "step": 2588
    },
    {
      "epoch": 1.2925611582626062,
      "grad_norm": 0.442218542098999,
      "learning_rate": 0.00048707438841737394,
      "loss": 0.0125,
      "step": 2589
    },
    {
      "epoch": 1.293060409385921,
      "grad_norm": 0.052917513996362686,
      "learning_rate": 0.0004870693959061408,
      "loss": 0.0016,
      "step": 2590
    },
    {
      "epoch": 1.293559660509236,
      "grad_norm": 0.041361961513757706,
      "learning_rate": 0.00048706440339490764,
      "loss": 0.0013,
      "step": 2591
    },
    {
      "epoch": 1.2940589116325512,
      "grad_norm": 1.8880726099014282,
      "learning_rate": 0.0004870594108836745,
      "loss": 0.016,
      "step": 2592
    },
    {
      "epoch": 1.2945581627558662,
      "grad_norm": 0.35280343890190125,
      "learning_rate": 0.00048705441837244135,
      "loss": 0.0237,
      "step": 2593
    },
    {
      "epoch": 1.2950574138791813,
      "grad_norm": 0.10150764137506485,
      "learning_rate": 0.0004870494258612082,
      "loss": 0.0012,
      "step": 2594
    },
    {
      "epoch": 1.2955566650024963,
      "grad_norm": 0.026915593072772026,
      "learning_rate": 0.00048704443334997505,
      "loss": 0.0011,
      "step": 2595
    },
    {
      "epoch": 1.2960559161258112,
      "grad_norm": 0.08724687248468399,
      "learning_rate": 0.0004870394408387419,
      "loss": 0.0017,
      "step": 2596
    },
    {
      "epoch": 1.2965551672491262,
      "grad_norm": 0.29067352414131165,
      "learning_rate": 0.00048703444832750876,
      "loss": 0.0031,
      "step": 2597
    },
    {
      "epoch": 1.2970544183724413,
      "grad_norm": 0.24274027347564697,
      "learning_rate": 0.0004870294558162756,
      "loss": 0.0174,
      "step": 2598
    },
    {
      "epoch": 1.2975536694957563,
      "grad_norm": 0.10078778117895126,
      "learning_rate": 0.00048702446330504246,
      "loss": 0.0016,
      "step": 2599
    },
    {
      "epoch": 1.2980529206190714,
      "grad_norm": 0.19927538931369781,
      "learning_rate": 0.0004870194707938093,
      "loss": 0.0027,
      "step": 2600
    },
    {
      "epoch": 1.2985521717423865,
      "grad_norm": 0.028065526857972145,
      "learning_rate": 0.00048701447828257616,
      "loss": 0.0011,
      "step": 2601
    },
    {
      "epoch": 1.2990514228657015,
      "grad_norm": 0.02889315038919449,
      "learning_rate": 0.000487009485771343,
      "loss": 0.0012,
      "step": 2602
    },
    {
      "epoch": 1.2995506739890166,
      "grad_norm": 0.0488395057618618,
      "learning_rate": 0.00048700449326010987,
      "loss": 0.0011,
      "step": 2603
    },
    {
      "epoch": 1.3000499251123316,
      "grad_norm": 0.08272653818130493,
      "learning_rate": 0.0004869995007488767,
      "loss": 0.0021,
      "step": 2604
    },
    {
      "epoch": 1.3005491762356465,
      "grad_norm": 0.0945894792675972,
      "learning_rate": 0.0004869945082376435,
      "loss": 0.0016,
      "step": 2605
    },
    {
      "epoch": 1.3010484273589615,
      "grad_norm": 0.14785733819007874,
      "learning_rate": 0.00048698951572641037,
      "loss": 0.0037,
      "step": 2606
    },
    {
      "epoch": 1.3015476784822766,
      "grad_norm": 0.14520448446273804,
      "learning_rate": 0.0004869845232151772,
      "loss": 0.0019,
      "step": 2607
    },
    {
      "epoch": 1.3020469296055917,
      "grad_norm": 0.15338219702243805,
      "learning_rate": 0.0004869795307039441,
      "loss": 0.0034,
      "step": 2608
    },
    {
      "epoch": 1.3025461807289067,
      "grad_norm": 0.02095249854028225,
      "learning_rate": 0.00048697453819271093,
      "loss": 0.0011,
      "step": 2609
    },
    {
      "epoch": 1.3030454318522215,
      "grad_norm": 0.5319147706031799,
      "learning_rate": 0.0004869695456814778,
      "loss": 0.0069,
      "step": 2610
    },
    {
      "epoch": 1.3035446829755366,
      "grad_norm": 0.02124072052538395,
      "learning_rate": 0.00048696455317024463,
      "loss": 0.001,
      "step": 2611
    },
    {
      "epoch": 1.3040439340988517,
      "grad_norm": 0.09094725549221039,
      "learning_rate": 0.0004869595606590115,
      "loss": 0.0012,
      "step": 2612
    },
    {
      "epoch": 1.3045431852221667,
      "grad_norm": 0.0695767030119896,
      "learning_rate": 0.00048695456814777834,
      "loss": 0.0012,
      "step": 2613
    },
    {
      "epoch": 1.3050424363454818,
      "grad_norm": 0.3363952040672302,
      "learning_rate": 0.0004869495756365452,
      "loss": 0.01,
      "step": 2614
    },
    {
      "epoch": 1.3055416874687968,
      "grad_norm": 0.09527657926082611,
      "learning_rate": 0.00048694458312531204,
      "loss": 0.0017,
      "step": 2615
    },
    {
      "epoch": 1.306040938592112,
      "grad_norm": 0.40615251660346985,
      "learning_rate": 0.0004869395906140789,
      "loss": 0.019,
      "step": 2616
    },
    {
      "epoch": 1.306540189715427,
      "grad_norm": 0.0186765156686306,
      "learning_rate": 0.00048693459810284574,
      "loss": 0.0009,
      "step": 2617
    },
    {
      "epoch": 1.307039440838742,
      "grad_norm": 0.05132019519805908,
      "learning_rate": 0.0004869296055916126,
      "loss": 0.0012,
      "step": 2618
    },
    {
      "epoch": 1.3075386919620569,
      "grad_norm": 0.0778043195605278,
      "learning_rate": 0.00048692461308037945,
      "loss": 0.0017,
      "step": 2619
    },
    {
      "epoch": 1.308037943085372,
      "grad_norm": 0.0987171083688736,
      "learning_rate": 0.0004869196205691463,
      "loss": 0.0017,
      "step": 2620
    },
    {
      "epoch": 1.308537194208687,
      "grad_norm": 0.9619347453117371,
      "learning_rate": 0.00048691462805791315,
      "loss": 0.0104,
      "step": 2621
    },
    {
      "epoch": 1.309036445332002,
      "grad_norm": 0.012505032122135162,
      "learning_rate": 0.00048690963554668,
      "loss": 0.0009,
      "step": 2622
    },
    {
      "epoch": 1.309535696455317,
      "grad_norm": 0.016000457108020782,
      "learning_rate": 0.00048690464303544686,
      "loss": 0.0008,
      "step": 2623
    },
    {
      "epoch": 1.310034947578632,
      "grad_norm": 0.044328201562166214,
      "learning_rate": 0.0004868996505242137,
      "loss": 0.0013,
      "step": 2624
    },
    {
      "epoch": 1.310534198701947,
      "grad_norm": 0.06524176150560379,
      "learning_rate": 0.00048689465801298056,
      "loss": 0.0017,
      "step": 2625
    },
    {
      "epoch": 1.311033449825262,
      "grad_norm": 0.0940566286444664,
      "learning_rate": 0.0004868896655017474,
      "loss": 0.0013,
      "step": 2626
    },
    {
      "epoch": 1.311532700948577,
      "grad_norm": 0.4258826971054077,
      "learning_rate": 0.00048688467299051426,
      "loss": 0.0061,
      "step": 2627
    },
    {
      "epoch": 1.3120319520718922,
      "grad_norm": 0.02416018396615982,
      "learning_rate": 0.0004868796804792811,
      "loss": 0.0008,
      "step": 2628
    },
    {
      "epoch": 1.3125312031952072,
      "grad_norm": 0.1411295384168625,
      "learning_rate": 0.00048687468796804797,
      "loss": 0.0031,
      "step": 2629
    },
    {
      "epoch": 1.3130304543185223,
      "grad_norm": 0.37499549984931946,
      "learning_rate": 0.0004868696954568148,
      "loss": 0.0119,
      "step": 2630
    },
    {
      "epoch": 1.3135297054418373,
      "grad_norm": 0.2701627016067505,
      "learning_rate": 0.00048686470294558167,
      "loss": 0.0167,
      "step": 2631
    },
    {
      "epoch": 1.3140289565651524,
      "grad_norm": 0.009234915487468243,
      "learning_rate": 0.0004868597104343485,
      "loss": 0.0006,
      "step": 2632
    },
    {
      "epoch": 1.3145282076884672,
      "grad_norm": 0.2226438969373703,
      "learning_rate": 0.0004868547179231154,
      "loss": 0.0037,
      "step": 2633
    },
    {
      "epoch": 1.3150274588117823,
      "grad_norm": 0.47093865275382996,
      "learning_rate": 0.0004868497254118822,
      "loss": 0.0104,
      "step": 2634
    },
    {
      "epoch": 1.3155267099350973,
      "grad_norm": 1.8127071857452393,
      "learning_rate": 0.000486844732900649,
      "loss": 0.0161,
      "step": 2635
    },
    {
      "epoch": 1.3160259610584124,
      "grad_norm": 0.19853520393371582,
      "learning_rate": 0.0004868397403894159,
      "loss": 0.0022,
      "step": 2636
    },
    {
      "epoch": 1.3165252121817275,
      "grad_norm": 0.07916136085987091,
      "learning_rate": 0.00048683474787818273,
      "loss": 0.0018,
      "step": 2637
    },
    {
      "epoch": 1.3170244633050423,
      "grad_norm": 0.08276491612195969,
      "learning_rate": 0.0004868297553669496,
      "loss": 0.0026,
      "step": 2638
    },
    {
      "epoch": 1.3175237144283574,
      "grad_norm": 0.06953655928373337,
      "learning_rate": 0.00048682476285571644,
      "loss": 0.0019,
      "step": 2639
    },
    {
      "epoch": 1.3180229655516724,
      "grad_norm": 0.8055957555770874,
      "learning_rate": 0.0004868197703444833,
      "loss": 0.0124,
      "step": 2640
    },
    {
      "epoch": 1.3185222166749875,
      "grad_norm": 0.44334283471107483,
      "learning_rate": 0.00048681477783325014,
      "loss": 0.014,
      "step": 2641
    },
    {
      "epoch": 1.3190214677983025,
      "grad_norm": 0.038104161620140076,
      "learning_rate": 0.000486809785322017,
      "loss": 0.0013,
      "step": 2642
    },
    {
      "epoch": 1.3195207189216176,
      "grad_norm": 0.3174091577529907,
      "learning_rate": 0.00048680479281078384,
      "loss": 0.0071,
      "step": 2643
    },
    {
      "epoch": 1.3200199700449327,
      "grad_norm": 0.4500976502895355,
      "learning_rate": 0.0004867998002995507,
      "loss": 0.0059,
      "step": 2644
    },
    {
      "epoch": 1.3205192211682477,
      "grad_norm": 0.3783894181251526,
      "learning_rate": 0.00048679480778831755,
      "loss": 0.0064,
      "step": 2645
    },
    {
      "epoch": 1.3210184722915628,
      "grad_norm": 0.11257472634315491,
      "learning_rate": 0.0004867898152770844,
      "loss": 0.0022,
      "step": 2646
    },
    {
      "epoch": 1.3215177234148776,
      "grad_norm": 0.24904553592205048,
      "learning_rate": 0.00048678482276585125,
      "loss": 0.002,
      "step": 2647
    },
    {
      "epoch": 1.3220169745381927,
      "grad_norm": 0.23250392079353333,
      "learning_rate": 0.0004867798302546181,
      "loss": 0.0039,
      "step": 2648
    },
    {
      "epoch": 1.3225162256615077,
      "grad_norm": 0.08658386766910553,
      "learning_rate": 0.00048677483774338496,
      "loss": 0.0019,
      "step": 2649
    },
    {
      "epoch": 1.3230154767848228,
      "grad_norm": 1.621633768081665,
      "learning_rate": 0.0004867698452321518,
      "loss": 0.0209,
      "step": 2650
    },
    {
      "epoch": 1.3235147279081378,
      "grad_norm": 0.09931569546461105,
      "learning_rate": 0.00048676485272091866,
      "loss": 0.0023,
      "step": 2651
    },
    {
      "epoch": 1.324013979031453,
      "grad_norm": 0.21936136484146118,
      "learning_rate": 0.0004867598602096855,
      "loss": 0.0036,
      "step": 2652
    },
    {
      "epoch": 1.3245132301547677,
      "grad_norm": 0.409339040517807,
      "learning_rate": 0.00048675486769845236,
      "loss": 0.0048,
      "step": 2653
    },
    {
      "epoch": 1.3250124812780828,
      "grad_norm": 0.12346222251653671,
      "learning_rate": 0.0004867498751872192,
      "loss": 0.0024,
      "step": 2654
    },
    {
      "epoch": 1.3255117324013979,
      "grad_norm": 0.1056726947426796,
      "learning_rate": 0.00048674488267598607,
      "loss": 0.0025,
      "step": 2655
    },
    {
      "epoch": 1.326010983524713,
      "grad_norm": 0.5207973122596741,
      "learning_rate": 0.0004867398901647529,
      "loss": 0.011,
      "step": 2656
    },
    {
      "epoch": 1.326510234648028,
      "grad_norm": 0.4765581786632538,
      "learning_rate": 0.00048673489765351977,
      "loss": 0.0083,
      "step": 2657
    },
    {
      "epoch": 1.327009485771343,
      "grad_norm": 0.3856737017631531,
      "learning_rate": 0.0004867299051422866,
      "loss": 0.0042,
      "step": 2658
    },
    {
      "epoch": 1.327508736894658,
      "grad_norm": 0.04462464153766632,
      "learning_rate": 0.0004867249126310535,
      "loss": 0.0012,
      "step": 2659
    },
    {
      "epoch": 1.3280079880179732,
      "grad_norm": 0.13117103278636932,
      "learning_rate": 0.00048671992011982033,
      "loss": 0.0017,
      "step": 2660
    },
    {
      "epoch": 1.328507239141288,
      "grad_norm": 0.21056115627288818,
      "learning_rate": 0.0004867149276085872,
      "loss": 0.0028,
      "step": 2661
    },
    {
      "epoch": 1.329006490264603,
      "grad_norm": 0.1261138767004013,
      "learning_rate": 0.00048670993509735403,
      "loss": 0.0024,
      "step": 2662
    },
    {
      "epoch": 1.329505741387918,
      "grad_norm": 0.463504433631897,
      "learning_rate": 0.0004867049425861208,
      "loss": 0.0149,
      "step": 2663
    },
    {
      "epoch": 1.3300049925112332,
      "grad_norm": 0.9410284161567688,
      "learning_rate": 0.00048669995007488763,
      "loss": 0.0034,
      "step": 2664
    },
    {
      "epoch": 1.3305042436345482,
      "grad_norm": 0.08008396625518799,
      "learning_rate": 0.0004866949575636545,
      "loss": 0.0021,
      "step": 2665
    },
    {
      "epoch": 1.3310034947578633,
      "grad_norm": 0.5665096044540405,
      "learning_rate": 0.00048668996505242133,
      "loss": 0.0192,
      "step": 2666
    },
    {
      "epoch": 1.3315027458811781,
      "grad_norm": 0.09895297139883041,
      "learning_rate": 0.0004866849725411882,
      "loss": 0.0024,
      "step": 2667
    },
    {
      "epoch": 1.3320019970044932,
      "grad_norm": 0.53016597032547,
      "learning_rate": 0.00048667998002995504,
      "loss": 0.0074,
      "step": 2668
    },
    {
      "epoch": 1.3325012481278082,
      "grad_norm": 0.0604172982275486,
      "learning_rate": 0.0004866749875187219,
      "loss": 0.0023,
      "step": 2669
    },
    {
      "epoch": 1.3330004992511233,
      "grad_norm": 0.08985427767038345,
      "learning_rate": 0.00048666999500748874,
      "loss": 0.0021,
      "step": 2670
    },
    {
      "epoch": 1.3334997503744384,
      "grad_norm": 0.21628661453723907,
      "learning_rate": 0.0004866650024962556,
      "loss": 0.0042,
      "step": 2671
    },
    {
      "epoch": 1.3339990014977534,
      "grad_norm": 0.10475073009729385,
      "learning_rate": 0.00048666000998502244,
      "loss": 0.0023,
      "step": 2672
    },
    {
      "epoch": 1.3344982526210685,
      "grad_norm": 0.24009545147418976,
      "learning_rate": 0.0004866550174737893,
      "loss": 0.0121,
      "step": 2673
    },
    {
      "epoch": 1.3349975037443835,
      "grad_norm": 0.013277256861329079,
      "learning_rate": 0.00048665002496255615,
      "loss": 0.001,
      "step": 2674
    },
    {
      "epoch": 1.3354967548676986,
      "grad_norm": 1.3239449262619019,
      "learning_rate": 0.000486645032451323,
      "loss": 0.0232,
      "step": 2675
    },
    {
      "epoch": 1.3359960059910134,
      "grad_norm": 0.290993869304657,
      "learning_rate": 0.00048664003994008985,
      "loss": 0.0118,
      "step": 2676
    },
    {
      "epoch": 1.3364952571143285,
      "grad_norm": 0.41606682538986206,
      "learning_rate": 0.0004866350474288567,
      "loss": 0.0105,
      "step": 2677
    },
    {
      "epoch": 1.3369945082376435,
      "grad_norm": 0.10081005841493607,
      "learning_rate": 0.00048663005491762356,
      "loss": 0.0023,
      "step": 2678
    },
    {
      "epoch": 1.3374937593609586,
      "grad_norm": 0.46249279379844666,
      "learning_rate": 0.0004866250624063904,
      "loss": 0.0096,
      "step": 2679
    },
    {
      "epoch": 1.3379930104842737,
      "grad_norm": 0.5306825637817383,
      "learning_rate": 0.00048662006989515726,
      "loss": 0.0236,
      "step": 2680
    },
    {
      "epoch": 1.3384922616075885,
      "grad_norm": 0.3536666929721832,
      "learning_rate": 0.0004866150773839241,
      "loss": 0.0036,
      "step": 2681
    },
    {
      "epoch": 1.3389915127309036,
      "grad_norm": 0.29429107904434204,
      "learning_rate": 0.00048661008487269097,
      "loss": 0.0056,
      "step": 2682
    },
    {
      "epoch": 1.3394907638542186,
      "grad_norm": 0.09407585114240646,
      "learning_rate": 0.0004866050923614578,
      "loss": 0.0024,
      "step": 2683
    },
    {
      "epoch": 1.3399900149775337,
      "grad_norm": 0.06601755321025848,
      "learning_rate": 0.00048660009985022467,
      "loss": 0.0023,
      "step": 2684
    },
    {
      "epoch": 1.3404892661008487,
      "grad_norm": 0.33572882413864136,
      "learning_rate": 0.0004865951073389915,
      "loss": 0.0063,
      "step": 2685
    },
    {
      "epoch": 1.3409885172241638,
      "grad_norm": 0.2330433577299118,
      "learning_rate": 0.0004865901148277584,
      "loss": 0.0065,
      "step": 2686
    },
    {
      "epoch": 1.3414877683474788,
      "grad_norm": 0.3332303464412689,
      "learning_rate": 0.0004865851223165252,
      "loss": 0.0076,
      "step": 2687
    },
    {
      "epoch": 1.341987019470794,
      "grad_norm": 0.3458109200000763,
      "learning_rate": 0.0004865801298052921,
      "loss": 0.0101,
      "step": 2688
    },
    {
      "epoch": 1.342486270594109,
      "grad_norm": 0.3158918619155884,
      "learning_rate": 0.00048657513729405893,
      "loss": 0.0074,
      "step": 2689
    },
    {
      "epoch": 1.3429855217174238,
      "grad_norm": 0.47543269395828247,
      "learning_rate": 0.0004865701447828258,
      "loss": 0.0138,
      "step": 2690
    },
    {
      "epoch": 1.3434847728407389,
      "grad_norm": 0.2507237493991852,
      "learning_rate": 0.00048656515227159263,
      "loss": 0.0035,
      "step": 2691
    },
    {
      "epoch": 1.343984023964054,
      "grad_norm": 0.10025303065776825,
      "learning_rate": 0.00048656015976035943,
      "loss": 0.0019,
      "step": 2692
    },
    {
      "epoch": 1.344483275087369,
      "grad_norm": 0.0333232544362545,
      "learning_rate": 0.0004865551672491263,
      "loss": 0.0016,
      "step": 2693
    },
    {
      "epoch": 1.344982526210684,
      "grad_norm": 0.04820714518427849,
      "learning_rate": 0.00048655017473789314,
      "loss": 0.0015,
      "step": 2694
    },
    {
      "epoch": 1.3454817773339989,
      "grad_norm": 0.0660119354724884,
      "learning_rate": 0.00048654518222666,
      "loss": 0.0028,
      "step": 2695
    },
    {
      "epoch": 1.345981028457314,
      "grad_norm": 0.25193163752555847,
      "learning_rate": 0.00048654018971542684,
      "loss": 0.0099,
      "step": 2696
    },
    {
      "epoch": 1.346480279580629,
      "grad_norm": 0.4400884509086609,
      "learning_rate": 0.0004865351972041937,
      "loss": 0.0172,
      "step": 2697
    },
    {
      "epoch": 1.346979530703944,
      "grad_norm": 0.0496794730424881,
      "learning_rate": 0.00048653020469296054,
      "loss": 0.0017,
      "step": 2698
    },
    {
      "epoch": 1.347478781827259,
      "grad_norm": 0.5088082551956177,
      "learning_rate": 0.0004865252121817274,
      "loss": 0.0037,
      "step": 2699
    },
    {
      "epoch": 1.3479780329505742,
      "grad_norm": 0.20490573346614838,
      "learning_rate": 0.00048652021967049425,
      "loss": 0.0032,
      "step": 2700
    },
    {
      "epoch": 1.3484772840738892,
      "grad_norm": 0.577852189540863,
      "learning_rate": 0.0004865152271592611,
      "loss": 0.0084,
      "step": 2701
    },
    {
      "epoch": 1.3489765351972043,
      "grad_norm": 0.9778582453727722,
      "learning_rate": 0.00048651023464802795,
      "loss": 0.0164,
      "step": 2702
    },
    {
      "epoch": 1.3494757863205193,
      "grad_norm": 0.5214506983757019,
      "learning_rate": 0.0004865052421367948,
      "loss": 0.009,
      "step": 2703
    },
    {
      "epoch": 1.3499750374438342,
      "grad_norm": 0.30279961228370667,
      "learning_rate": 0.00048650024962556166,
      "loss": 0.0108,
      "step": 2704
    },
    {
      "epoch": 1.3504742885671492,
      "grad_norm": 0.07824020087718964,
      "learning_rate": 0.0004864952571143285,
      "loss": 0.002,
      "step": 2705
    },
    {
      "epoch": 1.3509735396904643,
      "grad_norm": 0.09314113110303879,
      "learning_rate": 0.00048649026460309536,
      "loss": 0.0027,
      "step": 2706
    },
    {
      "epoch": 1.3514727908137794,
      "grad_norm": 0.09833583235740662,
      "learning_rate": 0.0004864852720918622,
      "loss": 0.0028,
      "step": 2707
    },
    {
      "epoch": 1.3519720419370944,
      "grad_norm": 0.5392090678215027,
      "learning_rate": 0.00048648027958062907,
      "loss": 0.013,
      "step": 2708
    },
    {
      "epoch": 1.3524712930604093,
      "grad_norm": 0.1055990606546402,
      "learning_rate": 0.0004864752870693959,
      "loss": 0.0022,
      "step": 2709
    },
    {
      "epoch": 1.3529705441837243,
      "grad_norm": 0.4911380410194397,
      "learning_rate": 0.00048647029455816277,
      "loss": 0.009,
      "step": 2710
    },
    {
      "epoch": 1.3534697953070394,
      "grad_norm": 0.6451190114021301,
      "learning_rate": 0.0004864653020469296,
      "loss": 0.0191,
      "step": 2711
    },
    {
      "epoch": 1.3539690464303544,
      "grad_norm": 0.35810399055480957,
      "learning_rate": 0.0004864603095356965,
      "loss": 0.0057,
      "step": 2712
    },
    {
      "epoch": 1.3544682975536695,
      "grad_norm": 0.17267121374607086,
      "learning_rate": 0.0004864553170244633,
      "loss": 0.0029,
      "step": 2713
    },
    {
      "epoch": 1.3549675486769845,
      "grad_norm": 0.07451941072940826,
      "learning_rate": 0.0004864503245132302,
      "loss": 0.0022,
      "step": 2714
    },
    {
      "epoch": 1.3554667998002996,
      "grad_norm": 0.02991310879588127,
      "learning_rate": 0.00048644533200199703,
      "loss": 0.0012,
      "step": 2715
    },
    {
      "epoch": 1.3559660509236147,
      "grad_norm": 1.1693696975708008,
      "learning_rate": 0.0004864403394907639,
      "loss": 0.0218,
      "step": 2716
    },
    {
      "epoch": 1.3564653020469297,
      "grad_norm": 2.301255702972412,
      "learning_rate": 0.00048643534697953073,
      "loss": 0.0309,
      "step": 2717
    },
    {
      "epoch": 1.3569645531702446,
      "grad_norm": 0.11594389379024506,
      "learning_rate": 0.0004864303544682976,
      "loss": 0.0027,
      "step": 2718
    },
    {
      "epoch": 1.3574638042935596,
      "grad_norm": 0.02505451999604702,
      "learning_rate": 0.00048642536195706444,
      "loss": 0.0013,
      "step": 2719
    },
    {
      "epoch": 1.3579630554168747,
      "grad_norm": 0.3510832190513611,
      "learning_rate": 0.0004864203694458313,
      "loss": 0.0059,
      "step": 2720
    },
    {
      "epoch": 1.3584623065401897,
      "grad_norm": 0.026384184136986732,
      "learning_rate": 0.0004864153769345981,
      "loss": 0.0013,
      "step": 2721
    },
    {
      "epoch": 1.3589615576635048,
      "grad_norm": 0.9853716492652893,
      "learning_rate": 0.00048641038442336494,
      "loss": 0.0333,
      "step": 2722
    },
    {
      "epoch": 1.3594608087868199,
      "grad_norm": 0.022374369204044342,
      "learning_rate": 0.0004864053919121318,
      "loss": 0.0014,
      "step": 2723
    },
    {
      "epoch": 1.3599600599101347,
      "grad_norm": 0.25165969133377075,
      "learning_rate": 0.00048640039940089864,
      "loss": 0.0129,
      "step": 2724
    },
    {
      "epoch": 1.3604593110334497,
      "grad_norm": 0.5910137295722961,
      "learning_rate": 0.0004863954068896655,
      "loss": 0.0144,
      "step": 2725
    },
    {
      "epoch": 1.3609585621567648,
      "grad_norm": 0.0587879978120327,
      "learning_rate": 0.00048639041437843235,
      "loss": 0.0018,
      "step": 2726
    },
    {
      "epoch": 1.3614578132800799,
      "grad_norm": 0.7779810428619385,
      "learning_rate": 0.0004863854218671992,
      "loss": 0.0083,
      "step": 2727
    },
    {
      "epoch": 1.361957064403395,
      "grad_norm": 0.4788345694541931,
      "learning_rate": 0.00048638042935596605,
      "loss": 0.0119,
      "step": 2728
    },
    {
      "epoch": 1.36245631552671,
      "grad_norm": 0.30806204676628113,
      "learning_rate": 0.0004863754368447329,
      "loss": 0.0033,
      "step": 2729
    },
    {
      "epoch": 1.362955566650025,
      "grad_norm": 0.04353838413953781,
      "learning_rate": 0.00048637044433349976,
      "loss": 0.0016,
      "step": 2730
    },
    {
      "epoch": 1.36345481777334,
      "grad_norm": 0.2245190292596817,
      "learning_rate": 0.0004863654518222666,
      "loss": 0.0056,
      "step": 2731
    },
    {
      "epoch": 1.363954068896655,
      "grad_norm": 0.2586045563220978,
      "learning_rate": 0.00048636045931103346,
      "loss": 0.0022,
      "step": 2732
    },
    {
      "epoch": 1.36445332001997,
      "grad_norm": 1.3787349462509155,
      "learning_rate": 0.0004863554667998003,
      "loss": 0.0122,
      "step": 2733
    },
    {
      "epoch": 1.364952571143285,
      "grad_norm": 0.6663946509361267,
      "learning_rate": 0.00048635047428856716,
      "loss": 0.0194,
      "step": 2734
    },
    {
      "epoch": 1.3654518222666001,
      "grad_norm": 0.35896116495132446,
      "learning_rate": 0.000486345481777334,
      "loss": 0.0101,
      "step": 2735
    },
    {
      "epoch": 1.3659510733899152,
      "grad_norm": 0.04434265196323395,
      "learning_rate": 0.00048634048926610087,
      "loss": 0.0016,
      "step": 2736
    },
    {
      "epoch": 1.3664503245132302,
      "grad_norm": 0.053538188338279724,
      "learning_rate": 0.0004863354967548677,
      "loss": 0.0018,
      "step": 2737
    },
    {
      "epoch": 1.366949575636545,
      "grad_norm": 0.23259814083576202,
      "learning_rate": 0.00048633050424363457,
      "loss": 0.0045,
      "step": 2738
    },
    {
      "epoch": 1.3674488267598601,
      "grad_norm": 0.3214775025844574,
      "learning_rate": 0.0004863255117324014,
      "loss": 0.046,
      "step": 2739
    },
    {
      "epoch": 1.3679480778831752,
      "grad_norm": 0.39628201723098755,
      "learning_rate": 0.0004863205192211683,
      "loss": 0.0163,
      "step": 2740
    },
    {
      "epoch": 1.3684473290064902,
      "grad_norm": 0.22443531453609467,
      "learning_rate": 0.00048631552670993513,
      "loss": 0.0057,
      "step": 2741
    },
    {
      "epoch": 1.3689465801298053,
      "grad_norm": 0.8263009786605835,
      "learning_rate": 0.000486310534198702,
      "loss": 0.0073,
      "step": 2742
    },
    {
      "epoch": 1.3694458312531204,
      "grad_norm": 1.103247046470642,
      "learning_rate": 0.00048630554168746883,
      "loss": 0.0158,
      "step": 2743
    },
    {
      "epoch": 1.3699450823764354,
      "grad_norm": 0.023797333240509033,
      "learning_rate": 0.0004863005491762357,
      "loss": 0.0014,
      "step": 2744
    },
    {
      "epoch": 1.3704443334997505,
      "grad_norm": 0.501621663570404,
      "learning_rate": 0.00048629555666500254,
      "loss": 0.0046,
      "step": 2745
    },
    {
      "epoch": 1.3709435846230655,
      "grad_norm": 0.3725373148918152,
      "learning_rate": 0.0004862905641537694,
      "loss": 0.0073,
      "step": 2746
    },
    {
      "epoch": 1.3714428357463804,
      "grad_norm": 1.29042387008667,
      "learning_rate": 0.00048628557164253624,
      "loss": 0.0229,
      "step": 2747
    },
    {
      "epoch": 1.3719420868696954,
      "grad_norm": 1.213292121887207,
      "learning_rate": 0.0004862805791313031,
      "loss": 0.0118,
      "step": 2748
    },
    {
      "epoch": 1.3724413379930105,
      "grad_norm": 0.2098720818758011,
      "learning_rate": 0.00048627558662006995,
      "loss": 0.0049,
      "step": 2749
    },
    {
      "epoch": 1.3729405891163255,
      "grad_norm": 2.73671555519104,
      "learning_rate": 0.00048627059410883674,
      "loss": 0.0318,
      "step": 2750
    },
    {
      "epoch": 1.3734398402396406,
      "grad_norm": 0.47352901101112366,
      "learning_rate": 0.0004862656015976036,
      "loss": 0.012,
      "step": 2751
    },
    {
      "epoch": 1.3739390913629554,
      "grad_norm": 0.1839805543422699,
      "learning_rate": 0.00048626060908637045,
      "loss": 0.0026,
      "step": 2752
    },
    {
      "epoch": 1.3744383424862705,
      "grad_norm": 0.2362460494041443,
      "learning_rate": 0.0004862556165751373,
      "loss": 0.0332,
      "step": 2753
    },
    {
      "epoch": 1.3749375936095856,
      "grad_norm": 1.2229279279708862,
      "learning_rate": 0.00048625062406390415,
      "loss": 0.0288,
      "step": 2754
    },
    {
      "epoch": 1.3754368447329006,
      "grad_norm": 0.5722277760505676,
      "learning_rate": 0.000486245631552671,
      "loss": 0.0063,
      "step": 2755
    },
    {
      "epoch": 1.3759360958562157,
      "grad_norm": 0.5113716721534729,
      "learning_rate": 0.00048624063904143786,
      "loss": 0.0188,
      "step": 2756
    },
    {
      "epoch": 1.3764353469795307,
      "grad_norm": 0.2043006867170334,
      "learning_rate": 0.0004862356465302047,
      "loss": 0.004,
      "step": 2757
    },
    {
      "epoch": 1.3769345981028458,
      "grad_norm": 0.7488970756530762,
      "learning_rate": 0.00048623065401897156,
      "loss": 0.0227,
      "step": 2758
    },
    {
      "epoch": 1.3774338492261609,
      "grad_norm": 0.17022281885147095,
      "learning_rate": 0.0004862256615077384,
      "loss": 0.0034,
      "step": 2759
    },
    {
      "epoch": 1.377933100349476,
      "grad_norm": 0.14934472739696503,
      "learning_rate": 0.00048622066899650526,
      "loss": 0.0029,
      "step": 2760
    },
    {
      "epoch": 1.3784323514727908,
      "grad_norm": 0.4727209806442261,
      "learning_rate": 0.0004862156764852721,
      "loss": 0.0047,
      "step": 2761
    },
    {
      "epoch": 1.3789316025961058,
      "grad_norm": 1.0805118083953857,
      "learning_rate": 0.00048621068397403897,
      "loss": 0.0161,
      "step": 2762
    },
    {
      "epoch": 1.3794308537194209,
      "grad_norm": 1.3869737386703491,
      "learning_rate": 0.0004862056914628058,
      "loss": 0.0095,
      "step": 2763
    },
    {
      "epoch": 1.379930104842736,
      "grad_norm": 0.465641051530838,
      "learning_rate": 0.00048620069895157267,
      "loss": 0.0195,
      "step": 2764
    },
    {
      "epoch": 1.380429355966051,
      "grad_norm": 0.08401553332805634,
      "learning_rate": 0.0004861957064403395,
      "loss": 0.0017,
      "step": 2765
    },
    {
      "epoch": 1.3809286070893658,
      "grad_norm": 0.2664104104042053,
      "learning_rate": 0.0004861907139291064,
      "loss": 0.0043,
      "step": 2766
    },
    {
      "epoch": 1.3814278582126809,
      "grad_norm": 0.2450181245803833,
      "learning_rate": 0.00048618572141787323,
      "loss": 0.0061,
      "step": 2767
    },
    {
      "epoch": 1.381927109335996,
      "grad_norm": 0.5520176887512207,
      "learning_rate": 0.0004861807289066401,
      "loss": 0.0102,
      "step": 2768
    },
    {
      "epoch": 1.382426360459311,
      "grad_norm": 0.7360451817512512,
      "learning_rate": 0.00048617573639540693,
      "loss": 0.0293,
      "step": 2769
    },
    {
      "epoch": 1.382925611582626,
      "grad_norm": 0.15850557386875153,
      "learning_rate": 0.0004861707438841738,
      "loss": 0.0033,
      "step": 2770
    },
    {
      "epoch": 1.3834248627059411,
      "grad_norm": 0.14211834967136383,
      "learning_rate": 0.00048616575137294064,
      "loss": 0.0028,
      "step": 2771
    },
    {
      "epoch": 1.3839241138292562,
      "grad_norm": 1.0981864929199219,
      "learning_rate": 0.0004861607588617075,
      "loss": 0.0193,
      "step": 2772
    },
    {
      "epoch": 1.3844233649525712,
      "grad_norm": 0.03275860473513603,
      "learning_rate": 0.00048615576635047434,
      "loss": 0.0016,
      "step": 2773
    },
    {
      "epoch": 1.3849226160758863,
      "grad_norm": 0.9409998059272766,
      "learning_rate": 0.0004861507738392412,
      "loss": 0.0204,
      "step": 2774
    },
    {
      "epoch": 1.3854218671992011,
      "grad_norm": 0.13046739995479584,
      "learning_rate": 0.00048614578132800805,
      "loss": 0.0028,
      "step": 2775
    },
    {
      "epoch": 1.3859211183225162,
      "grad_norm": 0.43022048473358154,
      "learning_rate": 0.0004861407888167749,
      "loss": 0.0257,
      "step": 2776
    },
    {
      "epoch": 1.3864203694458312,
      "grad_norm": 2.011012315750122,
      "learning_rate": 0.0004861357963055417,
      "loss": 0.0267,
      "step": 2777
    },
    {
      "epoch": 1.3869196205691463,
      "grad_norm": 0.912659227848053,
      "learning_rate": 0.00048613080379430855,
      "loss": 0.0087,
      "step": 2778
    },
    {
      "epoch": 1.3874188716924614,
      "grad_norm": 0.6355982422828674,
      "learning_rate": 0.00048612581128307534,
      "loss": 0.0145,
      "step": 2779
    },
    {
      "epoch": 1.3879181228157762,
      "grad_norm": 1.2407546043395996,
      "learning_rate": 0.0004861208187718422,
      "loss": 0.0134,
      "step": 2780
    },
    {
      "epoch": 1.3884173739390913,
      "grad_norm": 0.30296650528907776,
      "learning_rate": 0.00048611582626060905,
      "loss": 0.0083,
      "step": 2781
    },
    {
      "epoch": 1.3889166250624063,
      "grad_norm": 0.31756892800331116,
      "learning_rate": 0.0004861108337493759,
      "loss": 0.0052,
      "step": 2782
    },
    {
      "epoch": 1.3894158761857214,
      "grad_norm": 0.21935023367404938,
      "learning_rate": 0.00048610584123814275,
      "loss": 0.0039,
      "step": 2783
    },
    {
      "epoch": 1.3899151273090364,
      "grad_norm": 0.3353346288204193,
      "learning_rate": 0.0004861008487269096,
      "loss": 0.0052,
      "step": 2784
    },
    {
      "epoch": 1.3904143784323515,
      "grad_norm": 0.6494097113609314,
      "learning_rate": 0.00048609585621567646,
      "loss": 0.0091,
      "step": 2785
    },
    {
      "epoch": 1.3909136295556666,
      "grad_norm": 0.5914085507392883,
      "learning_rate": 0.0004860908637044433,
      "loss": 0.0058,
      "step": 2786
    },
    {
      "epoch": 1.3914128806789816,
      "grad_norm": 0.30688804388046265,
      "learning_rate": 0.00048608587119321016,
      "loss": 0.0074,
      "step": 2787
    },
    {
      "epoch": 1.3919121318022967,
      "grad_norm": 0.85486900806427,
      "learning_rate": 0.000486080878681977,
      "loss": 0.0147,
      "step": 2788
    },
    {
      "epoch": 1.3924113829256115,
      "grad_norm": 0.39326825737953186,
      "learning_rate": 0.00048607588617074387,
      "loss": 0.0072,
      "step": 2789
    },
    {
      "epoch": 1.3929106340489266,
      "grad_norm": 0.7231006622314453,
      "learning_rate": 0.0004860708936595107,
      "loss": 0.0244,
      "step": 2790
    },
    {
      "epoch": 1.3934098851722416,
      "grad_norm": 0.18582431972026825,
      "learning_rate": 0.00048606590114827757,
      "loss": 0.0042,
      "step": 2791
    },
    {
      "epoch": 1.3939091362955567,
      "grad_norm": 0.04904232546687126,
      "learning_rate": 0.0004860609086370444,
      "loss": 0.0021,
      "step": 2792
    },
    {
      "epoch": 1.3944083874188717,
      "grad_norm": 0.8792830109596252,
      "learning_rate": 0.0004860559161258113,
      "loss": 0.0157,
      "step": 2793
    },
    {
      "epoch": 1.3949076385421868,
      "grad_norm": 0.2740669250488281,
      "learning_rate": 0.0004860509236145781,
      "loss": 0.0065,
      "step": 2794
    },
    {
      "epoch": 1.3954068896655016,
      "grad_norm": 0.3308844566345215,
      "learning_rate": 0.000486045931103345,
      "loss": 0.0051,
      "step": 2795
    },
    {
      "epoch": 1.3959061407888167,
      "grad_norm": 0.3573375642299652,
      "learning_rate": 0.00048604093859211183,
      "loss": 0.005,
      "step": 2796
    },
    {
      "epoch": 1.3964053919121318,
      "grad_norm": 0.25944921374320984,
      "learning_rate": 0.0004860359460808787,
      "loss": 0.005,
      "step": 2797
    },
    {
      "epoch": 1.3969046430354468,
      "grad_norm": 0.19472730159759521,
      "learning_rate": 0.00048603095356964553,
      "loss": 0.0049,
      "step": 2798
    },
    {
      "epoch": 1.3974038941587619,
      "grad_norm": 0.7519558072090149,
      "learning_rate": 0.0004860259610584124,
      "loss": 0.0125,
      "step": 2799
    },
    {
      "epoch": 1.397903145282077,
      "grad_norm": 0.03704726696014404,
      "learning_rate": 0.00048602096854717924,
      "loss": 0.0018,
      "step": 2800
    },
    {
      "epoch": 1.398402396405392,
      "grad_norm": 0.33284515142440796,
      "learning_rate": 0.0004860159760359461,
      "loss": 0.006,
      "step": 2801
    },
    {
      "epoch": 1.398901647528707,
      "grad_norm": 0.2267574518918991,
      "learning_rate": 0.00048601098352471294,
      "loss": 0.0044,
      "step": 2802
    },
    {
      "epoch": 1.3994008986520219,
      "grad_norm": 0.8411304950714111,
      "learning_rate": 0.0004860059910134798,
      "loss": 0.0093,
      "step": 2803
    },
    {
      "epoch": 1.399900149775337,
      "grad_norm": 0.06461471319198608,
      "learning_rate": 0.00048600099850224665,
      "loss": 0.0022,
      "step": 2804
    },
    {
      "epoch": 1.400399400898652,
      "grad_norm": 0.7129238247871399,
      "learning_rate": 0.0004859960059910135,
      "loss": 0.0126,
      "step": 2805
    },
    {
      "epoch": 1.400898652021967,
      "grad_norm": 0.5162407755851746,
      "learning_rate": 0.00048599101347978035,
      "loss": 0.015,
      "step": 2806
    },
    {
      "epoch": 1.4013979031452821,
      "grad_norm": 0.1032169833779335,
      "learning_rate": 0.0004859860209685472,
      "loss": 0.0019,
      "step": 2807
    },
    {
      "epoch": 1.4018971542685972,
      "grad_norm": 0.05902666971087456,
      "learning_rate": 0.000485981028457314,
      "loss": 0.0019,
      "step": 2808
    },
    {
      "epoch": 1.402396405391912,
      "grad_norm": 0.6594063639640808,
      "learning_rate": 0.00048597603594608085,
      "loss": 0.0111,
      "step": 2809
    },
    {
      "epoch": 1.402895656515227,
      "grad_norm": 0.0530838817358017,
      "learning_rate": 0.0004859710434348477,
      "loss": 0.0017,
      "step": 2810
    },
    {
      "epoch": 1.4033949076385421,
      "grad_norm": 0.5194961428642273,
      "learning_rate": 0.00048596605092361456,
      "loss": 0.0129,
      "step": 2811
    },
    {
      "epoch": 1.4038941587618572,
      "grad_norm": 0.6578063368797302,
      "learning_rate": 0.0004859610584123814,
      "loss": 0.0235,
      "step": 2812
    },
    {
      "epoch": 1.4043934098851723,
      "grad_norm": 0.22495372593402863,
      "learning_rate": 0.00048595606590114826,
      "loss": 0.0045,
      "step": 2813
    },
    {
      "epoch": 1.4048926610084873,
      "grad_norm": 1.1965701580047607,
      "learning_rate": 0.0004859510733899151,
      "loss": 0.0277,
      "step": 2814
    },
    {
      "epoch": 1.4053919121318024,
      "grad_norm": 1.0223126411437988,
      "learning_rate": 0.00048594608087868197,
      "loss": 0.0163,
      "step": 2815
    },
    {
      "epoch": 1.4058911632551174,
      "grad_norm": 0.060964517295360565,
      "learning_rate": 0.0004859410883674488,
      "loss": 0.0022,
      "step": 2816
    },
    {
      "epoch": 1.4063904143784325,
      "grad_norm": 0.20487399399280548,
      "learning_rate": 0.00048593609585621567,
      "loss": 0.0048,
      "step": 2817
    },
    {
      "epoch": 1.4068896655017473,
      "grad_norm": 1.0247081518173218,
      "learning_rate": 0.0004859311033449825,
      "loss": 0.0317,
      "step": 2818
    },
    {
      "epoch": 1.4073889166250624,
      "grad_norm": 0.8761138916015625,
      "learning_rate": 0.0004859261108337494,
      "loss": 0.0278,
      "step": 2819
    },
    {
      "epoch": 1.4078881677483774,
      "grad_norm": 0.2667318880558014,
      "learning_rate": 0.0004859211183225162,
      "loss": 0.0055,
      "step": 2820
    },
    {
      "epoch": 1.4083874188716925,
      "grad_norm": 0.3879914879798889,
      "learning_rate": 0.0004859161258112831,
      "loss": 0.0082,
      "step": 2821
    },
    {
      "epoch": 1.4088866699950076,
      "grad_norm": 0.43370169401168823,
      "learning_rate": 0.00048591113330004993,
      "loss": 0.0118,
      "step": 2822
    },
    {
      "epoch": 1.4093859211183224,
      "grad_norm": 0.33986467123031616,
      "learning_rate": 0.0004859061407888168,
      "loss": 0.0062,
      "step": 2823
    },
    {
      "epoch": 1.4098851722416375,
      "grad_norm": 1.0800018310546875,
      "learning_rate": 0.00048590114827758363,
      "loss": 0.016,
      "step": 2824
    },
    {
      "epoch": 1.4103844233649525,
      "grad_norm": 0.36114147305488586,
      "learning_rate": 0.0004858961557663505,
      "loss": 0.0175,
      "step": 2825
    },
    {
      "epoch": 1.4108836744882676,
      "grad_norm": 2.715339422225952,
      "learning_rate": 0.00048589116325511734,
      "loss": 0.0137,
      "step": 2826
    },
    {
      "epoch": 1.4113829256115826,
      "grad_norm": 1.162993311882019,
      "learning_rate": 0.0004858861707438842,
      "loss": 0.0242,
      "step": 2827
    },
    {
      "epoch": 1.4118821767348977,
      "grad_norm": 0.7103233933448792,
      "learning_rate": 0.00048588117823265104,
      "loss": 0.0319,
      "step": 2828
    },
    {
      "epoch": 1.4123814278582127,
      "grad_norm": 0.5872461795806885,
      "learning_rate": 0.0004858761857214179,
      "loss": 0.026,
      "step": 2829
    },
    {
      "epoch": 1.4128806789815278,
      "grad_norm": 0.5702923536300659,
      "learning_rate": 0.00048587119321018475,
      "loss": 0.0073,
      "step": 2830
    },
    {
      "epoch": 1.4133799301048429,
      "grad_norm": 1.1778647899627686,
      "learning_rate": 0.0004858662006989516,
      "loss": 0.0081,
      "step": 2831
    },
    {
      "epoch": 1.4138791812281577,
      "grad_norm": 0.7490244507789612,
      "learning_rate": 0.00048586120818771845,
      "loss": 0.0139,
      "step": 2832
    },
    {
      "epoch": 1.4143784323514728,
      "grad_norm": 0.48266056180000305,
      "learning_rate": 0.0004858562156764853,
      "loss": 0.0124,
      "step": 2833
    },
    {
      "epoch": 1.4148776834747878,
      "grad_norm": 0.25257590413093567,
      "learning_rate": 0.00048585122316525215,
      "loss": 0.0083,
      "step": 2834
    },
    {
      "epoch": 1.4153769345981029,
      "grad_norm": 0.22066786885261536,
      "learning_rate": 0.000485846230654019,
      "loss": 0.004,
      "step": 2835
    },
    {
      "epoch": 1.415876185721418,
      "grad_norm": 0.40813523530960083,
      "learning_rate": 0.00048584123814278586,
      "loss": 0.0113,
      "step": 2836
    },
    {
      "epoch": 1.4163754368447328,
      "grad_norm": 1.62530517578125,
      "learning_rate": 0.00048583624563155266,
      "loss": 0.0152,
      "step": 2837
    },
    {
      "epoch": 1.4168746879680478,
      "grad_norm": 0.0960080549120903,
      "learning_rate": 0.0004858312531203195,
      "loss": 0.003,
      "step": 2838
    },
    {
      "epoch": 1.4173739390913629,
      "grad_norm": 0.7969096899032593,
      "learning_rate": 0.00048582626060908636,
      "loss": 0.0092,
      "step": 2839
    },
    {
      "epoch": 1.417873190214678,
      "grad_norm": 2.820694923400879,
      "learning_rate": 0.0004858212680978532,
      "loss": 0.0308,
      "step": 2840
    },
    {
      "epoch": 1.418372441337993,
      "grad_norm": 0.33861812949180603,
      "learning_rate": 0.00048581627558662006,
      "loss": 0.007,
      "step": 2841
    },
    {
      "epoch": 1.418871692461308,
      "grad_norm": 1.162806749343872,
      "learning_rate": 0.0004858112830753869,
      "loss": 0.0099,
      "step": 2842
    },
    {
      "epoch": 1.4193709435846231,
      "grad_norm": 0.42059022188186646,
      "learning_rate": 0.00048580629056415377,
      "loss": 0.0053,
      "step": 2843
    },
    {
      "epoch": 1.4198701947079382,
      "grad_norm": 0.2077445685863495,
      "learning_rate": 0.0004858012980529206,
      "loss": 0.0039,
      "step": 2844
    },
    {
      "epoch": 1.4203694458312532,
      "grad_norm": 0.4738553464412689,
      "learning_rate": 0.0004857963055416875,
      "loss": 0.0101,
      "step": 2845
    },
    {
      "epoch": 1.420868696954568,
      "grad_norm": 0.5049041509628296,
      "learning_rate": 0.0004857913130304543,
      "loss": 0.0039,
      "step": 2846
    },
    {
      "epoch": 1.4213679480778831,
      "grad_norm": 0.8272708058357239,
      "learning_rate": 0.0004857863205192212,
      "loss": 0.0101,
      "step": 2847
    },
    {
      "epoch": 1.4218671992011982,
      "grad_norm": 0.04970207065343857,
      "learning_rate": 0.00048578132800798803,
      "loss": 0.0019,
      "step": 2848
    },
    {
      "epoch": 1.4223664503245133,
      "grad_norm": 0.9401605725288391,
      "learning_rate": 0.0004857763354967549,
      "loss": 0.0201,
      "step": 2849
    },
    {
      "epoch": 1.4228657014478283,
      "grad_norm": 1.6319079399108887,
      "learning_rate": 0.00048577134298552173,
      "loss": 0.0132,
      "step": 2850
    },
    {
      "epoch": 1.4233649525711431,
      "grad_norm": 0.04706592112779617,
      "learning_rate": 0.0004857663504742886,
      "loss": 0.0022,
      "step": 2851
    },
    {
      "epoch": 1.4238642036944582,
      "grad_norm": 0.3799581229686737,
      "learning_rate": 0.00048576135796305544,
      "loss": 0.0098,
      "step": 2852
    },
    {
      "epoch": 1.4243634548177733,
      "grad_norm": 0.604535698890686,
      "learning_rate": 0.0004857563654518223,
      "loss": 0.0064,
      "step": 2853
    },
    {
      "epoch": 1.4248627059410883,
      "grad_norm": 1.5541459321975708,
      "learning_rate": 0.00048575137294058914,
      "loss": 0.0114,
      "step": 2854
    },
    {
      "epoch": 1.4253619570644034,
      "grad_norm": 0.5036667585372925,
      "learning_rate": 0.000485746380429356,
      "loss": 0.0234,
      "step": 2855
    },
    {
      "epoch": 1.4258612081877184,
      "grad_norm": 0.44532573223114014,
      "learning_rate": 0.00048574138791812285,
      "loss": 0.0125,
      "step": 2856
    },
    {
      "epoch": 1.4263604593110335,
      "grad_norm": 0.40009909868240356,
      "learning_rate": 0.0004857363954068897,
      "loss": 0.008,
      "step": 2857
    },
    {
      "epoch": 1.4268597104343486,
      "grad_norm": 0.40242746472358704,
      "learning_rate": 0.00048573140289565655,
      "loss": 0.0056,
      "step": 2858
    },
    {
      "epoch": 1.4273589615576636,
      "grad_norm": 0.8144528865814209,
      "learning_rate": 0.0004857264103844234,
      "loss": 0.0192,
      "step": 2859
    },
    {
      "epoch": 1.4278582126809785,
      "grad_norm": 0.43264904618263245,
      "learning_rate": 0.00048572141787319025,
      "loss": 0.008,
      "step": 2860
    },
    {
      "epoch": 1.4283574638042935,
      "grad_norm": 0.5228936672210693,
      "learning_rate": 0.0004857164253619571,
      "loss": 0.0055,
      "step": 2861
    },
    {
      "epoch": 1.4288567149276086,
      "grad_norm": 0.6071588397026062,
      "learning_rate": 0.00048571143285072396,
      "loss": 0.0172,
      "step": 2862
    },
    {
      "epoch": 1.4293559660509236,
      "grad_norm": 0.11645721644163132,
      "learning_rate": 0.0004857064403394908,
      "loss": 0.0032,
      "step": 2863
    },
    {
      "epoch": 1.4298552171742387,
      "grad_norm": 0.41551151871681213,
      "learning_rate": 0.00048570144782825766,
      "loss": 0.0063,
      "step": 2864
    },
    {
      "epoch": 1.4303544682975537,
      "grad_norm": 0.049371156841516495,
      "learning_rate": 0.0004856964553170245,
      "loss": 0.0021,
      "step": 2865
    },
    {
      "epoch": 1.4308537194208686,
      "grad_norm": 0.6896209120750427,
      "learning_rate": 0.00048569146280579137,
      "loss": 0.0179,
      "step": 2866
    },
    {
      "epoch": 1.4313529705441836,
      "grad_norm": 0.30851051211357117,
      "learning_rate": 0.00048568647029455816,
      "loss": 0.0065,
      "step": 2867
    },
    {
      "epoch": 1.4318522216674987,
      "grad_norm": 0.05597208812832832,
      "learning_rate": 0.000485681477783325,
      "loss": 0.0016,
      "step": 2868
    },
    {
      "epoch": 1.4323514727908138,
      "grad_norm": 0.200466588139534,
      "learning_rate": 0.00048567648527209187,
      "loss": 0.0045,
      "step": 2869
    },
    {
      "epoch": 1.4328507239141288,
      "grad_norm": 0.5639690160751343,
      "learning_rate": 0.0004856714927608587,
      "loss": 0.01,
      "step": 2870
    },
    {
      "epoch": 1.4333499750374439,
      "grad_norm": 0.3789111375808716,
      "learning_rate": 0.00048566650024962557,
      "loss": 0.0121,
      "step": 2871
    },
    {
      "epoch": 1.433849226160759,
      "grad_norm": 0.05143502727150917,
      "learning_rate": 0.0004856615077383924,
      "loss": 0.0021,
      "step": 2872
    },
    {
      "epoch": 1.434348477284074,
      "grad_norm": 0.15154922008514404,
      "learning_rate": 0.0004856565152271593,
      "loss": 0.0029,
      "step": 2873
    },
    {
      "epoch": 1.4348477284073888,
      "grad_norm": 0.18442563712596893,
      "learning_rate": 0.00048565152271592613,
      "loss": 0.0047,
      "step": 2874
    },
    {
      "epoch": 1.435346979530704,
      "grad_norm": 0.050818149000406265,
      "learning_rate": 0.000485646530204693,
      "loss": 0.0021,
      "step": 2875
    },
    {
      "epoch": 1.435846230654019,
      "grad_norm": 0.14408110082149506,
      "learning_rate": 0.00048564153769345983,
      "loss": 0.0036,
      "step": 2876
    },
    {
      "epoch": 1.436345481777334,
      "grad_norm": 0.24604199826717377,
      "learning_rate": 0.0004856365451822267,
      "loss": 0.0072,
      "step": 2877
    },
    {
      "epoch": 1.436844732900649,
      "grad_norm": 0.2372368723154068,
      "learning_rate": 0.00048563155267099354,
      "loss": 0.0048,
      "step": 2878
    },
    {
      "epoch": 1.4373439840239641,
      "grad_norm": 0.24267186224460602,
      "learning_rate": 0.0004856265601597604,
      "loss": 0.0088,
      "step": 2879
    },
    {
      "epoch": 1.437843235147279,
      "grad_norm": 0.048464275896549225,
      "learning_rate": 0.00048562156764852724,
      "loss": 0.0018,
      "step": 2880
    },
    {
      "epoch": 1.438342486270594,
      "grad_norm": 0.14279544353485107,
      "learning_rate": 0.0004856165751372941,
      "loss": 0.0375,
      "step": 2881
    },
    {
      "epoch": 1.438841737393909,
      "grad_norm": 0.24278715252876282,
      "learning_rate": 0.00048561158262606095,
      "loss": 0.0049,
      "step": 2882
    },
    {
      "epoch": 1.4393409885172241,
      "grad_norm": 0.27227652072906494,
      "learning_rate": 0.0004856065901148278,
      "loss": 0.0066,
      "step": 2883
    },
    {
      "epoch": 1.4398402396405392,
      "grad_norm": 0.43732523918151855,
      "learning_rate": 0.00048560159760359465,
      "loss": 0.0179,
      "step": 2884
    },
    {
      "epoch": 1.4403394907638543,
      "grad_norm": 0.12382082641124725,
      "learning_rate": 0.0004855966050923615,
      "loss": 0.0032,
      "step": 2885
    },
    {
      "epoch": 1.4408387418871693,
      "grad_norm": 0.13766750693321228,
      "learning_rate": 0.00048559161258112835,
      "loss": 0.0028,
      "step": 2886
    },
    {
      "epoch": 1.4413379930104844,
      "grad_norm": 0.0581028051674366,
      "learning_rate": 0.0004855866200698952,
      "loss": 0.0015,
      "step": 2887
    },
    {
      "epoch": 1.4418372441337994,
      "grad_norm": 0.1939217895269394,
      "learning_rate": 0.00048558162755866206,
      "loss": 0.0037,
      "step": 2888
    },
    {
      "epoch": 1.4423364952571143,
      "grad_norm": 0.0453689806163311,
      "learning_rate": 0.0004855766350474289,
      "loss": 0.0015,
      "step": 2889
    },
    {
      "epoch": 1.4428357463804293,
      "grad_norm": 0.013846452347934246,
      "learning_rate": 0.0004855716425361957,
      "loss": 0.001,
      "step": 2890
    },
    {
      "epoch": 1.4433349975037444,
      "grad_norm": 0.07278158515691757,
      "learning_rate": 0.00048556665002496256,
      "loss": 0.0016,
      "step": 2891
    },
    {
      "epoch": 1.4438342486270594,
      "grad_norm": 0.04819553717970848,
      "learning_rate": 0.0004855616575137294,
      "loss": 0.0019,
      "step": 2892
    },
    {
      "epoch": 1.4443334997503745,
      "grad_norm": 0.6874504089355469,
      "learning_rate": 0.00048555666500249626,
      "loss": 0.0055,
      "step": 2893
    },
    {
      "epoch": 1.4448327508736893,
      "grad_norm": 0.028801728039979935,
      "learning_rate": 0.0004855516724912631,
      "loss": 0.0013,
      "step": 2894
    },
    {
      "epoch": 1.4453320019970044,
      "grad_norm": 0.08199701458215714,
      "learning_rate": 0.00048554667998002997,
      "loss": 0.0019,
      "step": 2895
    },
    {
      "epoch": 1.4458312531203195,
      "grad_norm": 0.5463229417800903,
      "learning_rate": 0.00048554168746879677,
      "loss": 0.0142,
      "step": 2896
    },
    {
      "epoch": 1.4463305042436345,
      "grad_norm": 0.1785830706357956,
      "learning_rate": 0.0004855366949575636,
      "loss": 0.0023,
      "step": 2897
    },
    {
      "epoch": 1.4468297553669496,
      "grad_norm": 0.18653811514377594,
      "learning_rate": 0.00048553170244633047,
      "loss": 0.0042,
      "step": 2898
    },
    {
      "epoch": 1.4473290064902646,
      "grad_norm": 0.05728980153799057,
      "learning_rate": 0.0004855267099350973,
      "loss": 0.0014,
      "step": 2899
    },
    {
      "epoch": 1.4478282576135797,
      "grad_norm": 0.36059442162513733,
      "learning_rate": 0.0004855217174238642,
      "loss": 0.0063,
      "step": 2900
    },
    {
      "epoch": 1.4483275087368948,
      "grad_norm": 0.4351281225681305,
      "learning_rate": 0.000485516724912631,
      "loss": 0.007,
      "step": 2901
    },
    {
      "epoch": 1.4488267598602098,
      "grad_norm": 0.019588114693760872,
      "learning_rate": 0.0004855117324013979,
      "loss": 0.001,
      "step": 2902
    },
    {
      "epoch": 1.4493260109835246,
      "grad_norm": 0.031722865998744965,
      "learning_rate": 0.00048550673989016473,
      "loss": 0.0012,
      "step": 2903
    },
    {
      "epoch": 1.4498252621068397,
      "grad_norm": 0.08386301249265671,
      "learning_rate": 0.0004855017473789316,
      "loss": 0.0019,
      "step": 2904
    },
    {
      "epoch": 1.4503245132301548,
      "grad_norm": 0.015322493389248848,
      "learning_rate": 0.00048549675486769843,
      "loss": 0.0008,
      "step": 2905
    },
    {
      "epoch": 1.4508237643534698,
      "grad_norm": 0.2820989787578583,
      "learning_rate": 0.0004854917623564653,
      "loss": 0.0073,
      "step": 2906
    },
    {
      "epoch": 1.4513230154767849,
      "grad_norm": 0.6037318110466003,
      "learning_rate": 0.00048548676984523214,
      "loss": 0.0258,
      "step": 2907
    },
    {
      "epoch": 1.4518222666000997,
      "grad_norm": 0.14072944223880768,
      "learning_rate": 0.000485481777333999,
      "loss": 0.0031,
      "step": 2908
    },
    {
      "epoch": 1.4523215177234148,
      "grad_norm": 0.07403627038002014,
      "learning_rate": 0.00048547678482276584,
      "loss": 0.0021,
      "step": 2909
    },
    {
      "epoch": 1.4528207688467298,
      "grad_norm": 0.5921067595481873,
      "learning_rate": 0.0004854717923115327,
      "loss": 0.0125,
      "step": 2910
    },
    {
      "epoch": 1.453320019970045,
      "grad_norm": 0.1418863981962204,
      "learning_rate": 0.00048546679980029955,
      "loss": 0.0032,
      "step": 2911
    },
    {
      "epoch": 1.45381927109336,
      "grad_norm": 3.152388572692871,
      "learning_rate": 0.0004854618072890664,
      "loss": 0.0111,
      "step": 2912
    },
    {
      "epoch": 1.454318522216675,
      "grad_norm": 0.5240069031715393,
      "learning_rate": 0.00048545681477783325,
      "loss": 0.0088,
      "step": 2913
    },
    {
      "epoch": 1.45481777333999,
      "grad_norm": 0.6372325420379639,
      "learning_rate": 0.0004854518222666001,
      "loss": 0.0164,
      "step": 2914
    },
    {
      "epoch": 1.4553170244633051,
      "grad_norm": 0.048726774752140045,
      "learning_rate": 0.00048544682975536696,
      "loss": 0.0014,
      "step": 2915
    },
    {
      "epoch": 1.4558162755866202,
      "grad_norm": 0.22529436647891998,
      "learning_rate": 0.0004854418372441338,
      "loss": 0.0044,
      "step": 2916
    },
    {
      "epoch": 1.456315526709935,
      "grad_norm": 0.12175646424293518,
      "learning_rate": 0.00048543684473290066,
      "loss": 0.0025,
      "step": 2917
    },
    {
      "epoch": 1.45681477783325,
      "grad_norm": 0.36168956756591797,
      "learning_rate": 0.0004854318522216675,
      "loss": 0.0213,
      "step": 2918
    },
    {
      "epoch": 1.4573140289565651,
      "grad_norm": 2.833998441696167,
      "learning_rate": 0.00048542685971043436,
      "loss": 0.0132,
      "step": 2919
    },
    {
      "epoch": 1.4578132800798802,
      "grad_norm": 0.12023214995861053,
      "learning_rate": 0.0004854218671992012,
      "loss": 0.0035,
      "step": 2920
    },
    {
      "epoch": 1.4583125312031953,
      "grad_norm": 0.05939868092536926,
      "learning_rate": 0.00048541687468796807,
      "loss": 0.0021,
      "step": 2921
    },
    {
      "epoch": 1.45881178232651,
      "grad_norm": 0.24333471059799194,
      "learning_rate": 0.0004854118821767349,
      "loss": 0.0072,
      "step": 2922
    },
    {
      "epoch": 1.4593110334498252,
      "grad_norm": 1.367100477218628,
      "learning_rate": 0.00048540688966550177,
      "loss": 0.0041,
      "step": 2923
    },
    {
      "epoch": 1.4598102845731402,
      "grad_norm": 0.13246828317642212,
      "learning_rate": 0.0004854018971542686,
      "loss": 0.0036,
      "step": 2924
    },
    {
      "epoch": 1.4603095356964553,
      "grad_norm": 0.4545402526855469,
      "learning_rate": 0.0004853969046430354,
      "loss": 0.0112,
      "step": 2925
    },
    {
      "epoch": 1.4608087868197703,
      "grad_norm": 0.08318812400102615,
      "learning_rate": 0.0004853919121318023,
      "loss": 0.0018,
      "step": 2926
    },
    {
      "epoch": 1.4613080379430854,
      "grad_norm": 0.4848935008049011,
      "learning_rate": 0.0004853869196205691,
      "loss": 0.0177,
      "step": 2927
    },
    {
      "epoch": 1.4618072890664005,
      "grad_norm": 0.29785609245300293,
      "learning_rate": 0.000485381927109336,
      "loss": 0.0055,
      "step": 2928
    },
    {
      "epoch": 1.4623065401897155,
      "grad_norm": 0.02246996760368347,
      "learning_rate": 0.00048537693459810283,
      "loss": 0.0012,
      "step": 2929
    },
    {
      "epoch": 1.4628057913130306,
      "grad_norm": 0.18416155874729156,
      "learning_rate": 0.0004853719420868697,
      "loss": 0.0033,
      "step": 2930
    },
    {
      "epoch": 1.4633050424363454,
      "grad_norm": 0.20608727633953094,
      "learning_rate": 0.00048536694957563653,
      "loss": 0.003,
      "step": 2931
    },
    {
      "epoch": 1.4638042935596605,
      "grad_norm": 0.026777472347021103,
      "learning_rate": 0.0004853619570644034,
      "loss": 0.0011,
      "step": 2932
    },
    {
      "epoch": 1.4643035446829755,
      "grad_norm": 0.2813190519809723,
      "learning_rate": 0.00048535696455317024,
      "loss": 0.007,
      "step": 2933
    },
    {
      "epoch": 1.4648027958062906,
      "grad_norm": 1.184311866760254,
      "learning_rate": 0.0004853519720419371,
      "loss": 0.0115,
      "step": 2934
    },
    {
      "epoch": 1.4653020469296056,
      "grad_norm": 0.43661007285118103,
      "learning_rate": 0.00048534697953070394,
      "loss": 0.0226,
      "step": 2935
    },
    {
      "epoch": 1.4658012980529207,
      "grad_norm": 1.1983174085617065,
      "learning_rate": 0.0004853419870194708,
      "loss": 0.007,
      "step": 2936
    },
    {
      "epoch": 1.4663005491762355,
      "grad_norm": 0.24819941818714142,
      "learning_rate": 0.00048533699450823765,
      "loss": 0.0031,
      "step": 2937
    },
    {
      "epoch": 1.4667998002995506,
      "grad_norm": 0.11220750957727432,
      "learning_rate": 0.0004853320019970045,
      "loss": 0.0022,
      "step": 2938
    },
    {
      "epoch": 1.4672990514228657,
      "grad_norm": 0.9112797975540161,
      "learning_rate": 0.00048532700948577135,
      "loss": 0.0113,
      "step": 2939
    },
    {
      "epoch": 1.4677983025461807,
      "grad_norm": 0.5968531370162964,
      "learning_rate": 0.0004853220169745382,
      "loss": 0.0134,
      "step": 2940
    },
    {
      "epoch": 1.4682975536694958,
      "grad_norm": 0.6163100600242615,
      "learning_rate": 0.00048531702446330505,
      "loss": 0.0075,
      "step": 2941
    },
    {
      "epoch": 1.4687968047928108,
      "grad_norm": 0.8766379952430725,
      "learning_rate": 0.0004853120319520719,
      "loss": 0.0071,
      "step": 2942
    },
    {
      "epoch": 1.4692960559161259,
      "grad_norm": 0.36700010299682617,
      "learning_rate": 0.00048530703944083876,
      "loss": 0.0133,
      "step": 2943
    },
    {
      "epoch": 1.469795307039441,
      "grad_norm": 0.1361786425113678,
      "learning_rate": 0.0004853020469296056,
      "loss": 0.0035,
      "step": 2944
    },
    {
      "epoch": 1.470294558162756,
      "grad_norm": 0.9512583613395691,
      "learning_rate": 0.00048529705441837246,
      "loss": 0.0125,
      "step": 2945
    },
    {
      "epoch": 1.4707938092860708,
      "grad_norm": 0.5309964418411255,
      "learning_rate": 0.0004852920619071393,
      "loss": 0.035,
      "step": 2946
    },
    {
      "epoch": 1.471293060409386,
      "grad_norm": 0.26258671283721924,
      "learning_rate": 0.00048528706939590617,
      "loss": 0.0057,
      "step": 2947
    },
    {
      "epoch": 1.471792311532701,
      "grad_norm": 0.5464454293251038,
      "learning_rate": 0.000485282076884673,
      "loss": 0.0111,
      "step": 2948
    },
    {
      "epoch": 1.472291562656016,
      "grad_norm": 0.19008496403694153,
      "learning_rate": 0.00048527708437343987,
      "loss": 0.0049,
      "step": 2949
    },
    {
      "epoch": 1.472790813779331,
      "grad_norm": 0.3336692750453949,
      "learning_rate": 0.0004852720918622067,
      "loss": 0.0107,
      "step": 2950
    },
    {
      "epoch": 1.473290064902646,
      "grad_norm": 0.18964485824108124,
      "learning_rate": 0.0004852670993509736,
      "loss": 0.0034,
      "step": 2951
    },
    {
      "epoch": 1.473789316025961,
      "grad_norm": 0.42796191573143005,
      "learning_rate": 0.00048526210683974043,
      "loss": 0.0083,
      "step": 2952
    },
    {
      "epoch": 1.474288567149276,
      "grad_norm": 0.04174056276679039,
      "learning_rate": 0.0004852571143285073,
      "loss": 0.0016,
      "step": 2953
    },
    {
      "epoch": 1.474787818272591,
      "grad_norm": 0.1663903146982193,
      "learning_rate": 0.0004852521218172741,
      "loss": 0.004,
      "step": 2954
    },
    {
      "epoch": 1.4752870693959061,
      "grad_norm": 0.15839695930480957,
      "learning_rate": 0.00048524712930604093,
      "loss": 0.0036,
      "step": 2955
    },
    {
      "epoch": 1.4757863205192212,
      "grad_norm": 0.20725779235363007,
      "learning_rate": 0.0004852421367948078,
      "loss": 0.0037,
      "step": 2956
    },
    {
      "epoch": 1.4762855716425363,
      "grad_norm": 0.3911895453929901,
      "learning_rate": 0.00048523714428357463,
      "loss": 0.0108,
      "step": 2957
    },
    {
      "epoch": 1.4767848227658513,
      "grad_norm": 0.9194213151931763,
      "learning_rate": 0.0004852321517723415,
      "loss": 0.051,
      "step": 2958
    },
    {
      "epoch": 1.4772840738891664,
      "grad_norm": 0.2647134065628052,
      "learning_rate": 0.00048522715926110834,
      "loss": 0.0055,
      "step": 2959
    },
    {
      "epoch": 1.4777833250124812,
      "grad_norm": 0.574737548828125,
      "learning_rate": 0.0004852221667498752,
      "loss": 0.0169,
      "step": 2960
    },
    {
      "epoch": 1.4782825761357963,
      "grad_norm": 0.19980905950069427,
      "learning_rate": 0.00048521717423864204,
      "loss": 0.0061,
      "step": 2961
    },
    {
      "epoch": 1.4787818272591113,
      "grad_norm": 0.3229709565639496,
      "learning_rate": 0.0004852121817274089,
      "loss": 0.0041,
      "step": 2962
    },
    {
      "epoch": 1.4792810783824264,
      "grad_norm": 0.05457499623298645,
      "learning_rate": 0.00048520718921617575,
      "loss": 0.0021,
      "step": 2963
    },
    {
      "epoch": 1.4797803295057415,
      "grad_norm": 0.09069270640611649,
      "learning_rate": 0.0004852021967049426,
      "loss": 0.0026,
      "step": 2964
    },
    {
      "epoch": 1.4802795806290563,
      "grad_norm": 0.0984083041548729,
      "learning_rate": 0.00048519720419370945,
      "loss": 0.0025,
      "step": 2965
    },
    {
      "epoch": 1.4807788317523713,
      "grad_norm": 0.12402647733688354,
      "learning_rate": 0.0004851922116824763,
      "loss": 0.0041,
      "step": 2966
    },
    {
      "epoch": 1.4812780828756864,
      "grad_norm": 0.4341820776462555,
      "learning_rate": 0.00048518721917124315,
      "loss": 0.0066,
      "step": 2967
    },
    {
      "epoch": 1.4817773339990015,
      "grad_norm": 0.12396406382322311,
      "learning_rate": 0.00048518222666001,
      "loss": 0.0025,
      "step": 2968
    },
    {
      "epoch": 1.4822765851223165,
      "grad_norm": 0.0909370705485344,
      "learning_rate": 0.00048517723414877686,
      "loss": 0.0023,
      "step": 2969
    },
    {
      "epoch": 1.4827758362456316,
      "grad_norm": 0.2715446650981903,
      "learning_rate": 0.0004851722416375437,
      "loss": 0.004,
      "step": 2970
    },
    {
      "epoch": 1.4832750873689466,
      "grad_norm": 0.1451701670885086,
      "learning_rate": 0.00048516724912631056,
      "loss": 0.0035,
      "step": 2971
    },
    {
      "epoch": 1.4837743384922617,
      "grad_norm": 0.5486969947814941,
      "learning_rate": 0.0004851622566150774,
      "loss": 0.0076,
      "step": 2972
    },
    {
      "epoch": 1.4842735896155768,
      "grad_norm": 0.0965503677725792,
      "learning_rate": 0.00048515726410384427,
      "loss": 0.0025,
      "step": 2973
    },
    {
      "epoch": 1.4847728407388916,
      "grad_norm": 0.07915959507226944,
      "learning_rate": 0.0004851522715926111,
      "loss": 0.0018,
      "step": 2974
    },
    {
      "epoch": 1.4852720918622067,
      "grad_norm": 0.05642109736800194,
      "learning_rate": 0.00048514727908137797,
      "loss": 0.0017,
      "step": 2975
    },
    {
      "epoch": 1.4857713429855217,
      "grad_norm": 0.38456088304519653,
      "learning_rate": 0.0004851422865701448,
      "loss": 0.0098,
      "step": 2976
    },
    {
      "epoch": 1.4862705941088368,
      "grad_norm": 0.4825532138347626,
      "learning_rate": 0.0004851372940589117,
      "loss": 0.008,
      "step": 2977
    },
    {
      "epoch": 1.4867698452321518,
      "grad_norm": 0.26411867141723633,
      "learning_rate": 0.0004851323015476785,
      "loss": 0.0029,
      "step": 2978
    },
    {
      "epoch": 1.4872690963554667,
      "grad_norm": 0.3210064470767975,
      "learning_rate": 0.0004851273090364454,
      "loss": 0.0055,
      "step": 2979
    },
    {
      "epoch": 1.4877683474787817,
      "grad_norm": 0.17926424741744995,
      "learning_rate": 0.00048512231652521223,
      "loss": 0.0024,
      "step": 2980
    },
    {
      "epoch": 1.4882675986020968,
      "grad_norm": 0.48747947812080383,
      "learning_rate": 0.0004851173240139791,
      "loss": 0.0069,
      "step": 2981
    },
    {
      "epoch": 1.4887668497254118,
      "grad_norm": 0.5571371912956238,
      "learning_rate": 0.00048511233150274594,
      "loss": 0.0051,
      "step": 2982
    },
    {
      "epoch": 1.489266100848727,
      "grad_norm": 0.5016904473304749,
      "learning_rate": 0.00048510733899151273,
      "loss": 0.0103,
      "step": 2983
    },
    {
      "epoch": 1.489765351972042,
      "grad_norm": 0.044634945690631866,
      "learning_rate": 0.0004851023464802796,
      "loss": 0.0015,
      "step": 2984
    },
    {
      "epoch": 1.490264603095357,
      "grad_norm": 0.35746172070503235,
      "learning_rate": 0.00048509735396904644,
      "loss": 0.0061,
      "step": 2985
    },
    {
      "epoch": 1.490763854218672,
      "grad_norm": 0.4543982148170471,
      "learning_rate": 0.0004850923614578133,
      "loss": 0.0133,
      "step": 2986
    },
    {
      "epoch": 1.4912631053419871,
      "grad_norm": 0.12143050879240036,
      "learning_rate": 0.00048508736894658014,
      "loss": 0.0027,
      "step": 2987
    },
    {
      "epoch": 1.491762356465302,
      "grad_norm": 0.39107537269592285,
      "learning_rate": 0.000485082376435347,
      "loss": 0.0068,
      "step": 2988
    },
    {
      "epoch": 1.492261607588617,
      "grad_norm": 0.12617720663547516,
      "learning_rate": 0.00048507738392411385,
      "loss": 0.0017,
      "step": 2989
    },
    {
      "epoch": 1.492760858711932,
      "grad_norm": 0.1845654398202896,
      "learning_rate": 0.0004850723914128807,
      "loss": 0.003,
      "step": 2990
    },
    {
      "epoch": 1.4932601098352472,
      "grad_norm": 0.21924804151058197,
      "learning_rate": 0.00048506739890164755,
      "loss": 0.0093,
      "step": 2991
    },
    {
      "epoch": 1.4937593609585622,
      "grad_norm": 0.04967522248625755,
      "learning_rate": 0.0004850624063904144,
      "loss": 0.0012,
      "step": 2992
    },
    {
      "epoch": 1.494258612081877,
      "grad_norm": 0.05125974491238594,
      "learning_rate": 0.00048505741387918125,
      "loss": 0.0013,
      "step": 2993
    },
    {
      "epoch": 1.494757863205192,
      "grad_norm": 0.6262826919555664,
      "learning_rate": 0.0004850524213679481,
      "loss": 0.0076,
      "step": 2994
    },
    {
      "epoch": 1.4952571143285072,
      "grad_norm": 0.8528044819831848,
      "learning_rate": 0.00048504742885671496,
      "loss": 0.0245,
      "step": 2995
    },
    {
      "epoch": 1.4957563654518222,
      "grad_norm": 0.050725970417261124,
      "learning_rate": 0.0004850424363454818,
      "loss": 0.0018,
      "step": 2996
    },
    {
      "epoch": 1.4962556165751373,
      "grad_norm": 0.17270636558532715,
      "learning_rate": 0.00048503744383424866,
      "loss": 0.0027,
      "step": 2997
    },
    {
      "epoch": 1.4967548676984523,
      "grad_norm": 0.7110828757286072,
      "learning_rate": 0.0004850324513230155,
      "loss": 0.0136,
      "step": 2998
    },
    {
      "epoch": 1.4972541188217674,
      "grad_norm": 0.31321609020233154,
      "learning_rate": 0.00048502745881178237,
      "loss": 0.0089,
      "step": 2999
    },
    {
      "epoch": 1.4977533699450825,
      "grad_norm": 0.293381929397583,
      "learning_rate": 0.0004850224663005492,
      "loss": 0.0036,
      "step": 3000
    },
    {
      "epoch": 1.4982526210683975,
      "grad_norm": 0.03911711648106575,
      "learning_rate": 0.00048501747378931607,
      "loss": 0.0013,
      "step": 3001
    },
    {
      "epoch": 1.4987518721917124,
      "grad_norm": 0.4694178104400635,
      "learning_rate": 0.0004850124812780829,
      "loss": 0.0115,
      "step": 3002
    },
    {
      "epoch": 1.4992511233150274,
      "grad_norm": 0.4326001703739166,
      "learning_rate": 0.0004850074887668497,
      "loss": 0.0138,
      "step": 3003
    },
    {
      "epoch": 1.4997503744383425,
      "grad_norm": 0.4124661982059479,
      "learning_rate": 0.00048500249625561657,
      "loss": 0.0288,
      "step": 3004
    },
    {
      "epoch": 1.5002496255616575,
      "grad_norm": 0.0626487210392952,
      "learning_rate": 0.0004849975037443834,
      "loss": 0.0016,
      "step": 3005
    },
    {
      "epoch": 1.5007488766849726,
      "grad_norm": 0.19548353552818298,
      "learning_rate": 0.0004849925112331503,
      "loss": 0.0039,
      "step": 3006
    },
    {
      "epoch": 1.5012481278082874,
      "grad_norm": 0.22798244655132294,
      "learning_rate": 0.00048498751872191713,
      "loss": 0.0038,
      "step": 3007
    },
    {
      "epoch": 1.5017473789316025,
      "grad_norm": 0.6985301375389099,
      "learning_rate": 0.000484982526210684,
      "loss": 0.006,
      "step": 3008
    },
    {
      "epoch": 1.5022466300549175,
      "grad_norm": 0.29411765933036804,
      "learning_rate": 0.00048497753369945083,
      "loss": 0.0061,
      "step": 3009
    },
    {
      "epoch": 1.5027458811782326,
      "grad_norm": 0.23750969767570496,
      "learning_rate": 0.0004849725411882177,
      "loss": 0.0061,
      "step": 3010
    },
    {
      "epoch": 1.5032451323015477,
      "grad_norm": 0.5074988007545471,
      "learning_rate": 0.00048496754867698454,
      "loss": 0.0071,
      "step": 3011
    },
    {
      "epoch": 1.5037443834248627,
      "grad_norm": 0.15342187881469727,
      "learning_rate": 0.00048496255616575133,
      "loss": 0.0037,
      "step": 3012
    },
    {
      "epoch": 1.5042436345481778,
      "grad_norm": 0.033374618738889694,
      "learning_rate": 0.0004849575636545182,
      "loss": 0.0017,
      "step": 3013
    },
    {
      "epoch": 1.5047428856714928,
      "grad_norm": 0.0827919989824295,
      "learning_rate": 0.00048495257114328504,
      "loss": 0.0024,
      "step": 3014
    },
    {
      "epoch": 1.505242136794808,
      "grad_norm": 0.24499304592609406,
      "learning_rate": 0.0004849475786320519,
      "loss": 0.0038,
      "step": 3015
    },
    {
      "epoch": 1.505741387918123,
      "grad_norm": 0.17104460299015045,
      "learning_rate": 0.00048494258612081874,
      "loss": 0.004,
      "step": 3016
    },
    {
      "epoch": 1.5062406390414378,
      "grad_norm": 0.06294286996126175,
      "learning_rate": 0.0004849375936095856,
      "loss": 0.002,
      "step": 3017
    },
    {
      "epoch": 1.5067398901647528,
      "grad_norm": 0.25914183259010315,
      "learning_rate": 0.00048493260109835245,
      "loss": 0.003,
      "step": 3018
    },
    {
      "epoch": 1.507239141288068,
      "grad_norm": 0.19178269803524017,
      "learning_rate": 0.0004849276085871193,
      "loss": 0.0031,
      "step": 3019
    },
    {
      "epoch": 1.507738392411383,
      "grad_norm": 0.25000840425491333,
      "learning_rate": 0.00048492261607588615,
      "loss": 0.0058,
      "step": 3020
    },
    {
      "epoch": 1.5082376435346978,
      "grad_norm": 0.19357788562774658,
      "learning_rate": 0.000484917623564653,
      "loss": 0.002,
      "step": 3021
    },
    {
      "epoch": 1.5087368946580129,
      "grad_norm": 0.18660463392734528,
      "learning_rate": 0.00048491263105341986,
      "loss": 0.0024,
      "step": 3022
    },
    {
      "epoch": 1.509236145781328,
      "grad_norm": 0.2534445524215698,
      "learning_rate": 0.0004849076385421867,
      "loss": 0.0045,
      "step": 3023
    },
    {
      "epoch": 1.509735396904643,
      "grad_norm": 0.08242201805114746,
      "learning_rate": 0.00048490264603095356,
      "loss": 0.0018,
      "step": 3024
    },
    {
      "epoch": 1.510234648027958,
      "grad_norm": 0.24008002877235413,
      "learning_rate": 0.0004848976535197204,
      "loss": 0.0105,
      "step": 3025
    },
    {
      "epoch": 1.510733899151273,
      "grad_norm": 0.2364865094423294,
      "learning_rate": 0.00048489266100848726,
      "loss": 0.0061,
      "step": 3026
    },
    {
      "epoch": 1.5112331502745882,
      "grad_norm": 0.24019642174243927,
      "learning_rate": 0.0004848876684972541,
      "loss": 0.004,
      "step": 3027
    },
    {
      "epoch": 1.5117324013979032,
      "grad_norm": 0.12920792400836945,
      "learning_rate": 0.00048488267598602097,
      "loss": 0.0017,
      "step": 3028
    },
    {
      "epoch": 1.5122316525212183,
      "grad_norm": 0.060666829347610474,
      "learning_rate": 0.0004848776834747878,
      "loss": 0.0014,
      "step": 3029
    },
    {
      "epoch": 1.5127309036445333,
      "grad_norm": 0.09848256409168243,
      "learning_rate": 0.00048487269096355467,
      "loss": 0.0016,
      "step": 3030
    },
    {
      "epoch": 1.5132301547678484,
      "grad_norm": 0.03469453379511833,
      "learning_rate": 0.0004848676984523215,
      "loss": 0.0012,
      "step": 3031
    },
    {
      "epoch": 1.5137294058911632,
      "grad_norm": 0.10222595185041428,
      "learning_rate": 0.0004848627059410884,
      "loss": 0.0014,
      "step": 3032
    },
    {
      "epoch": 1.5142286570144783,
      "grad_norm": 0.14414066076278687,
      "learning_rate": 0.00048485771342985523,
      "loss": 0.0016,
      "step": 3033
    },
    {
      "epoch": 1.5147279081377933,
      "grad_norm": 0.09170113503932953,
      "learning_rate": 0.0004848527209186221,
      "loss": 0.0025,
      "step": 3034
    },
    {
      "epoch": 1.5152271592611082,
      "grad_norm": 0.045922551304101944,
      "learning_rate": 0.00048484772840738893,
      "loss": 0.001,
      "step": 3035
    },
    {
      "epoch": 1.5157264103844232,
      "grad_norm": 0.19223223626613617,
      "learning_rate": 0.0004848427358961558,
      "loss": 0.0029,
      "step": 3036
    },
    {
      "epoch": 1.5162256615077383,
      "grad_norm": 0.051372118294239044,
      "learning_rate": 0.00048483774338492264,
      "loss": 0.0011,
      "step": 3037
    },
    {
      "epoch": 1.5167249126310534,
      "grad_norm": 0.013717788271605968,
      "learning_rate": 0.0004848327508736895,
      "loss": 0.0006,
      "step": 3038
    },
    {
      "epoch": 1.5172241637543684,
      "grad_norm": 0.10092608630657196,
      "learning_rate": 0.00048482775836245634,
      "loss": 0.0025,
      "step": 3039
    },
    {
      "epoch": 1.5177234148776835,
      "grad_norm": 0.1724635660648346,
      "learning_rate": 0.0004848227658512232,
      "loss": 0.0027,
      "step": 3040
    },
    {
      "epoch": 1.5182226660009985,
      "grad_norm": 0.45229193568229675,
      "learning_rate": 0.00048481777333999,
      "loss": 0.0094,
      "step": 3041
    },
    {
      "epoch": 1.5187219171243136,
      "grad_norm": 0.03170297294855118,
      "learning_rate": 0.00048481278082875684,
      "loss": 0.001,
      "step": 3042
    },
    {
      "epoch": 1.5192211682476287,
      "grad_norm": 0.0058517977595329285,
      "learning_rate": 0.0004848077883175237,
      "loss": 0.0005,
      "step": 3043
    },
    {
      "epoch": 1.5197204193709437,
      "grad_norm": 0.007054995279759169,
      "learning_rate": 0.00048480279580629055,
      "loss": 0.0006,
      "step": 3044
    },
    {
      "epoch": 1.5202196704942588,
      "grad_norm": 0.20889918506145477,
      "learning_rate": 0.0004847978032950574,
      "loss": 0.0026,
      "step": 3045
    },
    {
      "epoch": 1.5207189216175736,
      "grad_norm": 0.02150713838636875,
      "learning_rate": 0.00048479281078382425,
      "loss": 0.0007,
      "step": 3046
    },
    {
      "epoch": 1.5212181727408887,
      "grad_norm": 0.015623833984136581,
      "learning_rate": 0.0004847878182725911,
      "loss": 0.0007,
      "step": 3047
    },
    {
      "epoch": 1.5217174238642037,
      "grad_norm": 0.9577504396438599,
      "learning_rate": 0.00048478282576135795,
      "loss": 0.0065,
      "step": 3048
    },
    {
      "epoch": 1.5222166749875186,
      "grad_norm": 0.11712690442800522,
      "learning_rate": 0.0004847778332501248,
      "loss": 0.0014,
      "step": 3049
    },
    {
      "epoch": 1.5227159261108336,
      "grad_norm": 0.1779811829328537,
      "learning_rate": 0.00048477284073889166,
      "loss": 0.0012,
      "step": 3050
    },
    {
      "epoch": 1.5232151772341487,
      "grad_norm": 0.00705816550180316,
      "learning_rate": 0.0004847678482276585,
      "loss": 0.0006,
      "step": 3051
    },
    {
      "epoch": 1.5237144283574637,
      "grad_norm": 0.04186853766441345,
      "learning_rate": 0.00048476285571642536,
      "loss": 0.0013,
      "step": 3052
    },
    {
      "epoch": 1.5242136794807788,
      "grad_norm": 0.06599728763103485,
      "learning_rate": 0.0004847578632051922,
      "loss": 0.0011,
      "step": 3053
    },
    {
      "epoch": 1.5247129306040939,
      "grad_norm": 0.0075394283048808575,
      "learning_rate": 0.00048475287069395907,
      "loss": 0.0007,
      "step": 3054
    },
    {
      "epoch": 1.525212181727409,
      "grad_norm": 0.3502693772315979,
      "learning_rate": 0.0004847478781827259,
      "loss": 0.0117,
      "step": 3055
    },
    {
      "epoch": 1.525711432850724,
      "grad_norm": 0.08661679178476334,
      "learning_rate": 0.00048474288567149277,
      "loss": 0.0013,
      "step": 3056
    },
    {
      "epoch": 1.526210683974039,
      "grad_norm": 0.36605581641197205,
      "learning_rate": 0.0004847378931602596,
      "loss": 0.0036,
      "step": 3057
    },
    {
      "epoch": 1.526709935097354,
      "grad_norm": 0.2649212181568146,
      "learning_rate": 0.0004847329006490265,
      "loss": 0.0026,
      "step": 3058
    },
    {
      "epoch": 1.5272091862206691,
      "grad_norm": 0.6327939033508301,
      "learning_rate": 0.00048472790813779333,
      "loss": 0.0133,
      "step": 3059
    },
    {
      "epoch": 1.527708437343984,
      "grad_norm": 0.30699512362480164,
      "learning_rate": 0.0004847229156265602,
      "loss": 0.0041,
      "step": 3060
    },
    {
      "epoch": 1.528207688467299,
      "grad_norm": 0.02151464857161045,
      "learning_rate": 0.00048471792311532703,
      "loss": 0.001,
      "step": 3061
    },
    {
      "epoch": 1.528706939590614,
      "grad_norm": 0.01804298162460327,
      "learning_rate": 0.0004847129306040939,
      "loss": 0.0008,
      "step": 3062
    },
    {
      "epoch": 1.5292061907139292,
      "grad_norm": 0.14996236562728882,
      "learning_rate": 0.00048470793809286074,
      "loss": 0.0026,
      "step": 3063
    },
    {
      "epoch": 1.529705441837244,
      "grad_norm": 0.37002336978912354,
      "learning_rate": 0.0004847029455816276,
      "loss": 0.0101,
      "step": 3064
    },
    {
      "epoch": 1.530204692960559,
      "grad_norm": 0.6308546662330627,
      "learning_rate": 0.00048469795307039444,
      "loss": 0.0069,
      "step": 3065
    },
    {
      "epoch": 1.5307039440838741,
      "grad_norm": 0.5246270895004272,
      "learning_rate": 0.0004846929605591613,
      "loss": 0.0056,
      "step": 3066
    },
    {
      "epoch": 1.5312031952071892,
      "grad_norm": 1.3318217992782593,
      "learning_rate": 0.00048468796804792814,
      "loss": 0.0141,
      "step": 3067
    },
    {
      "epoch": 1.5317024463305042,
      "grad_norm": 0.06594014167785645,
      "learning_rate": 0.000484682975536695,
      "loss": 0.0014,
      "step": 3068
    },
    {
      "epoch": 1.5322016974538193,
      "grad_norm": 0.018775764852762222,
      "learning_rate": 0.00048467798302546185,
      "loss": 0.0008,
      "step": 3069
    },
    {
      "epoch": 1.5327009485771343,
      "grad_norm": 0.03154667839407921,
      "learning_rate": 0.00048467299051422865,
      "loss": 0.0008,
      "step": 3070
    },
    {
      "epoch": 1.5332001997004494,
      "grad_norm": 0.5133610367774963,
      "learning_rate": 0.0004846679980029955,
      "loss": 0.0103,
      "step": 3071
    },
    {
      "epoch": 1.5336994508237645,
      "grad_norm": 0.056326087564229965,
      "learning_rate": 0.00048466300549176235,
      "loss": 0.0011,
      "step": 3072
    },
    {
      "epoch": 1.5341987019470795,
      "grad_norm": 0.022770855575799942,
      "learning_rate": 0.0004846580129805292,
      "loss": 0.0011,
      "step": 3073
    },
    {
      "epoch": 1.5346979530703944,
      "grad_norm": 0.02309463545680046,
      "learning_rate": 0.00048465302046929605,
      "loss": 0.0007,
      "step": 3074
    },
    {
      "epoch": 1.5351972041937094,
      "grad_norm": 0.33569690585136414,
      "learning_rate": 0.0004846480279580629,
      "loss": 0.0089,
      "step": 3075
    },
    {
      "epoch": 1.5356964553170245,
      "grad_norm": 0.060556598007678986,
      "learning_rate": 0.00048464303544682976,
      "loss": 0.0011,
      "step": 3076
    },
    {
      "epoch": 1.5361957064403395,
      "grad_norm": 0.3471793830394745,
      "learning_rate": 0.0004846380429355966,
      "loss": 0.0062,
      "step": 3077
    },
    {
      "epoch": 1.5366949575636544,
      "grad_norm": 0.03599863499403,
      "learning_rate": 0.00048463305042436346,
      "loss": 0.0011,
      "step": 3078
    },
    {
      "epoch": 1.5371942086869694,
      "grad_norm": 0.23623384535312653,
      "learning_rate": 0.0004846280579131303,
      "loss": 0.0032,
      "step": 3079
    },
    {
      "epoch": 1.5376934598102845,
      "grad_norm": 0.04487161710858345,
      "learning_rate": 0.00048462306540189717,
      "loss": 0.0013,
      "step": 3080
    },
    {
      "epoch": 1.5381927109335995,
      "grad_norm": 0.0585906021296978,
      "learning_rate": 0.000484618072890664,
      "loss": 0.0012,
      "step": 3081
    },
    {
      "epoch": 1.5386919620569146,
      "grad_norm": 0.4241219162940979,
      "learning_rate": 0.00048461308037943087,
      "loss": 0.0049,
      "step": 3082
    },
    {
      "epoch": 1.5391912131802297,
      "grad_norm": 0.05549095198512077,
      "learning_rate": 0.0004846080878681977,
      "loss": 0.0017,
      "step": 3083
    },
    {
      "epoch": 1.5396904643035447,
      "grad_norm": 0.6936100721359253,
      "learning_rate": 0.0004846030953569646,
      "loss": 0.0165,
      "step": 3084
    },
    {
      "epoch": 1.5401897154268598,
      "grad_norm": 0.024437328800559044,
      "learning_rate": 0.0004845981028457314,
      "loss": 0.0009,
      "step": 3085
    },
    {
      "epoch": 1.5406889665501748,
      "grad_norm": 0.4083392322063446,
      "learning_rate": 0.0004845931103344983,
      "loss": 0.0055,
      "step": 3086
    },
    {
      "epoch": 1.54118821767349,
      "grad_norm": 0.017154287546873093,
      "learning_rate": 0.00048458811782326513,
      "loss": 0.0007,
      "step": 3087
    },
    {
      "epoch": 1.5416874687968047,
      "grad_norm": 0.15198999643325806,
      "learning_rate": 0.000484583125312032,
      "loss": 0.0019,
      "step": 3088
    },
    {
      "epoch": 1.5421867199201198,
      "grad_norm": 0.1091424822807312,
      "learning_rate": 0.00048457813280079884,
      "loss": 0.0017,
      "step": 3089
    },
    {
      "epoch": 1.5426859710434349,
      "grad_norm": 1.7655245065689087,
      "learning_rate": 0.0004845731402895657,
      "loss": 0.0359,
      "step": 3090
    },
    {
      "epoch": 1.54318522216675,
      "grad_norm": 0.022847944870591164,
      "learning_rate": 0.00048456814777833254,
      "loss": 0.0008,
      "step": 3091
    },
    {
      "epoch": 1.5436844732900648,
      "grad_norm": 0.06943830102682114,
      "learning_rate": 0.0004845631552670994,
      "loss": 0.001,
      "step": 3092
    },
    {
      "epoch": 1.5441837244133798,
      "grad_norm": 0.7129969000816345,
      "learning_rate": 0.00048455816275586624,
      "loss": 0.008,
      "step": 3093
    },
    {
      "epoch": 1.5446829755366949,
      "grad_norm": 0.23174628615379333,
      "learning_rate": 0.0004845531702446331,
      "loss": 0.0061,
      "step": 3094
    },
    {
      "epoch": 1.54518222666001,
      "grad_norm": 1.0213170051574707,
      "learning_rate": 0.00048454817773339995,
      "loss": 0.0048,
      "step": 3095
    },
    {
      "epoch": 1.545681477783325,
      "grad_norm": 0.08547104895114899,
      "learning_rate": 0.0004845431852221668,
      "loss": 0.0016,
      "step": 3096
    },
    {
      "epoch": 1.54618072890664,
      "grad_norm": 0.03743506222963333,
      "learning_rate": 0.00048453819271093365,
      "loss": 0.0011,
      "step": 3097
    },
    {
      "epoch": 1.546679980029955,
      "grad_norm": 0.17844970524311066,
      "learning_rate": 0.0004845332001997005,
      "loss": 0.0047,
      "step": 3098
    },
    {
      "epoch": 1.5471792311532702,
      "grad_norm": 0.6364378333091736,
      "learning_rate": 0.0004845282076884673,
      "loss": 0.0176,
      "step": 3099
    },
    {
      "epoch": 1.5476784822765852,
      "grad_norm": 0.09607494622468948,
      "learning_rate": 0.00048452321517723415,
      "loss": 0.0021,
      "step": 3100
    },
    {
      "epoch": 1.5481777333999003,
      "grad_norm": 0.4600907862186432,
      "learning_rate": 0.000484518222666001,
      "loss": 0.0134,
      "step": 3101
    },
    {
      "epoch": 1.5486769845232153,
      "grad_norm": 0.5071492195129395,
      "learning_rate": 0.00048451323015476786,
      "loss": 0.0097,
      "step": 3102
    },
    {
      "epoch": 1.5491762356465302,
      "grad_norm": 0.07487279176712036,
      "learning_rate": 0.0004845082376435347,
      "loss": 0.0019,
      "step": 3103
    },
    {
      "epoch": 1.5496754867698452,
      "grad_norm": 0.443636029958725,
      "learning_rate": 0.00048450324513230156,
      "loss": 0.0054,
      "step": 3104
    },
    {
      "epoch": 1.5501747378931603,
      "grad_norm": 0.019413430243730545,
      "learning_rate": 0.0004844982526210684,
      "loss": 0.0011,
      "step": 3105
    },
    {
      "epoch": 1.5506739890164751,
      "grad_norm": 0.1845504492521286,
      "learning_rate": 0.00048449326010983527,
      "loss": 0.0032,
      "step": 3106
    },
    {
      "epoch": 1.5511732401397902,
      "grad_norm": 0.11081983894109726,
      "learning_rate": 0.0004844882675986021,
      "loss": 0.0018,
      "step": 3107
    },
    {
      "epoch": 1.5516724912631052,
      "grad_norm": 0.04558759182691574,
      "learning_rate": 0.00048448327508736897,
      "loss": 0.0017,
      "step": 3108
    },
    {
      "epoch": 1.5521717423864203,
      "grad_norm": 0.4469987154006958,
      "learning_rate": 0.0004844782825761358,
      "loss": 0.0061,
      "step": 3109
    },
    {
      "epoch": 1.5526709935097354,
      "grad_norm": 0.32075849175453186,
      "learning_rate": 0.0004844732900649027,
      "loss": 0.012,
      "step": 3110
    },
    {
      "epoch": 1.5531702446330504,
      "grad_norm": 0.03330389782786369,
      "learning_rate": 0.0004844682975536695,
      "loss": 0.0011,
      "step": 3111
    },
    {
      "epoch": 1.5536694957563655,
      "grad_norm": 0.1009143814444542,
      "learning_rate": 0.0004844633050424364,
      "loss": 0.002,
      "step": 3112
    },
    {
      "epoch": 1.5541687468796805,
      "grad_norm": 0.05642811208963394,
      "learning_rate": 0.00048445831253120323,
      "loss": 0.0013,
      "step": 3113
    },
    {
      "epoch": 1.5546679980029956,
      "grad_norm": 0.18925298750400543,
      "learning_rate": 0.0004844533200199701,
      "loss": 0.0042,
      "step": 3114
    },
    {
      "epoch": 1.5551672491263107,
      "grad_norm": 0.420174241065979,
      "learning_rate": 0.00048444832750873693,
      "loss": 0.0071,
      "step": 3115
    },
    {
      "epoch": 1.5556665002496257,
      "grad_norm": 0.030214237049221992,
      "learning_rate": 0.00048444333499750373,
      "loss": 0.0011,
      "step": 3116
    },
    {
      "epoch": 1.5561657513729406,
      "grad_norm": 0.281647652387619,
      "learning_rate": 0.0004844383424862706,
      "loss": 0.0025,
      "step": 3117
    },
    {
      "epoch": 1.5566650024962556,
      "grad_norm": 0.32176005840301514,
      "learning_rate": 0.00048443334997503744,
      "loss": 0.0039,
      "step": 3118
    },
    {
      "epoch": 1.5571642536195707,
      "grad_norm": 0.10465089231729507,
      "learning_rate": 0.0004844283574638043,
      "loss": 0.0016,
      "step": 3119
    },
    {
      "epoch": 1.5576635047428855,
      "grad_norm": 0.09297601133584976,
      "learning_rate": 0.00048442336495257114,
      "loss": 0.0021,
      "step": 3120
    },
    {
      "epoch": 1.5581627558662006,
      "grad_norm": 0.3595878481864929,
      "learning_rate": 0.000484418372441338,
      "loss": 0.006,
      "step": 3121
    },
    {
      "epoch": 1.5586620069895156,
      "grad_norm": 0.11469966918230057,
      "learning_rate": 0.00048441337993010484,
      "loss": 0.0032,
      "step": 3122
    },
    {
      "epoch": 1.5591612581128307,
      "grad_norm": 0.165529265999794,
      "learning_rate": 0.0004844083874188717,
      "loss": 0.0029,
      "step": 3123
    },
    {
      "epoch": 1.5596605092361457,
      "grad_norm": 0.27805861830711365,
      "learning_rate": 0.00048440339490763855,
      "loss": 0.0055,
      "step": 3124
    },
    {
      "epoch": 1.5601597603594608,
      "grad_norm": 0.3328130841255188,
      "learning_rate": 0.0004843984023964054,
      "loss": 0.0033,
      "step": 3125
    },
    {
      "epoch": 1.5606590114827759,
      "grad_norm": 0.028117550536990166,
      "learning_rate": 0.00048439340988517225,
      "loss": 0.0008,
      "step": 3126
    },
    {
      "epoch": 1.561158262606091,
      "grad_norm": 0.07715009152889252,
      "learning_rate": 0.0004843884173739391,
      "loss": 0.0016,
      "step": 3127
    },
    {
      "epoch": 1.561657513729406,
      "grad_norm": 0.14569222927093506,
      "learning_rate": 0.0004843834248627059,
      "loss": 0.0029,
      "step": 3128
    },
    {
      "epoch": 1.562156764852721,
      "grad_norm": 0.26884207129478455,
      "learning_rate": 0.00048437843235147276,
      "loss": 0.007,
      "step": 3129
    },
    {
      "epoch": 1.562656015976036,
      "grad_norm": 0.0400584377348423,
      "learning_rate": 0.0004843734398402396,
      "loss": 0.0013,
      "step": 3130
    },
    {
      "epoch": 1.563155267099351,
      "grad_norm": 0.04113668203353882,
      "learning_rate": 0.00048436844732900646,
      "loss": 0.0011,
      "step": 3131
    },
    {
      "epoch": 1.563654518222666,
      "grad_norm": 0.8310951590538025,
      "learning_rate": 0.0004843634548177733,
      "loss": 0.0052,
      "step": 3132
    },
    {
      "epoch": 1.564153769345981,
      "grad_norm": 0.324102520942688,
      "learning_rate": 0.00048435846230654016,
      "loss": 0.0046,
      "step": 3133
    },
    {
      "epoch": 1.564653020469296,
      "grad_norm": 0.250031977891922,
      "learning_rate": 0.000484353469795307,
      "loss": 0.0029,
      "step": 3134
    },
    {
      "epoch": 1.565152271592611,
      "grad_norm": 0.11249136179685593,
      "learning_rate": 0.00048434847728407387,
      "loss": 0.0016,
      "step": 3135
    },
    {
      "epoch": 1.565651522715926,
      "grad_norm": 0.06194803863763809,
      "learning_rate": 0.0004843434847728407,
      "loss": 0.0013,
      "step": 3136
    },
    {
      "epoch": 1.566150773839241,
      "grad_norm": 0.03372754901647568,
      "learning_rate": 0.00048433849226160757,
      "loss": 0.0011,
      "step": 3137
    },
    {
      "epoch": 1.5666500249625561,
      "grad_norm": 1.285559058189392,
      "learning_rate": 0.0004843334997503744,
      "loss": 0.0084,
      "step": 3138
    },
    {
      "epoch": 1.5671492760858712,
      "grad_norm": 0.043888360261917114,
      "learning_rate": 0.0004843285072391413,
      "loss": 0.0012,
      "step": 3139
    },
    {
      "epoch": 1.5676485272091862,
      "grad_norm": 1.3540246486663818,
      "learning_rate": 0.00048432351472790813,
      "loss": 0.0306,
      "step": 3140
    },
    {
      "epoch": 1.5681477783325013,
      "grad_norm": 1.1895736455917358,
      "learning_rate": 0.000484318522216675,
      "loss": 0.0088,
      "step": 3141
    },
    {
      "epoch": 1.5686470294558164,
      "grad_norm": 0.5395000576972961,
      "learning_rate": 0.00048431352970544183,
      "loss": 0.0105,
      "step": 3142
    },
    {
      "epoch": 1.5691462805791314,
      "grad_norm": 0.023699238896369934,
      "learning_rate": 0.0004843085371942087,
      "loss": 0.0011,
      "step": 3143
    },
    {
      "epoch": 1.5696455317024465,
      "grad_norm": 0.7794061899185181,
      "learning_rate": 0.00048430354468297554,
      "loss": 0.0173,
      "step": 3144
    },
    {
      "epoch": 1.5701447828257613,
      "grad_norm": 0.01720651052892208,
      "learning_rate": 0.0004842985521717424,
      "loss": 0.0009,
      "step": 3145
    },
    {
      "epoch": 1.5706440339490764,
      "grad_norm": 0.04775731638073921,
      "learning_rate": 0.00048429355966050924,
      "loss": 0.0013,
      "step": 3146
    },
    {
      "epoch": 1.5711432850723914,
      "grad_norm": 0.025702431797981262,
      "learning_rate": 0.0004842885671492761,
      "loss": 0.0009,
      "step": 3147
    },
    {
      "epoch": 1.5716425361957065,
      "grad_norm": 0.04822225496172905,
      "learning_rate": 0.00048428357463804294,
      "loss": 0.0012,
      "step": 3148
    },
    {
      "epoch": 1.5721417873190213,
      "grad_norm": 0.028803328052163124,
      "learning_rate": 0.0004842785821268098,
      "loss": 0.001,
      "step": 3149
    },
    {
      "epoch": 1.5726410384423364,
      "grad_norm": 0.5750745534896851,
      "learning_rate": 0.00048427358961557665,
      "loss": 0.0026,
      "step": 3150
    },
    {
      "epoch": 1.5731402895656514,
      "grad_norm": 0.15531820058822632,
      "learning_rate": 0.0004842685971043435,
      "loss": 0.0031,
      "step": 3151
    },
    {
      "epoch": 1.5736395406889665,
      "grad_norm": 0.5426210165023804,
      "learning_rate": 0.00048426360459311035,
      "loss": 0.0065,
      "step": 3152
    },
    {
      "epoch": 1.5741387918122816,
      "grad_norm": 0.08087573945522308,
      "learning_rate": 0.0004842586120818772,
      "loss": 0.0022,
      "step": 3153
    },
    {
      "epoch": 1.5746380429355966,
      "grad_norm": 0.7315743565559387,
      "learning_rate": 0.00048425361957064406,
      "loss": 0.0186,
      "step": 3154
    },
    {
      "epoch": 1.5751372940589117,
      "grad_norm": 0.23055095970630646,
      "learning_rate": 0.0004842486270594109,
      "loss": 0.0027,
      "step": 3155
    },
    {
      "epoch": 1.5756365451822267,
      "grad_norm": 0.14076310396194458,
      "learning_rate": 0.00048424363454817776,
      "loss": 0.003,
      "step": 3156
    },
    {
      "epoch": 1.5761357963055418,
      "grad_norm": 0.1507352739572525,
      "learning_rate": 0.00048423864203694456,
      "loss": 0.0013,
      "step": 3157
    },
    {
      "epoch": 1.5766350474288569,
      "grad_norm": 0.45102936029434204,
      "learning_rate": 0.0004842336495257114,
      "loss": 0.0248,
      "step": 3158
    },
    {
      "epoch": 1.5771342985521717,
      "grad_norm": 0.08654117584228516,
      "learning_rate": 0.00048422865701447826,
      "loss": 0.0025,
      "step": 3159
    },
    {
      "epoch": 1.5776335496754867,
      "grad_norm": 0.14157281816005707,
      "learning_rate": 0.0004842236645032451,
      "loss": 0.0038,
      "step": 3160
    },
    {
      "epoch": 1.5781328007988018,
      "grad_norm": 0.6675612330436707,
      "learning_rate": 0.00048421867199201197,
      "loss": 0.0122,
      "step": 3161
    },
    {
      "epoch": 1.5786320519221169,
      "grad_norm": 0.1264815479516983,
      "learning_rate": 0.0004842136794807788,
      "loss": 0.0026,
      "step": 3162
    },
    {
      "epoch": 1.5791313030454317,
      "grad_norm": 0.10894029587507248,
      "learning_rate": 0.00048420868696954567,
      "loss": 0.002,
      "step": 3163
    },
    {
      "epoch": 1.5796305541687468,
      "grad_norm": 0.11105231940746307,
      "learning_rate": 0.0004842036944583125,
      "loss": 0.0012,
      "step": 3164
    },
    {
      "epoch": 1.5801298052920618,
      "grad_norm": 0.13103334605693817,
      "learning_rate": 0.0004841987019470794,
      "loss": 0.0026,
      "step": 3165
    },
    {
      "epoch": 1.5806290564153769,
      "grad_norm": 0.4233381152153015,
      "learning_rate": 0.00048419370943584623,
      "loss": 0.0083,
      "step": 3166
    },
    {
      "epoch": 1.581128307538692,
      "grad_norm": 0.30812886357307434,
      "learning_rate": 0.0004841887169246131,
      "loss": 0.0068,
      "step": 3167
    },
    {
      "epoch": 1.581627558662007,
      "grad_norm": 0.49418166279792786,
      "learning_rate": 0.00048418372441337993,
      "loss": 0.0125,
      "step": 3168
    },
    {
      "epoch": 1.582126809785322,
      "grad_norm": 0.4308861494064331,
      "learning_rate": 0.0004841787319021468,
      "loss": 0.0085,
      "step": 3169
    },
    {
      "epoch": 1.5826260609086371,
      "grad_norm": 0.08073214441537857,
      "learning_rate": 0.00048417373939091364,
      "loss": 0.0025,
      "step": 3170
    },
    {
      "epoch": 1.5831253120319522,
      "grad_norm": 0.34408196806907654,
      "learning_rate": 0.0004841687468796805,
      "loss": 0.0089,
      "step": 3171
    },
    {
      "epoch": 1.5836245631552672,
      "grad_norm": 0.4136040210723877,
      "learning_rate": 0.00048416375436844734,
      "loss": 0.0043,
      "step": 3172
    },
    {
      "epoch": 1.5841238142785823,
      "grad_norm": 0.23811058700084686,
      "learning_rate": 0.0004841587618572142,
      "loss": 0.0106,
      "step": 3173
    },
    {
      "epoch": 1.5846230654018971,
      "grad_norm": 0.1855497509241104,
      "learning_rate": 0.00048415376934598104,
      "loss": 0.0034,
      "step": 3174
    },
    {
      "epoch": 1.5851223165252122,
      "grad_norm": 0.14067065715789795,
      "learning_rate": 0.0004841487768347479,
      "loss": 0.0023,
      "step": 3175
    },
    {
      "epoch": 1.5856215676485272,
      "grad_norm": 0.3043203353881836,
      "learning_rate": 0.00048414378432351475,
      "loss": 0.0042,
      "step": 3176
    },
    {
      "epoch": 1.586120818771842,
      "grad_norm": 0.27113398909568787,
      "learning_rate": 0.0004841387918122816,
      "loss": 0.004,
      "step": 3177
    },
    {
      "epoch": 1.5866200698951571,
      "grad_norm": 0.10683689266443253,
      "learning_rate": 0.00048413379930104845,
      "loss": 0.0022,
      "step": 3178
    },
    {
      "epoch": 1.5871193210184722,
      "grad_norm": 0.072463259100914,
      "learning_rate": 0.0004841288067898153,
      "loss": 0.0014,
      "step": 3179
    },
    {
      "epoch": 1.5876185721417873,
      "grad_norm": 1.760029911994934,
      "learning_rate": 0.00048412381427858216,
      "loss": 0.0157,
      "step": 3180
    },
    {
      "epoch": 1.5881178232651023,
      "grad_norm": 0.37994518876075745,
      "learning_rate": 0.000484118821767349,
      "loss": 0.0056,
      "step": 3181
    },
    {
      "epoch": 1.5886170743884174,
      "grad_norm": 0.07170813530683517,
      "learning_rate": 0.00048411382925611586,
      "loss": 0.0015,
      "step": 3182
    },
    {
      "epoch": 1.5891163255117324,
      "grad_norm": 0.01688672974705696,
      "learning_rate": 0.0004841088367448827,
      "loss": 0.001,
      "step": 3183
    },
    {
      "epoch": 1.5896155766350475,
      "grad_norm": 0.10057523846626282,
      "learning_rate": 0.00048410384423364956,
      "loss": 0.002,
      "step": 3184
    },
    {
      "epoch": 1.5901148277583625,
      "grad_norm": 0.18254420161247253,
      "learning_rate": 0.0004840988517224164,
      "loss": 0.0032,
      "step": 3185
    },
    {
      "epoch": 1.5906140788816776,
      "grad_norm": 0.031949952244758606,
      "learning_rate": 0.00048409385921118327,
      "loss": 0.0012,
      "step": 3186
    },
    {
      "epoch": 1.5911133300049927,
      "grad_norm": 0.38601982593536377,
      "learning_rate": 0.00048408886669995007,
      "loss": 0.0082,
      "step": 3187
    },
    {
      "epoch": 1.5916125811283075,
      "grad_norm": 0.06950999796390533,
      "learning_rate": 0.0004840838741887169,
      "loss": 0.0014,
      "step": 3188
    },
    {
      "epoch": 1.5921118322516226,
      "grad_norm": 0.01976640336215496,
      "learning_rate": 0.00048407888167748377,
      "loss": 0.0009,
      "step": 3189
    },
    {
      "epoch": 1.5926110833749376,
      "grad_norm": 0.2810003161430359,
      "learning_rate": 0.0004840738891662506,
      "loss": 0.0028,
      "step": 3190
    },
    {
      "epoch": 1.5931103344982525,
      "grad_norm": 0.18272893130779266,
      "learning_rate": 0.0004840688966550175,
      "loss": 0.0024,
      "step": 3191
    },
    {
      "epoch": 1.5936095856215675,
      "grad_norm": 0.3695998787879944,
      "learning_rate": 0.0004840639041437843,
      "loss": 0.0054,
      "step": 3192
    },
    {
      "epoch": 1.5941088367448826,
      "grad_norm": 0.39714881777763367,
      "learning_rate": 0.0004840589116325512,
      "loss": 0.0022,
      "step": 3193
    },
    {
      "epoch": 1.5946080878681976,
      "grad_norm": 0.11294333636760712,
      "learning_rate": 0.00048405391912131803,
      "loss": 0.0019,
      "step": 3194
    },
    {
      "epoch": 1.5951073389915127,
      "grad_norm": 0.37319865822792053,
      "learning_rate": 0.0004840489266100849,
      "loss": 0.0052,
      "step": 3195
    },
    {
      "epoch": 1.5956065901148277,
      "grad_norm": 0.0147040244191885,
      "learning_rate": 0.00048404393409885174,
      "loss": 0.0006,
      "step": 3196
    },
    {
      "epoch": 1.5961058412381428,
      "grad_norm": 0.015141582116484642,
      "learning_rate": 0.0004840389415876186,
      "loss": 0.0007,
      "step": 3197
    },
    {
      "epoch": 1.5966050923614579,
      "grad_norm": 0.07792532444000244,
      "learning_rate": 0.00048403394907638544,
      "loss": 0.001,
      "step": 3198
    },
    {
      "epoch": 1.597104343484773,
      "grad_norm": 0.10485807061195374,
      "learning_rate": 0.0004840289565651523,
      "loss": 0.002,
      "step": 3199
    },
    {
      "epoch": 1.597603594608088,
      "grad_norm": 0.4566255807876587,
      "learning_rate": 0.00048402396405391914,
      "loss": 0.0061,
      "step": 3200
    },
    {
      "epoch": 1.598102845731403,
      "grad_norm": 0.03813653066754341,
      "learning_rate": 0.000484018971542686,
      "loss": 0.0008,
      "step": 3201
    },
    {
      "epoch": 1.5986020968547179,
      "grad_norm": 0.11590638011693954,
      "learning_rate": 0.00048401397903145285,
      "loss": 0.0016,
      "step": 3202
    },
    {
      "epoch": 1.599101347978033,
      "grad_norm": 0.11115191131830215,
      "learning_rate": 0.0004840089865202197,
      "loss": 0.0013,
      "step": 3203
    },
    {
      "epoch": 1.599600599101348,
      "grad_norm": 0.21627338230609894,
      "learning_rate": 0.00048400399400898655,
      "loss": 0.0017,
      "step": 3204
    },
    {
      "epoch": 1.600099850224663,
      "grad_norm": 0.13246509432792664,
      "learning_rate": 0.0004839990014977534,
      "loss": 0.0018,
      "step": 3205
    },
    {
      "epoch": 1.600599101347978,
      "grad_norm": 0.027712661772966385,
      "learning_rate": 0.00048399400898652026,
      "loss": 0.0008,
      "step": 3206
    },
    {
      "epoch": 1.601098352471293,
      "grad_norm": 0.048410072922706604,
      "learning_rate": 0.0004839890164752871,
      "loss": 0.001,
      "step": 3207
    },
    {
      "epoch": 1.601597603594608,
      "grad_norm": 0.024889182299375534,
      "learning_rate": 0.00048398402396405396,
      "loss": 0.0007,
      "step": 3208
    },
    {
      "epoch": 1.602096854717923,
      "grad_norm": 0.24987104535102844,
      "learning_rate": 0.0004839790314528208,
      "loss": 0.0041,
      "step": 3209
    },
    {
      "epoch": 1.6025961058412381,
      "grad_norm": 0.024874182417988777,
      "learning_rate": 0.00048397403894158766,
      "loss": 0.0008,
      "step": 3210
    },
    {
      "epoch": 1.6030953569645532,
      "grad_norm": 0.020468365401029587,
      "learning_rate": 0.0004839690464303545,
      "loss": 0.0008,
      "step": 3211
    },
    {
      "epoch": 1.6035946080878682,
      "grad_norm": 0.20398400723934174,
      "learning_rate": 0.00048396405391912137,
      "loss": 0.0023,
      "step": 3212
    },
    {
      "epoch": 1.6040938592111833,
      "grad_norm": 0.2581503093242645,
      "learning_rate": 0.0004839590614078882,
      "loss": 0.0055,
      "step": 3213
    },
    {
      "epoch": 1.6045931103344984,
      "grad_norm": 0.0457301139831543,
      "learning_rate": 0.00048395406889665507,
      "loss": 0.0012,
      "step": 3214
    },
    {
      "epoch": 1.6050923614578134,
      "grad_norm": 0.3142743408679962,
      "learning_rate": 0.0004839490763854219,
      "loss": 0.0084,
      "step": 3215
    },
    {
      "epoch": 1.6055916125811283,
      "grad_norm": 0.013469315133988857,
      "learning_rate": 0.0004839440838741887,
      "loss": 0.0006,
      "step": 3216
    },
    {
      "epoch": 1.6060908637044433,
      "grad_norm": 0.07888590544462204,
      "learning_rate": 0.0004839390913629556,
      "loss": 0.0014,
      "step": 3217
    },
    {
      "epoch": 1.6065901148277584,
      "grad_norm": 0.07126627117395401,
      "learning_rate": 0.0004839340988517224,
      "loss": 0.0008,
      "step": 3218
    },
    {
      "epoch": 1.6070893659510734,
      "grad_norm": 0.22378848493099213,
      "learning_rate": 0.0004839291063404893,
      "loss": 0.003,
      "step": 3219
    },
    {
      "epoch": 1.6075886170743883,
      "grad_norm": 0.7940017580986023,
      "learning_rate": 0.00048392411382925613,
      "loss": 0.0069,
      "step": 3220
    },
    {
      "epoch": 1.6080878681977033,
      "grad_norm": 0.21792863309383392,
      "learning_rate": 0.000483919121318023,
      "loss": 0.0042,
      "step": 3221
    },
    {
      "epoch": 1.6085871193210184,
      "grad_norm": 0.1034473329782486,
      "learning_rate": 0.00048391412880678983,
      "loss": 0.0016,
      "step": 3222
    },
    {
      "epoch": 1.6090863704443334,
      "grad_norm": 0.5125317573547363,
      "learning_rate": 0.0004839091362955567,
      "loss": 0.0024,
      "step": 3223
    },
    {
      "epoch": 1.6095856215676485,
      "grad_norm": 0.04642916098237038,
      "learning_rate": 0.00048390414378432354,
      "loss": 0.0009,
      "step": 3224
    },
    {
      "epoch": 1.6100848726909636,
      "grad_norm": 0.2409535050392151,
      "learning_rate": 0.0004838991512730904,
      "loss": 0.0064,
      "step": 3225
    },
    {
      "epoch": 1.6105841238142786,
      "grad_norm": 0.6455031633377075,
      "learning_rate": 0.00048389415876185724,
      "loss": 0.005,
      "step": 3226
    },
    {
      "epoch": 1.6110833749375937,
      "grad_norm": 0.03998610004782677,
      "learning_rate": 0.0004838891662506241,
      "loss": 0.0008,
      "step": 3227
    },
    {
      "epoch": 1.6115826260609087,
      "grad_norm": 0.041773438453674316,
      "learning_rate": 0.00048388417373939095,
      "loss": 0.001,
      "step": 3228
    },
    {
      "epoch": 1.6120818771842238,
      "grad_norm": 0.11297965049743652,
      "learning_rate": 0.00048387918122815775,
      "loss": 0.0015,
      "step": 3229
    },
    {
      "epoch": 1.6125811283075389,
      "grad_norm": 0.644676923751831,
      "learning_rate": 0.0004838741887169246,
      "loss": 0.0089,
      "step": 3230
    },
    {
      "epoch": 1.6130803794308537,
      "grad_norm": 0.18353119492530823,
      "learning_rate": 0.00048386919620569145,
      "loss": 0.002,
      "step": 3231
    },
    {
      "epoch": 1.6135796305541688,
      "grad_norm": 1.2377194166183472,
      "learning_rate": 0.0004838642036944583,
      "loss": 0.0042,
      "step": 3232
    },
    {
      "epoch": 1.6140788816774838,
      "grad_norm": 0.38451439142227173,
      "learning_rate": 0.00048385921118322515,
      "loss": 0.0063,
      "step": 3233
    },
    {
      "epoch": 1.6145781328007986,
      "grad_norm": 0.3366641700267792,
      "learning_rate": 0.000483854218671992,
      "loss": 0.0019,
      "step": 3234
    },
    {
      "epoch": 1.6150773839241137,
      "grad_norm": 0.11388994008302689,
      "learning_rate": 0.00048384922616075886,
      "loss": 0.0011,
      "step": 3235
    },
    {
      "epoch": 1.6155766350474288,
      "grad_norm": 0.2694539427757263,
      "learning_rate": 0.0004838442336495257,
      "loss": 0.0196,
      "step": 3236
    },
    {
      "epoch": 1.6160758861707438,
      "grad_norm": 0.11016277968883514,
      "learning_rate": 0.00048383924113829256,
      "loss": 0.0009,
      "step": 3237
    },
    {
      "epoch": 1.6165751372940589,
      "grad_norm": 0.07866527885198593,
      "learning_rate": 0.0004838342486270594,
      "loss": 0.0015,
      "step": 3238
    },
    {
      "epoch": 1.617074388417374,
      "grad_norm": 0.9321317672729492,
      "learning_rate": 0.00048382925611582627,
      "loss": 0.006,
      "step": 3239
    },
    {
      "epoch": 1.617573639540689,
      "grad_norm": 0.0492793470621109,
      "learning_rate": 0.0004838242636045931,
      "loss": 0.0008,
      "step": 3240
    },
    {
      "epoch": 1.618072890664004,
      "grad_norm": 0.6202850937843323,
      "learning_rate": 0.00048381927109335997,
      "loss": 0.0279,
      "step": 3241
    },
    {
      "epoch": 1.6185721417873191,
      "grad_norm": 0.062279004603624344,
      "learning_rate": 0.0004838142785821268,
      "loss": 0.0007,
      "step": 3242
    },
    {
      "epoch": 1.6190713929106342,
      "grad_norm": 0.2719047963619232,
      "learning_rate": 0.0004838092860708937,
      "loss": 0.0033,
      "step": 3243
    },
    {
      "epoch": 1.6195706440339492,
      "grad_norm": 0.023426637053489685,
      "learning_rate": 0.0004838042935596605,
      "loss": 0.0007,
      "step": 3244
    },
    {
      "epoch": 1.620069895157264,
      "grad_norm": 0.6841473579406738,
      "learning_rate": 0.0004837993010484273,
      "loss": 0.0183,
      "step": 3245
    },
    {
      "epoch": 1.6205691462805791,
      "grad_norm": 0.35817405581474304,
      "learning_rate": 0.0004837943085371942,
      "loss": 0.0049,
      "step": 3246
    },
    {
      "epoch": 1.6210683974038942,
      "grad_norm": 0.44435834884643555,
      "learning_rate": 0.00048378931602596103,
      "loss": 0.0035,
      "step": 3247
    },
    {
      "epoch": 1.621567648527209,
      "grad_norm": 0.24926993250846863,
      "learning_rate": 0.0004837843235147279,
      "loss": 0.0016,
      "step": 3248
    },
    {
      "epoch": 1.622066899650524,
      "grad_norm": 0.528982400894165,
      "learning_rate": 0.00048377933100349473,
      "loss": 0.0074,
      "step": 3249
    },
    {
      "epoch": 1.6225661507738391,
      "grad_norm": 0.6896312236785889,
      "learning_rate": 0.0004837743384922616,
      "loss": 0.0274,
      "step": 3250
    },
    {
      "epoch": 1.6230654018971542,
      "grad_norm": 0.21423541009426117,
      "learning_rate": 0.00048376934598102844,
      "loss": 0.0029,
      "step": 3251
    },
    {
      "epoch": 1.6235646530204693,
      "grad_norm": 0.4565308690071106,
      "learning_rate": 0.0004837643534697953,
      "loss": 0.0068,
      "step": 3252
    },
    {
      "epoch": 1.6240639041437843,
      "grad_norm": 0.23180587589740753,
      "learning_rate": 0.00048375936095856214,
      "loss": 0.0041,
      "step": 3253
    },
    {
      "epoch": 1.6245631552670994,
      "grad_norm": 0.4241422116756439,
      "learning_rate": 0.000483754368447329,
      "loss": 0.0051,
      "step": 3254
    },
    {
      "epoch": 1.6250624063904144,
      "grad_norm": 0.2131117582321167,
      "learning_rate": 0.00048374937593609584,
      "loss": 0.0031,
      "step": 3255
    },
    {
      "epoch": 1.6255616575137295,
      "grad_norm": 0.07127543538808823,
      "learning_rate": 0.0004837443834248627,
      "loss": 0.002,
      "step": 3256
    },
    {
      "epoch": 1.6260609086370446,
      "grad_norm": 0.13302333652973175,
      "learning_rate": 0.00048373939091362955,
      "loss": 0.0023,
      "step": 3257
    },
    {
      "epoch": 1.6265601597603596,
      "grad_norm": 0.343712717294693,
      "learning_rate": 0.0004837343984023964,
      "loss": 0.0048,
      "step": 3258
    },
    {
      "epoch": 1.6270594108836745,
      "grad_norm": 0.016734208911657333,
      "learning_rate": 0.00048372940589116325,
      "loss": 0.0008,
      "step": 3259
    },
    {
      "epoch": 1.6275586620069895,
      "grad_norm": 0.32145956158638,
      "learning_rate": 0.0004837244133799301,
      "loss": 0.0022,
      "step": 3260
    },
    {
      "epoch": 1.6280579131303046,
      "grad_norm": 0.6599189043045044,
      "learning_rate": 0.00048371942086869696,
      "loss": 0.0167,
      "step": 3261
    },
    {
      "epoch": 1.6285571642536194,
      "grad_norm": 0.3507827818393707,
      "learning_rate": 0.0004837144283574638,
      "loss": 0.003,
      "step": 3262
    },
    {
      "epoch": 1.6290564153769345,
      "grad_norm": 0.16917502880096436,
      "learning_rate": 0.00048370943584623066,
      "loss": 0.0016,
      "step": 3263
    },
    {
      "epoch": 1.6295556665002495,
      "grad_norm": 0.31781864166259766,
      "learning_rate": 0.0004837044433349975,
      "loss": 0.0036,
      "step": 3264
    },
    {
      "epoch": 1.6300549176235646,
      "grad_norm": 0.13845765590667725,
      "learning_rate": 0.00048369945082376437,
      "loss": 0.0029,
      "step": 3265
    },
    {
      "epoch": 1.6305541687468796,
      "grad_norm": 0.5308583974838257,
      "learning_rate": 0.0004836944583125312,
      "loss": 0.0078,
      "step": 3266
    },
    {
      "epoch": 1.6310534198701947,
      "grad_norm": 0.22280016541481018,
      "learning_rate": 0.00048368946580129807,
      "loss": 0.0045,
      "step": 3267
    },
    {
      "epoch": 1.6315526709935098,
      "grad_norm": 0.6034335494041443,
      "learning_rate": 0.0004836844732900649,
      "loss": 0.0049,
      "step": 3268
    },
    {
      "epoch": 1.6320519221168248,
      "grad_norm": 0.2570061981678009,
      "learning_rate": 0.0004836794807788318,
      "loss": 0.0059,
      "step": 3269
    },
    {
      "epoch": 1.6325511732401399,
      "grad_norm": 0.41915827989578247,
      "learning_rate": 0.0004836744882675986,
      "loss": 0.0162,
      "step": 3270
    },
    {
      "epoch": 1.633050424363455,
      "grad_norm": 0.06605251133441925,
      "learning_rate": 0.0004836694957563655,
      "loss": 0.0015,
      "step": 3271
    },
    {
      "epoch": 1.63354967548677,
      "grad_norm": 0.11601489037275314,
      "learning_rate": 0.00048366450324513233,
      "loss": 0.0018,
      "step": 3272
    },
    {
      "epoch": 1.6340489266100848,
      "grad_norm": 0.7308101654052734,
      "learning_rate": 0.0004836595107338992,
      "loss": 0.0106,
      "step": 3273
    },
    {
      "epoch": 1.6345481777333999,
      "grad_norm": 0.38333600759506226,
      "learning_rate": 0.000483654518222666,
      "loss": 0.0046,
      "step": 3274
    },
    {
      "epoch": 1.635047428856715,
      "grad_norm": 0.18280336260795593,
      "learning_rate": 0.00048364952571143283,
      "loss": 0.0022,
      "step": 3275
    },
    {
      "epoch": 1.63554667998003,
      "grad_norm": 0.17926837503910065,
      "learning_rate": 0.0004836445332001997,
      "loss": 0.002,
      "step": 3276
    },
    {
      "epoch": 1.6360459311033448,
      "grad_norm": 0.02537977509200573,
      "learning_rate": 0.00048363954068896654,
      "loss": 0.0009,
      "step": 3277
    },
    {
      "epoch": 1.63654518222666,
      "grad_norm": 0.3825739324092865,
      "learning_rate": 0.0004836345481777334,
      "loss": 0.0046,
      "step": 3278
    },
    {
      "epoch": 1.637044433349975,
      "grad_norm": 1.479634165763855,
      "learning_rate": 0.00048362955566650024,
      "loss": 0.0295,
      "step": 3279
    },
    {
      "epoch": 1.63754368447329,
      "grad_norm": 0.1357097178697586,
      "learning_rate": 0.0004836245631552671,
      "loss": 0.002,
      "step": 3280
    },
    {
      "epoch": 1.638042935596605,
      "grad_norm": 0.6012372970581055,
      "learning_rate": 0.00048361957064403394,
      "loss": 0.022,
      "step": 3281
    },
    {
      "epoch": 1.6385421867199201,
      "grad_norm": 0.5128752589225769,
      "learning_rate": 0.0004836145781328008,
      "loss": 0.006,
      "step": 3282
    },
    {
      "epoch": 1.6390414378432352,
      "grad_norm": 0.15990816056728363,
      "learning_rate": 0.00048360958562156765,
      "loss": 0.0025,
      "step": 3283
    },
    {
      "epoch": 1.6395406889665503,
      "grad_norm": 0.28419265151023865,
      "learning_rate": 0.0004836045931103345,
      "loss": 0.0033,
      "step": 3284
    },
    {
      "epoch": 1.6400399400898653,
      "grad_norm": 0.7923956513404846,
      "learning_rate": 0.00048359960059910135,
      "loss": 0.0214,
      "step": 3285
    },
    {
      "epoch": 1.6405391912131804,
      "grad_norm": 0.9536893963813782,
      "learning_rate": 0.0004835946080878682,
      "loss": 0.0324,
      "step": 3286
    },
    {
      "epoch": 1.6410384423364952,
      "grad_norm": 0.19938041269779205,
      "learning_rate": 0.00048358961557663506,
      "loss": 0.0031,
      "step": 3287
    },
    {
      "epoch": 1.6415376934598103,
      "grad_norm": 0.27630650997161865,
      "learning_rate": 0.0004835846230654019,
      "loss": 0.0071,
      "step": 3288
    },
    {
      "epoch": 1.6420369445831253,
      "grad_norm": 0.30276086926460266,
      "learning_rate": 0.00048357963055416876,
      "loss": 0.0055,
      "step": 3289
    },
    {
      "epoch": 1.6425361957064404,
      "grad_norm": 0.13936856389045715,
      "learning_rate": 0.0004835746380429356,
      "loss": 0.0032,
      "step": 3290
    },
    {
      "epoch": 1.6430354468297552,
      "grad_norm": 0.09566957503557205,
      "learning_rate": 0.00048356964553170246,
      "loss": 0.0025,
      "step": 3291
    },
    {
      "epoch": 1.6435346979530703,
      "grad_norm": 0.161260724067688,
      "learning_rate": 0.0004835646530204693,
      "loss": 0.0034,
      "step": 3292
    },
    {
      "epoch": 1.6440339490763853,
      "grad_norm": 0.03568077087402344,
      "learning_rate": 0.00048355966050923617,
      "loss": 0.0016,
      "step": 3293
    },
    {
      "epoch": 1.6445332001997004,
      "grad_norm": 0.12170720845460892,
      "learning_rate": 0.000483554667998003,
      "loss": 0.002,
      "step": 3294
    },
    {
      "epoch": 1.6450324513230155,
      "grad_norm": 0.32082390785217285,
      "learning_rate": 0.0004835496754867699,
      "loss": 0.0066,
      "step": 3295
    },
    {
      "epoch": 1.6455317024463305,
      "grad_norm": 0.05775315687060356,
      "learning_rate": 0.0004835446829755367,
      "loss": 0.0019,
      "step": 3296
    },
    {
      "epoch": 1.6460309535696456,
      "grad_norm": 0.03706031292676926,
      "learning_rate": 0.0004835396904643036,
      "loss": 0.0016,
      "step": 3297
    },
    {
      "epoch": 1.6465302046929606,
      "grad_norm": 0.4701301157474518,
      "learning_rate": 0.00048353469795307043,
      "loss": 0.0147,
      "step": 3298
    },
    {
      "epoch": 1.6470294558162757,
      "grad_norm": 0.22203579545021057,
      "learning_rate": 0.0004835297054418373,
      "loss": 0.0038,
      "step": 3299
    },
    {
      "epoch": 1.6475287069395907,
      "grad_norm": 0.0815284326672554,
      "learning_rate": 0.00048352471293060413,
      "loss": 0.0018,
      "step": 3300
    },
    {
      "epoch": 1.6480279580629058,
      "grad_norm": 0.06458548456430435,
      "learning_rate": 0.000483519720419371,
      "loss": 0.0014,
      "step": 3301
    },
    {
      "epoch": 1.6485272091862206,
      "grad_norm": 0.693692684173584,
      "learning_rate": 0.00048351472790813784,
      "loss": 0.0234,
      "step": 3302
    },
    {
      "epoch": 1.6490264603095357,
      "grad_norm": 0.02615007385611534,
      "learning_rate": 0.00048350973539690464,
      "loss": 0.0011,
      "step": 3303
    },
    {
      "epoch": 1.6495257114328508,
      "grad_norm": 0.21721994876861572,
      "learning_rate": 0.0004835047428856715,
      "loss": 0.0052,
      "step": 3304
    },
    {
      "epoch": 1.6500249625561656,
      "grad_norm": 0.2633894979953766,
      "learning_rate": 0.00048349975037443834,
      "loss": 0.0062,
      "step": 3305
    },
    {
      "epoch": 1.6505242136794807,
      "grad_norm": 0.25173553824424744,
      "learning_rate": 0.0004834947578632052,
      "loss": 0.0054,
      "step": 3306
    },
    {
      "epoch": 1.6510234648027957,
      "grad_norm": 0.24724525213241577,
      "learning_rate": 0.00048348976535197204,
      "loss": 0.0027,
      "step": 3307
    },
    {
      "epoch": 1.6515227159261108,
      "grad_norm": 0.38143566250801086,
      "learning_rate": 0.0004834847728407389,
      "loss": 0.0035,
      "step": 3308
    },
    {
      "epoch": 1.6520219670494258,
      "grad_norm": 0.04509466513991356,
      "learning_rate": 0.00048347978032950575,
      "loss": 0.0012,
      "step": 3309
    },
    {
      "epoch": 1.652521218172741,
      "grad_norm": 1.0066325664520264,
      "learning_rate": 0.0004834747878182726,
      "loss": 0.0143,
      "step": 3310
    },
    {
      "epoch": 1.653020469296056,
      "grad_norm": 0.20313136279582977,
      "learning_rate": 0.00048346979530703945,
      "loss": 0.0043,
      "step": 3311
    },
    {
      "epoch": 1.653519720419371,
      "grad_norm": 0.012270907871425152,
      "learning_rate": 0.0004834648027958063,
      "loss": 0.0007,
      "step": 3312
    },
    {
      "epoch": 1.654018971542686,
      "grad_norm": 0.1765051633119583,
      "learning_rate": 0.00048345981028457316,
      "loss": 0.0012,
      "step": 3313
    },
    {
      "epoch": 1.6545182226660011,
      "grad_norm": 0.01089650858193636,
      "learning_rate": 0.00048345481777334,
      "loss": 0.0005,
      "step": 3314
    },
    {
      "epoch": 1.6550174737893162,
      "grad_norm": 0.18256501853466034,
      "learning_rate": 0.00048344982526210686,
      "loss": 0.0026,
      "step": 3315
    },
    {
      "epoch": 1.655516724912631,
      "grad_norm": 0.23094025254249573,
      "learning_rate": 0.0004834448327508737,
      "loss": 0.003,
      "step": 3316
    },
    {
      "epoch": 1.656015976035946,
      "grad_norm": 0.23788683116436005,
      "learning_rate": 0.00048343984023964056,
      "loss": 0.0035,
      "step": 3317
    },
    {
      "epoch": 1.6565152271592611,
      "grad_norm": 0.04796242341399193,
      "learning_rate": 0.0004834348477284074,
      "loss": 0.0012,
      "step": 3318
    },
    {
      "epoch": 1.657014478282576,
      "grad_norm": 0.5699912309646606,
      "learning_rate": 0.00048342985521717427,
      "loss": 0.0227,
      "step": 3319
    },
    {
      "epoch": 1.657513729405891,
      "grad_norm": 0.08442627638578415,
      "learning_rate": 0.0004834248627059411,
      "loss": 0.0019,
      "step": 3320
    },
    {
      "epoch": 1.658012980529206,
      "grad_norm": 0.05086710304021835,
      "learning_rate": 0.00048341987019470797,
      "loss": 0.0012,
      "step": 3321
    },
    {
      "epoch": 1.6585122316525212,
      "grad_norm": 0.21099667251110077,
      "learning_rate": 0.0004834148776834748,
      "loss": 0.0033,
      "step": 3322
    },
    {
      "epoch": 1.6590114827758362,
      "grad_norm": 0.1965099275112152,
      "learning_rate": 0.0004834098851722417,
      "loss": 0.005,
      "step": 3323
    },
    {
      "epoch": 1.6595107338991513,
      "grad_norm": 0.23871619999408722,
      "learning_rate": 0.00048340489266100853,
      "loss": 0.0048,
      "step": 3324
    },
    {
      "epoch": 1.6600099850224663,
      "grad_norm": 0.012962354347109795,
      "learning_rate": 0.0004833999001497754,
      "loss": 0.0006,
      "step": 3325
    },
    {
      "epoch": 1.6605092361457814,
      "grad_norm": 0.019506778568029404,
      "learning_rate": 0.00048339490763854223,
      "loss": 0.0007,
      "step": 3326
    },
    {
      "epoch": 1.6610084872690964,
      "grad_norm": 0.21926698088645935,
      "learning_rate": 0.0004833899151273091,
      "loss": 0.0031,
      "step": 3327
    },
    {
      "epoch": 1.6615077383924115,
      "grad_norm": 0.27932003140449524,
      "learning_rate": 0.00048338492261607594,
      "loss": 0.0074,
      "step": 3328
    },
    {
      "epoch": 1.6620069895157266,
      "grad_norm": 0.024731511250138283,
      "learning_rate": 0.0004833799301048428,
      "loss": 0.0007,
      "step": 3329
    },
    {
      "epoch": 1.6625062406390414,
      "grad_norm": 0.13780906796455383,
      "learning_rate": 0.00048337493759360964,
      "loss": 0.0025,
      "step": 3330
    },
    {
      "epoch": 1.6630054917623565,
      "grad_norm": 1.032846212387085,
      "learning_rate": 0.0004833699450823765,
      "loss": 0.0062,
      "step": 3331
    },
    {
      "epoch": 1.6635047428856715,
      "grad_norm": 0.1620756983757019,
      "learning_rate": 0.0004833649525711433,
      "loss": 0.0028,
      "step": 3332
    },
    {
      "epoch": 1.6640039940089866,
      "grad_norm": 0.24353627860546112,
      "learning_rate": 0.00048335996005991014,
      "loss": 0.0036,
      "step": 3333
    },
    {
      "epoch": 1.6645032451323014,
      "grad_norm": 0.250837504863739,
      "learning_rate": 0.000483354967548677,
      "loss": 0.0043,
      "step": 3334
    },
    {
      "epoch": 1.6650024962556165,
      "grad_norm": 0.011244842782616615,
      "learning_rate": 0.00048334997503744385,
      "loss": 0.0007,
      "step": 3335
    },
    {
      "epoch": 1.6655017473789315,
      "grad_norm": 0.16847772896289825,
      "learning_rate": 0.0004833449825262107,
      "loss": 0.002,
      "step": 3336
    },
    {
      "epoch": 1.6660009985022466,
      "grad_norm": 0.8361971378326416,
      "learning_rate": 0.00048333999001497755,
      "loss": 0.0173,
      "step": 3337
    },
    {
      "epoch": 1.6665002496255616,
      "grad_norm": 0.22557665407657623,
      "learning_rate": 0.0004833349975037444,
      "loss": 0.0041,
      "step": 3338
    },
    {
      "epoch": 1.6669995007488767,
      "grad_norm": 0.0372493602335453,
      "learning_rate": 0.00048333000499251126,
      "loss": 0.001,
      "step": 3339
    },
    {
      "epoch": 1.6674987518721918,
      "grad_norm": 0.22316378355026245,
      "learning_rate": 0.0004833250124812781,
      "loss": 0.003,
      "step": 3340
    },
    {
      "epoch": 1.6679980029955068,
      "grad_norm": 0.07585792988538742,
      "learning_rate": 0.00048332001997004496,
      "loss": 0.0012,
      "step": 3341
    },
    {
      "epoch": 1.6684972541188219,
      "grad_norm": 0.13096605241298676,
      "learning_rate": 0.00048331502745881176,
      "loss": 0.0019,
      "step": 3342
    },
    {
      "epoch": 1.668996505242137,
      "grad_norm": 0.23055456578731537,
      "learning_rate": 0.0004833100349475786,
      "loss": 0.0023,
      "step": 3343
    },
    {
      "epoch": 1.6694957563654518,
      "grad_norm": 0.35457611083984375,
      "learning_rate": 0.00048330504243634546,
      "loss": 0.0108,
      "step": 3344
    },
    {
      "epoch": 1.6699950074887668,
      "grad_norm": 0.5191398859024048,
      "learning_rate": 0.0004833000499251123,
      "loss": 0.0082,
      "step": 3345
    },
    {
      "epoch": 1.670494258612082,
      "grad_norm": 0.077799491584301,
      "learning_rate": 0.00048329505741387917,
      "loss": 0.0015,
      "step": 3346
    },
    {
      "epoch": 1.670993509735397,
      "grad_norm": 0.3056051731109619,
      "learning_rate": 0.000483290064902646,
      "loss": 0.0029,
      "step": 3347
    },
    {
      "epoch": 1.6714927608587118,
      "grad_norm": 0.18063074350357056,
      "learning_rate": 0.00048328507239141287,
      "loss": 0.0024,
      "step": 3348
    },
    {
      "epoch": 1.6719920119820268,
      "grad_norm": 0.24300411343574524,
      "learning_rate": 0.0004832800798801797,
      "loss": 0.004,
      "step": 3349
    },
    {
      "epoch": 1.672491263105342,
      "grad_norm": 0.007480504922568798,
      "learning_rate": 0.0004832750873689466,
      "loss": 0.0006,
      "step": 3350
    },
    {
      "epoch": 1.672990514228657,
      "grad_norm": 0.02400556392967701,
      "learning_rate": 0.0004832700948577134,
      "loss": 0.0009,
      "step": 3351
    },
    {
      "epoch": 1.673489765351972,
      "grad_norm": 0.023020749911665916,
      "learning_rate": 0.0004832651023464803,
      "loss": 0.0009,
      "step": 3352
    },
    {
      "epoch": 1.673989016475287,
      "grad_norm": 1.1544098854064941,
      "learning_rate": 0.00048326010983524713,
      "loss": 0.0104,
      "step": 3353
    },
    {
      "epoch": 1.6744882675986021,
      "grad_norm": 0.18053385615348816,
      "learning_rate": 0.000483255117324014,
      "loss": 0.0019,
      "step": 3354
    },
    {
      "epoch": 1.6749875187219172,
      "grad_norm": 0.05671953409910202,
      "learning_rate": 0.00048325012481278083,
      "loss": 0.0015,
      "step": 3355
    },
    {
      "epoch": 1.6754867698452323,
      "grad_norm": 0.024601586163043976,
      "learning_rate": 0.0004832451323015477,
      "loss": 0.001,
      "step": 3356
    },
    {
      "epoch": 1.6759860209685473,
      "grad_norm": 0.45052865147590637,
      "learning_rate": 0.00048324013979031454,
      "loss": 0.0064,
      "step": 3357
    },
    {
      "epoch": 1.6764852720918622,
      "grad_norm": 0.43360596895217896,
      "learning_rate": 0.0004832351472790814,
      "loss": 0.0077,
      "step": 3358
    },
    {
      "epoch": 1.6769845232151772,
      "grad_norm": 0.11621850728988647,
      "learning_rate": 0.00048323015476784824,
      "loss": 0.0025,
      "step": 3359
    },
    {
      "epoch": 1.6774837743384923,
      "grad_norm": 0.4002217650413513,
      "learning_rate": 0.0004832251622566151,
      "loss": 0.008,
      "step": 3360
    },
    {
      "epoch": 1.6779830254618073,
      "grad_norm": 0.8152862787246704,
      "learning_rate": 0.0004832201697453819,
      "loss": 0.0074,
      "step": 3361
    },
    {
      "epoch": 1.6784822765851222,
      "grad_norm": 0.43459370732307434,
      "learning_rate": 0.00048321517723414874,
      "loss": 0.0194,
      "step": 3362
    },
    {
      "epoch": 1.6789815277084372,
      "grad_norm": 0.13287164270877838,
      "learning_rate": 0.0004832101847229156,
      "loss": 0.002,
      "step": 3363
    },
    {
      "epoch": 1.6794807788317523,
      "grad_norm": 0.0397910550236702,
      "learning_rate": 0.00048320519221168245,
      "loss": 0.0012,
      "step": 3364
    },
    {
      "epoch": 1.6799800299550673,
      "grad_norm": 0.04481867700815201,
      "learning_rate": 0.0004832001997004493,
      "loss": 0.0012,
      "step": 3365
    },
    {
      "epoch": 1.6804792810783824,
      "grad_norm": 0.34873315691947937,
      "learning_rate": 0.00048319520718921615,
      "loss": 0.0067,
      "step": 3366
    },
    {
      "epoch": 1.6809785322016975,
      "grad_norm": 0.2810230851173401,
      "learning_rate": 0.000483190214677983,
      "loss": 0.0048,
      "step": 3367
    },
    {
      "epoch": 1.6814777833250125,
      "grad_norm": 0.16850169003009796,
      "learning_rate": 0.00048318522216674986,
      "loss": 0.0022,
      "step": 3368
    },
    {
      "epoch": 1.6819770344483276,
      "grad_norm": 0.9129940867424011,
      "learning_rate": 0.0004831802296555167,
      "loss": 0.0083,
      "step": 3369
    },
    {
      "epoch": 1.6824762855716426,
      "grad_norm": 0.11414743214845657,
      "learning_rate": 0.00048317523714428356,
      "loss": 0.0016,
      "step": 3370
    },
    {
      "epoch": 1.6829755366949577,
      "grad_norm": 0.08808156847953796,
      "learning_rate": 0.0004831702446330504,
      "loss": 0.001,
      "step": 3371
    },
    {
      "epoch": 1.6834747878182728,
      "grad_norm": 0.3349468410015106,
      "learning_rate": 0.00048316525212181727,
      "loss": 0.0043,
      "step": 3372
    },
    {
      "epoch": 1.6839740389415876,
      "grad_norm": 0.14096687734127045,
      "learning_rate": 0.0004831602596105841,
      "loss": 0.0031,
      "step": 3373
    },
    {
      "epoch": 1.6844732900649027,
      "grad_norm": 0.05611168220639229,
      "learning_rate": 0.00048315526709935097,
      "loss": 0.0012,
      "step": 3374
    },
    {
      "epoch": 1.6849725411882177,
      "grad_norm": 0.2603493630886078,
      "learning_rate": 0.0004831502745881178,
      "loss": 0.0026,
      "step": 3375
    },
    {
      "epoch": 1.6854717923115325,
      "grad_norm": 0.4916779398918152,
      "learning_rate": 0.0004831452820768847,
      "loss": 0.0091,
      "step": 3376
    },
    {
      "epoch": 1.6859710434348476,
      "grad_norm": 0.5601115226745605,
      "learning_rate": 0.0004831402895656515,
      "loss": 0.0091,
      "step": 3377
    },
    {
      "epoch": 1.6864702945581627,
      "grad_norm": 0.15321269631385803,
      "learning_rate": 0.0004831352970544184,
      "loss": 0.0037,
      "step": 3378
    },
    {
      "epoch": 1.6869695456814777,
      "grad_norm": 0.3415844142436981,
      "learning_rate": 0.00048313030454318523,
      "loss": 0.0123,
      "step": 3379
    },
    {
      "epoch": 1.6874687968047928,
      "grad_norm": 0.04304482042789459,
      "learning_rate": 0.0004831253120319521,
      "loss": 0.0012,
      "step": 3380
    },
    {
      "epoch": 1.6879680479281078,
      "grad_norm": 0.7048689723014832,
      "learning_rate": 0.00048312031952071893,
      "loss": 0.019,
      "step": 3381
    },
    {
      "epoch": 1.688467299051423,
      "grad_norm": 0.1016528308391571,
      "learning_rate": 0.0004831153270094858,
      "loss": 0.002,
      "step": 3382
    },
    {
      "epoch": 1.688966550174738,
      "grad_norm": 0.03605467453598976,
      "learning_rate": 0.00048311033449825264,
      "loss": 0.0011,
      "step": 3383
    },
    {
      "epoch": 1.689465801298053,
      "grad_norm": 0.40150606632232666,
      "learning_rate": 0.0004831053419870195,
      "loss": 0.0066,
      "step": 3384
    },
    {
      "epoch": 1.689965052421368,
      "grad_norm": 0.7874435186386108,
      "learning_rate": 0.00048310034947578634,
      "loss": 0.0144,
      "step": 3385
    },
    {
      "epoch": 1.6904643035446831,
      "grad_norm": 0.2112426906824112,
      "learning_rate": 0.0004830953569645532,
      "loss": 0.0041,
      "step": 3386
    },
    {
      "epoch": 1.690963554667998,
      "grad_norm": 0.07921554893255234,
      "learning_rate": 0.00048309036445332005,
      "loss": 0.0017,
      "step": 3387
    },
    {
      "epoch": 1.691462805791313,
      "grad_norm": 0.1172865480184555,
      "learning_rate": 0.0004830853719420869,
      "loss": 0.0019,
      "step": 3388
    },
    {
      "epoch": 1.691962056914628,
      "grad_norm": 0.5030819773674011,
      "learning_rate": 0.00048308037943085375,
      "loss": 0.0186,
      "step": 3389
    },
    {
      "epoch": 1.692461308037943,
      "grad_norm": 0.20008370280265808,
      "learning_rate": 0.00048307538691962055,
      "loss": 0.0035,
      "step": 3390
    },
    {
      "epoch": 1.692960559161258,
      "grad_norm": 0.3972465693950653,
      "learning_rate": 0.0004830703944083874,
      "loss": 0.008,
      "step": 3391
    },
    {
      "epoch": 1.693459810284573,
      "grad_norm": 0.18102841079235077,
      "learning_rate": 0.00048306540189715425,
      "loss": 0.0086,
      "step": 3392
    },
    {
      "epoch": 1.693959061407888,
      "grad_norm": 0.3028675317764282,
      "learning_rate": 0.0004830604093859211,
      "loss": 0.0069,
      "step": 3393
    },
    {
      "epoch": 1.6944583125312032,
      "grad_norm": 0.029178408905863762,
      "learning_rate": 0.00048305541687468796,
      "loss": 0.0012,
      "step": 3394
    },
    {
      "epoch": 1.6949575636545182,
      "grad_norm": 0.35754668712615967,
      "learning_rate": 0.0004830504243634548,
      "loss": 0.0062,
      "step": 3395
    },
    {
      "epoch": 1.6954568147778333,
      "grad_norm": 0.07290636003017426,
      "learning_rate": 0.00048304543185222166,
      "loss": 0.0015,
      "step": 3396
    },
    {
      "epoch": 1.6959560659011483,
      "grad_norm": 0.4071192145347595,
      "learning_rate": 0.0004830404393409885,
      "loss": 0.0044,
      "step": 3397
    },
    {
      "epoch": 1.6964553170244634,
      "grad_norm": 0.36752942204475403,
      "learning_rate": 0.00048303544682975536,
      "loss": 0.0075,
      "step": 3398
    },
    {
      "epoch": 1.6969545681477785,
      "grad_norm": 0.30199894309043884,
      "learning_rate": 0.0004830304543185222,
      "loss": 0.0046,
      "step": 3399
    },
    {
      "epoch": 1.6974538192710935,
      "grad_norm": 0.048493891954422,
      "learning_rate": 0.00048302546180728907,
      "loss": 0.0008,
      "step": 3400
    },
    {
      "epoch": 1.6979530703944083,
      "grad_norm": 0.5757771134376526,
      "learning_rate": 0.0004830204692960559,
      "loss": 0.003,
      "step": 3401
    },
    {
      "epoch": 1.6984523215177234,
      "grad_norm": 0.11609406024217606,
      "learning_rate": 0.0004830154767848228,
      "loss": 0.0014,
      "step": 3402
    },
    {
      "epoch": 1.6989515726410385,
      "grad_norm": 0.15458586812019348,
      "learning_rate": 0.0004830104842735896,
      "loss": 0.0015,
      "step": 3403
    },
    {
      "epoch": 1.6994508237643535,
      "grad_norm": 0.02892559953033924,
      "learning_rate": 0.0004830054917623565,
      "loss": 0.001,
      "step": 3404
    },
    {
      "epoch": 1.6999500748876684,
      "grad_norm": 0.207875594496727,
      "learning_rate": 0.00048300049925112333,
      "loss": 0.0134,
      "step": 3405
    },
    {
      "epoch": 1.7004493260109834,
      "grad_norm": 0.03753530979156494,
      "learning_rate": 0.0004829955067398902,
      "loss": 0.0009,
      "step": 3406
    },
    {
      "epoch": 1.7009485771342985,
      "grad_norm": 0.40878966450691223,
      "learning_rate": 0.00048299051422865703,
      "loss": 0.0036,
      "step": 3407
    },
    {
      "epoch": 1.7014478282576135,
      "grad_norm": 0.9895220398902893,
      "learning_rate": 0.0004829855217174239,
      "loss": 0.0152,
      "step": 3408
    },
    {
      "epoch": 1.7019470793809286,
      "grad_norm": 0.017441347241401672,
      "learning_rate": 0.00048298052920619074,
      "loss": 0.0009,
      "step": 3409
    },
    {
      "epoch": 1.7024463305042437,
      "grad_norm": 1.1089414358139038,
      "learning_rate": 0.0004829755366949576,
      "loss": 0.0102,
      "step": 3410
    },
    {
      "epoch": 1.7029455816275587,
      "grad_norm": 0.01826513558626175,
      "learning_rate": 0.00048297054418372444,
      "loss": 0.0009,
      "step": 3411
    },
    {
      "epoch": 1.7034448327508738,
      "grad_norm": 0.3087866008281708,
      "learning_rate": 0.0004829655516724913,
      "loss": 0.0116,
      "step": 3412
    },
    {
      "epoch": 1.7039440838741888,
      "grad_norm": 0.25956404209136963,
      "learning_rate": 0.00048296055916125815,
      "loss": 0.0055,
      "step": 3413
    },
    {
      "epoch": 1.704443334997504,
      "grad_norm": 0.05059356987476349,
      "learning_rate": 0.000482955566650025,
      "loss": 0.0014,
      "step": 3414
    },
    {
      "epoch": 1.7049425861208187,
      "grad_norm": 0.1739291548728943,
      "learning_rate": 0.00048295057413879185,
      "loss": 0.0026,
      "step": 3415
    },
    {
      "epoch": 1.7054418372441338,
      "grad_norm": 0.12917549908161163,
      "learning_rate": 0.0004829455816275587,
      "loss": 0.0011,
      "step": 3416
    },
    {
      "epoch": 1.7059410883674488,
      "grad_norm": 0.10775800794363022,
      "learning_rate": 0.00048294058911632555,
      "loss": 0.0023,
      "step": 3417
    },
    {
      "epoch": 1.706440339490764,
      "grad_norm": 1.6914530992507935,
      "learning_rate": 0.0004829355966050924,
      "loss": 0.0246,
      "step": 3418
    },
    {
      "epoch": 1.7069395906140787,
      "grad_norm": 1.1817883253097534,
      "learning_rate": 0.0004829306040938592,
      "loss": 0.0211,
      "step": 3419
    },
    {
      "epoch": 1.7074388417373938,
      "grad_norm": 1.4054359197616577,
      "learning_rate": 0.00048292561158262606,
      "loss": 0.0204,
      "step": 3420
    },
    {
      "epoch": 1.7079380928607089,
      "grad_norm": 0.29873791337013245,
      "learning_rate": 0.0004829206190713929,
      "loss": 0.0026,
      "step": 3421
    },
    {
      "epoch": 1.708437343984024,
      "grad_norm": 0.35667258501052856,
      "learning_rate": 0.00048291562656015976,
      "loss": 0.0081,
      "step": 3422
    },
    {
      "epoch": 1.708936595107339,
      "grad_norm": 0.06580795347690582,
      "learning_rate": 0.0004829106340489266,
      "loss": 0.0013,
      "step": 3423
    },
    {
      "epoch": 1.709435846230654,
      "grad_norm": 0.8987721800804138,
      "learning_rate": 0.00048290564153769346,
      "loss": 0.0148,
      "step": 3424
    },
    {
      "epoch": 1.709935097353969,
      "grad_norm": 0.6949543356895447,
      "learning_rate": 0.0004829006490264603,
      "loss": 0.0088,
      "step": 3425
    },
    {
      "epoch": 1.7104343484772841,
      "grad_norm": 0.3378896415233612,
      "learning_rate": 0.00048289565651522717,
      "loss": 0.0037,
      "step": 3426
    },
    {
      "epoch": 1.7109335996005992,
      "grad_norm": 0.7793383598327637,
      "learning_rate": 0.000482890664003994,
      "loss": 0.0225,
      "step": 3427
    },
    {
      "epoch": 1.7114328507239143,
      "grad_norm": 0.4439999461174011,
      "learning_rate": 0.00048288567149276087,
      "loss": 0.004,
      "step": 3428
    },
    {
      "epoch": 1.711932101847229,
      "grad_norm": 0.5129654407501221,
      "learning_rate": 0.0004828806789815277,
      "loss": 0.0122,
      "step": 3429
    },
    {
      "epoch": 1.7124313529705442,
      "grad_norm": 0.32476308941841125,
      "learning_rate": 0.0004828756864702946,
      "loss": 0.0052,
      "step": 3430
    },
    {
      "epoch": 1.7129306040938592,
      "grad_norm": 0.03311154246330261,
      "learning_rate": 0.00048287069395906143,
      "loss": 0.0011,
      "step": 3431
    },
    {
      "epoch": 1.7134298552171743,
      "grad_norm": 0.32075825333595276,
      "learning_rate": 0.0004828657014478283,
      "loss": 0.0033,
      "step": 3432
    },
    {
      "epoch": 1.7139291063404891,
      "grad_norm": 0.32678520679473877,
      "learning_rate": 0.00048286070893659513,
      "loss": 0.0023,
      "step": 3433
    },
    {
      "epoch": 1.7144283574638042,
      "grad_norm": 1.0364445447921753,
      "learning_rate": 0.000482855716425362,
      "loss": 0.0127,
      "step": 3434
    },
    {
      "epoch": 1.7149276085871192,
      "grad_norm": 0.6711345314979553,
      "learning_rate": 0.00048285072391412884,
      "loss": 0.0101,
      "step": 3435
    },
    {
      "epoch": 1.7154268597104343,
      "grad_norm": 0.24748843908309937,
      "learning_rate": 0.0004828457314028957,
      "loss": 0.003,
      "step": 3436
    },
    {
      "epoch": 1.7159261108337494,
      "grad_norm": 3.193093776702881,
      "learning_rate": 0.00048284073889166254,
      "loss": 0.0375,
      "step": 3437
    },
    {
      "epoch": 1.7164253619570644,
      "grad_norm": 1.4174776077270508,
      "learning_rate": 0.0004828357463804294,
      "loss": 0.0103,
      "step": 3438
    },
    {
      "epoch": 1.7169246130803795,
      "grad_norm": 0.3090221881866455,
      "learning_rate": 0.00048283075386919625,
      "loss": 0.0039,
      "step": 3439
    },
    {
      "epoch": 1.7174238642036945,
      "grad_norm": 0.57765132188797,
      "learning_rate": 0.0004828257613579631,
      "loss": 0.0096,
      "step": 3440
    },
    {
      "epoch": 1.7179231153270096,
      "grad_norm": 0.3345479965209961,
      "learning_rate": 0.00048282076884672995,
      "loss": 0.0037,
      "step": 3441
    },
    {
      "epoch": 1.7184223664503246,
      "grad_norm": 0.06808779388666153,
      "learning_rate": 0.0004828157763354968,
      "loss": 0.0015,
      "step": 3442
    },
    {
      "epoch": 1.7189216175736397,
      "grad_norm": 1.4465314149856567,
      "learning_rate": 0.00048281078382426365,
      "loss": 0.0293,
      "step": 3443
    },
    {
      "epoch": 1.7194208686969545,
      "grad_norm": 0.5353755354881287,
      "learning_rate": 0.0004828057913130305,
      "loss": 0.0108,
      "step": 3444
    },
    {
      "epoch": 1.7199201198202696,
      "grad_norm": 0.34662917256355286,
      "learning_rate": 0.00048280079880179736,
      "loss": 0.0065,
      "step": 3445
    },
    {
      "epoch": 1.7204193709435847,
      "grad_norm": 1.4334826469421387,
      "learning_rate": 0.0004827958062905642,
      "loss": 0.05,
      "step": 3446
    },
    {
      "epoch": 1.7209186220668995,
      "grad_norm": 0.9459741711616516,
      "learning_rate": 0.00048279081377933106,
      "loss": 0.0215,
      "step": 3447
    },
    {
      "epoch": 1.7214178731902146,
      "grad_norm": 0.05528799071907997,
      "learning_rate": 0.00048278582126809786,
      "loss": 0.0014,
      "step": 3448
    },
    {
      "epoch": 1.7219171243135296,
      "grad_norm": 0.9021728038787842,
      "learning_rate": 0.0004827808287568647,
      "loss": 0.0147,
      "step": 3449
    },
    {
      "epoch": 1.7224163754368447,
      "grad_norm": 0.16816067695617676,
      "learning_rate": 0.00048277583624563156,
      "loss": 0.0029,
      "step": 3450
    },
    {
      "epoch": 1.7229156265601597,
      "grad_norm": 0.6587756276130676,
      "learning_rate": 0.0004827708437343984,
      "loss": 0.0313,
      "step": 3451
    },
    {
      "epoch": 1.7234148776834748,
      "grad_norm": 0.27252712845802307,
      "learning_rate": 0.00048276585122316527,
      "loss": 0.0066,
      "step": 3452
    },
    {
      "epoch": 1.7239141288067898,
      "grad_norm": 0.3558976948261261,
      "learning_rate": 0.0004827608587119321,
      "loss": 0.0054,
      "step": 3453
    },
    {
      "epoch": 1.724413379930105,
      "grad_norm": 0.6018184423446655,
      "learning_rate": 0.00048275586620069897,
      "loss": 0.0193,
      "step": 3454
    },
    {
      "epoch": 1.72491263105342,
      "grad_norm": 0.43794146180152893,
      "learning_rate": 0.00048275087368946577,
      "loss": 0.0117,
      "step": 3455
    },
    {
      "epoch": 1.725411882176735,
      "grad_norm": 0.12040787935256958,
      "learning_rate": 0.0004827458811782326,
      "loss": 0.0033,
      "step": 3456
    },
    {
      "epoch": 1.72591113330005,
      "grad_norm": 0.13667067885398865,
      "learning_rate": 0.0004827408886669995,
      "loss": 0.0041,
      "step": 3457
    },
    {
      "epoch": 1.726410384423365,
      "grad_norm": 0.8529567718505859,
      "learning_rate": 0.0004827358961557663,
      "loss": 0.0215,
      "step": 3458
    },
    {
      "epoch": 1.72690963554668,
      "grad_norm": 0.8801934719085693,
      "learning_rate": 0.0004827309036445332,
      "loss": 0.0367,
      "step": 3459
    },
    {
      "epoch": 1.727408886669995,
      "grad_norm": 0.4826783835887909,
      "learning_rate": 0.00048272591113330003,
      "loss": 0.0068,
      "step": 3460
    },
    {
      "epoch": 1.7279081377933099,
      "grad_norm": 0.24042931199073792,
      "learning_rate": 0.0004827209186220669,
      "loss": 0.0071,
      "step": 3461
    },
    {
      "epoch": 1.728407388916625,
      "grad_norm": 0.26464468240737915,
      "learning_rate": 0.00048271592611083373,
      "loss": 0.0046,
      "step": 3462
    },
    {
      "epoch": 1.72890664003994,
      "grad_norm": 0.7368936538696289,
      "learning_rate": 0.0004827109335996006,
      "loss": 0.0214,
      "step": 3463
    },
    {
      "epoch": 1.729405891163255,
      "grad_norm": 0.21602162718772888,
      "learning_rate": 0.00048270594108836744,
      "loss": 0.0041,
      "step": 3464
    },
    {
      "epoch": 1.72990514228657,
      "grad_norm": 0.22008752822875977,
      "learning_rate": 0.0004827009485771343,
      "loss": 0.0072,
      "step": 3465
    },
    {
      "epoch": 1.7304043934098852,
      "grad_norm": 0.2553321123123169,
      "learning_rate": 0.00048269595606590114,
      "loss": 0.0063,
      "step": 3466
    },
    {
      "epoch": 1.7309036445332002,
      "grad_norm": 0.6772376298904419,
      "learning_rate": 0.000482690963554668,
      "loss": 0.0113,
      "step": 3467
    },
    {
      "epoch": 1.7314028956565153,
      "grad_norm": 0.4074811637401581,
      "learning_rate": 0.00048268597104343485,
      "loss": 0.0085,
      "step": 3468
    },
    {
      "epoch": 1.7319021467798303,
      "grad_norm": 0.7105661034584045,
      "learning_rate": 0.0004826809785322017,
      "loss": 0.0504,
      "step": 3469
    },
    {
      "epoch": 1.7324013979031454,
      "grad_norm": 1.0628063678741455,
      "learning_rate": 0.00048267598602096855,
      "loss": 0.0242,
      "step": 3470
    },
    {
      "epoch": 1.7329006490264605,
      "grad_norm": 0.18804386258125305,
      "learning_rate": 0.0004826709935097354,
      "loss": 0.0039,
      "step": 3471
    },
    {
      "epoch": 1.7333999001497753,
      "grad_norm": 0.05936892703175545,
      "learning_rate": 0.00048266600099850226,
      "loss": 0.0019,
      "step": 3472
    },
    {
      "epoch": 1.7338991512730904,
      "grad_norm": 0.25149375200271606,
      "learning_rate": 0.0004826610084872691,
      "loss": 0.006,
      "step": 3473
    },
    {
      "epoch": 1.7343984023964054,
      "grad_norm": 0.21119526028633118,
      "learning_rate": 0.00048265601597603596,
      "loss": 0.0047,
      "step": 3474
    },
    {
      "epoch": 1.7348976535197205,
      "grad_norm": 0.5953909754753113,
      "learning_rate": 0.0004826510234648028,
      "loss": 0.0142,
      "step": 3475
    },
    {
      "epoch": 1.7353969046430353,
      "grad_norm": 0.25251659750938416,
      "learning_rate": 0.00048264603095356966,
      "loss": 0.0042,
      "step": 3476
    },
    {
      "epoch": 1.7358961557663504,
      "grad_norm": 0.3282124400138855,
      "learning_rate": 0.0004826410384423365,
      "loss": 0.0115,
      "step": 3477
    },
    {
      "epoch": 1.7363954068896654,
      "grad_norm": 0.3364104926586151,
      "learning_rate": 0.0004826360459311033,
      "loss": 0.0123,
      "step": 3478
    },
    {
      "epoch": 1.7368946580129805,
      "grad_norm": 0.5669151544570923,
      "learning_rate": 0.00048263105341987017,
      "loss": 0.0149,
      "step": 3479
    },
    {
      "epoch": 1.7373939091362955,
      "grad_norm": 1.3634650707244873,
      "learning_rate": 0.000482626060908637,
      "loss": 0.0172,
      "step": 3480
    },
    {
      "epoch": 1.7378931602596106,
      "grad_norm": 0.14459030330181122,
      "learning_rate": 0.00048262106839740387,
      "loss": 0.0032,
      "step": 3481
    },
    {
      "epoch": 1.7383924113829257,
      "grad_norm": 0.7301414608955383,
      "learning_rate": 0.0004826160758861707,
      "loss": 0.0083,
      "step": 3482
    },
    {
      "epoch": 1.7388916625062407,
      "grad_norm": 0.19382402300834656,
      "learning_rate": 0.0004826110833749376,
      "loss": 0.003,
      "step": 3483
    },
    {
      "epoch": 1.7393909136295558,
      "grad_norm": 0.5691643953323364,
      "learning_rate": 0.0004826060908637044,
      "loss": 0.0072,
      "step": 3484
    },
    {
      "epoch": 1.7398901647528708,
      "grad_norm": 0.38263872265815735,
      "learning_rate": 0.0004826010983524713,
      "loss": 0.0061,
      "step": 3485
    },
    {
      "epoch": 1.7403894158761857,
      "grad_norm": 0.1972084492444992,
      "learning_rate": 0.00048259610584123813,
      "loss": 0.0031,
      "step": 3486
    },
    {
      "epoch": 1.7408886669995007,
      "grad_norm": 0.03782227635383606,
      "learning_rate": 0.000482591113330005,
      "loss": 0.0018,
      "step": 3487
    },
    {
      "epoch": 1.7413879181228158,
      "grad_norm": 0.5377028584480286,
      "learning_rate": 0.00048258612081877183,
      "loss": 0.0342,
      "step": 3488
    },
    {
      "epoch": 1.7418871692461309,
      "grad_norm": 0.0373641774058342,
      "learning_rate": 0.0004825811283075387,
      "loss": 0.0017,
      "step": 3489
    },
    {
      "epoch": 1.7423864203694457,
      "grad_norm": 0.07633092999458313,
      "learning_rate": 0.00048257613579630554,
      "loss": 0.0024,
      "step": 3490
    },
    {
      "epoch": 1.7428856714927607,
      "grad_norm": 0.11405368894338608,
      "learning_rate": 0.0004825711432850724,
      "loss": 0.0023,
      "step": 3491
    },
    {
      "epoch": 1.7433849226160758,
      "grad_norm": 0.06774968653917313,
      "learning_rate": 0.00048256615077383924,
      "loss": 0.0016,
      "step": 3492
    },
    {
      "epoch": 1.7438841737393909,
      "grad_norm": 1.902083158493042,
      "learning_rate": 0.0004825611582626061,
      "loss": 0.0692,
      "step": 3493
    },
    {
      "epoch": 1.744383424862706,
      "grad_norm": 0.051478657871484756,
      "learning_rate": 0.00048255616575137295,
      "loss": 0.0018,
      "step": 3494
    },
    {
      "epoch": 1.744882675986021,
      "grad_norm": 0.18147726356983185,
      "learning_rate": 0.0004825511732401398,
      "loss": 0.0032,
      "step": 3495
    },
    {
      "epoch": 1.745381927109336,
      "grad_norm": 0.7325614094734192,
      "learning_rate": 0.00048254618072890665,
      "loss": 0.0072,
      "step": 3496
    },
    {
      "epoch": 1.745881178232651,
      "grad_norm": 0.22063516080379486,
      "learning_rate": 0.0004825411882176735,
      "loss": 0.0037,
      "step": 3497
    },
    {
      "epoch": 1.7463804293559662,
      "grad_norm": 0.5410239696502686,
      "learning_rate": 0.00048253619570644035,
      "loss": 0.0219,
      "step": 3498
    },
    {
      "epoch": 1.7468796804792812,
      "grad_norm": 0.2814314365386963,
      "learning_rate": 0.0004825312031952072,
      "loss": 0.005,
      "step": 3499
    },
    {
      "epoch": 1.747378931602596,
      "grad_norm": 0.33690664172172546,
      "learning_rate": 0.00048252621068397406,
      "loss": 0.007,
      "step": 3500
    },
    {
      "epoch": 1.7478781827259111,
      "grad_norm": 0.023491786792874336,
      "learning_rate": 0.0004825212181727409,
      "loss": 0.0014,
      "step": 3501
    },
    {
      "epoch": 1.7483774338492262,
      "grad_norm": 0.06261978298425674,
      "learning_rate": 0.00048251622566150776,
      "loss": 0.0024,
      "step": 3502
    },
    {
      "epoch": 1.7488766849725412,
      "grad_norm": 0.2812567353248596,
      "learning_rate": 0.0004825112331502746,
      "loss": 0.0059,
      "step": 3503
    },
    {
      "epoch": 1.749375936095856,
      "grad_norm": 0.4817391037940979,
      "learning_rate": 0.00048250624063904147,
      "loss": 0.0206,
      "step": 3504
    },
    {
      "epoch": 1.7498751872191711,
      "grad_norm": 0.08120684325695038,
      "learning_rate": 0.0004825012481278083,
      "loss": 0.0026,
      "step": 3505
    },
    {
      "epoch": 1.7503744383424862,
      "grad_norm": 0.2242264300584793,
      "learning_rate": 0.00048249625561657517,
      "loss": 0.004,
      "step": 3506
    },
    {
      "epoch": 1.7508736894658012,
      "grad_norm": 0.04028528183698654,
      "learning_rate": 0.00048249126310534197,
      "loss": 0.0013,
      "step": 3507
    },
    {
      "epoch": 1.7513729405891163,
      "grad_norm": 0.14928191900253296,
      "learning_rate": 0.0004824862705941088,
      "loss": 0.0037,
      "step": 3508
    },
    {
      "epoch": 1.7518721917124314,
      "grad_norm": 0.3553072214126587,
      "learning_rate": 0.0004824812780828757,
      "loss": 0.0038,
      "step": 3509
    },
    {
      "epoch": 1.7523714428357464,
      "grad_norm": 0.06654419004917145,
      "learning_rate": 0.0004824762855716425,
      "loss": 0.0013,
      "step": 3510
    },
    {
      "epoch": 1.7528706939590615,
      "grad_norm": 0.9414671659469604,
      "learning_rate": 0.0004824712930604094,
      "loss": 0.0326,
      "step": 3511
    },
    {
      "epoch": 1.7533699450823765,
      "grad_norm": 0.06439688801765442,
      "learning_rate": 0.00048246630054917623,
      "loss": 0.0021,
      "step": 3512
    },
    {
      "epoch": 1.7538691962056916,
      "grad_norm": 0.18409162759780884,
      "learning_rate": 0.0004824613080379431,
      "loss": 0.0046,
      "step": 3513
    },
    {
      "epoch": 1.7543684473290067,
      "grad_norm": 0.2874443829059601,
      "learning_rate": 0.00048245631552670993,
      "loss": 0.0063,
      "step": 3514
    },
    {
      "epoch": 1.7548676984523215,
      "grad_norm": 0.12731271982192993,
      "learning_rate": 0.0004824513230154768,
      "loss": 0.0022,
      "step": 3515
    },
    {
      "epoch": 1.7553669495756365,
      "grad_norm": 0.360567182302475,
      "learning_rate": 0.00048244633050424364,
      "loss": 0.0043,
      "step": 3516
    },
    {
      "epoch": 1.7558662006989516,
      "grad_norm": 0.1568647027015686,
      "learning_rate": 0.0004824413379930105,
      "loss": 0.0031,
      "step": 3517
    },
    {
      "epoch": 1.7563654518222664,
      "grad_norm": 0.4531259834766388,
      "learning_rate": 0.00048243634548177734,
      "loss": 0.0041,
      "step": 3518
    },
    {
      "epoch": 1.7568647029455815,
      "grad_norm": 0.23002669215202332,
      "learning_rate": 0.0004824313529705442,
      "loss": 0.0027,
      "step": 3519
    },
    {
      "epoch": 1.7573639540688966,
      "grad_norm": 0.6295456290245056,
      "learning_rate": 0.00048242636045931105,
      "loss": 0.0131,
      "step": 3520
    },
    {
      "epoch": 1.7578632051922116,
      "grad_norm": 0.513906717300415,
      "learning_rate": 0.0004824213679480779,
      "loss": 0.0053,
      "step": 3521
    },
    {
      "epoch": 1.7583624563155267,
      "grad_norm": 1.2652047872543335,
      "learning_rate": 0.00048241637543684475,
      "loss": 0.0174,
      "step": 3522
    },
    {
      "epoch": 1.7588617074388417,
      "grad_norm": 0.06795110553503036,
      "learning_rate": 0.0004824113829256116,
      "loss": 0.0015,
      "step": 3523
    },
    {
      "epoch": 1.7593609585621568,
      "grad_norm": 0.013509209267795086,
      "learning_rate": 0.00048240639041437845,
      "loss": 0.0008,
      "step": 3524
    },
    {
      "epoch": 1.7598602096854719,
      "grad_norm": 0.2891157865524292,
      "learning_rate": 0.0004824013979031453,
      "loss": 0.0033,
      "step": 3525
    },
    {
      "epoch": 1.760359460808787,
      "grad_norm": 0.14695346355438232,
      "learning_rate": 0.00048239640539191216,
      "loss": 0.0025,
      "step": 3526
    },
    {
      "epoch": 1.760858711932102,
      "grad_norm": 0.09513213485479355,
      "learning_rate": 0.000482391412880679,
      "loss": 0.0016,
      "step": 3527
    },
    {
      "epoch": 1.761357963055417,
      "grad_norm": 0.8824360966682434,
      "learning_rate": 0.00048238642036944586,
      "loss": 0.0359,
      "step": 3528
    },
    {
      "epoch": 1.7618572141787319,
      "grad_norm": 0.05445869266986847,
      "learning_rate": 0.0004823814278582127,
      "loss": 0.0015,
      "step": 3529
    },
    {
      "epoch": 1.762356465302047,
      "grad_norm": 0.1603851318359375,
      "learning_rate": 0.00048237643534697957,
      "loss": 0.0033,
      "step": 3530
    },
    {
      "epoch": 1.762855716425362,
      "grad_norm": 0.3189663887023926,
      "learning_rate": 0.0004823714428357464,
      "loss": 0.0068,
      "step": 3531
    },
    {
      "epoch": 1.7633549675486768,
      "grad_norm": 0.64823317527771,
      "learning_rate": 0.00048236645032451327,
      "loss": 0.01,
      "step": 3532
    },
    {
      "epoch": 1.7638542186719919,
      "grad_norm": 0.08634104579687119,
      "learning_rate": 0.0004823614578132801,
      "loss": 0.0019,
      "step": 3533
    },
    {
      "epoch": 1.764353469795307,
      "grad_norm": 1.2435030937194824,
      "learning_rate": 0.000482356465302047,
      "loss": 0.0141,
      "step": 3534
    },
    {
      "epoch": 1.764852720918622,
      "grad_norm": 0.34998661279678345,
      "learning_rate": 0.0004823514727908138,
      "loss": 0.0064,
      "step": 3535
    },
    {
      "epoch": 1.765351972041937,
      "grad_norm": 0.08034522086381912,
      "learning_rate": 0.0004823464802795806,
      "loss": 0.0012,
      "step": 3536
    },
    {
      "epoch": 1.7658512231652521,
      "grad_norm": 0.2209049016237259,
      "learning_rate": 0.0004823414877683475,
      "loss": 0.0025,
      "step": 3537
    },
    {
      "epoch": 1.7663504742885672,
      "grad_norm": 0.7918493747711182,
      "learning_rate": 0.00048233649525711433,
      "loss": 0.0065,
      "step": 3538
    },
    {
      "epoch": 1.7668497254118822,
      "grad_norm": 0.18625982105731964,
      "learning_rate": 0.0004823315027458812,
      "loss": 0.0018,
      "step": 3539
    },
    {
      "epoch": 1.7673489765351973,
      "grad_norm": 8.376643180847168,
      "learning_rate": 0.00048232651023464803,
      "loss": 0.0086,
      "step": 3540
    },
    {
      "epoch": 1.7678482276585124,
      "grad_norm": 0.08567249774932861,
      "learning_rate": 0.0004823215177234149,
      "loss": 0.002,
      "step": 3541
    },
    {
      "epoch": 1.7683474787818274,
      "grad_norm": 2.61706805229187,
      "learning_rate": 0.00048231652521218174,
      "loss": 0.0183,
      "step": 3542
    },
    {
      "epoch": 1.7688467299051422,
      "grad_norm": 0.27190300822257996,
      "learning_rate": 0.0004823115327009486,
      "loss": 0.005,
      "step": 3543
    },
    {
      "epoch": 1.7693459810284573,
      "grad_norm": 0.2024255394935608,
      "learning_rate": 0.00048230654018971544,
      "loss": 0.0031,
      "step": 3544
    },
    {
      "epoch": 1.7698452321517724,
      "grad_norm": 1.1184537410736084,
      "learning_rate": 0.0004823015476784823,
      "loss": 0.0076,
      "step": 3545
    },
    {
      "epoch": 1.7703444832750874,
      "grad_norm": 0.5097563862800598,
      "learning_rate": 0.00048229655516724915,
      "loss": 0.0058,
      "step": 3546
    },
    {
      "epoch": 1.7708437343984023,
      "grad_norm": 1.5682337284088135,
      "learning_rate": 0.000482291562656016,
      "loss": 0.0586,
      "step": 3547
    },
    {
      "epoch": 1.7713429855217173,
      "grad_norm": 0.04393913969397545,
      "learning_rate": 0.00048228657014478285,
      "loss": 0.001,
      "step": 3548
    },
    {
      "epoch": 1.7718422366450324,
      "grad_norm": 0.3567548990249634,
      "learning_rate": 0.0004822815776335497,
      "loss": 0.0039,
      "step": 3549
    },
    {
      "epoch": 1.7723414877683474,
      "grad_norm": 0.6303862929344177,
      "learning_rate": 0.00048227658512231655,
      "loss": 0.0189,
      "step": 3550
    },
    {
      "epoch": 1.7728407388916625,
      "grad_norm": 0.10161939263343811,
      "learning_rate": 0.0004822715926110834,
      "loss": 0.0024,
      "step": 3551
    },
    {
      "epoch": 1.7733399900149776,
      "grad_norm": 0.06157880276441574,
      "learning_rate": 0.00048226660009985026,
      "loss": 0.0014,
      "step": 3552
    },
    {
      "epoch": 1.7738392411382926,
      "grad_norm": 1.8927704095840454,
      "learning_rate": 0.0004822616075886171,
      "loss": 0.0175,
      "step": 3553
    },
    {
      "epoch": 1.7743384922616077,
      "grad_norm": 0.33605653047561646,
      "learning_rate": 0.00048225661507738396,
      "loss": 0.005,
      "step": 3554
    },
    {
      "epoch": 1.7748377433849227,
      "grad_norm": 0.3183625340461731,
      "learning_rate": 0.0004822516225661508,
      "loss": 0.0052,
      "step": 3555
    },
    {
      "epoch": 1.7753369945082378,
      "grad_norm": 0.46656742691993713,
      "learning_rate": 0.00048224663005491767,
      "loss": 0.0313,
      "step": 3556
    },
    {
      "epoch": 1.7758362456315526,
      "grad_norm": 0.30176371335983276,
      "learning_rate": 0.0004822416375436845,
      "loss": 0.0043,
      "step": 3557
    },
    {
      "epoch": 1.7763354967548677,
      "grad_norm": 0.37350159883499146,
      "learning_rate": 0.00048223664503245137,
      "loss": 0.0039,
      "step": 3558
    },
    {
      "epoch": 1.7768347478781827,
      "grad_norm": 2.0015103816986084,
      "learning_rate": 0.0004822316525212182,
      "loss": 0.0061,
      "step": 3559
    },
    {
      "epoch": 1.7773339990014978,
      "grad_norm": 0.4858732223510742,
      "learning_rate": 0.0004822266600099851,
      "loss": 0.0145,
      "step": 3560
    },
    {
      "epoch": 1.7778332501248126,
      "grad_norm": 0.5507926344871521,
      "learning_rate": 0.0004822216674987519,
      "loss": 0.0146,
      "step": 3561
    },
    {
      "epoch": 1.7783325012481277,
      "grad_norm": 0.15243782103061676,
      "learning_rate": 0.0004822166749875188,
      "loss": 0.0019,
      "step": 3562
    },
    {
      "epoch": 1.7788317523714428,
      "grad_norm": 0.2605201303958893,
      "learning_rate": 0.00048221168247628563,
      "loss": 0.0037,
      "step": 3563
    },
    {
      "epoch": 1.7793310034947578,
      "grad_norm": 0.20829805731773376,
      "learning_rate": 0.0004822066899650525,
      "loss": 0.0029,
      "step": 3564
    },
    {
      "epoch": 1.7798302546180729,
      "grad_norm": 0.09525974094867706,
      "learning_rate": 0.0004822016974538193,
      "loss": 0.002,
      "step": 3565
    },
    {
      "epoch": 1.780329505741388,
      "grad_norm": 0.4090704023838043,
      "learning_rate": 0.00048219670494258613,
      "loss": 0.0227,
      "step": 3566
    },
    {
      "epoch": 1.780828756864703,
      "grad_norm": 0.06118195876479149,
      "learning_rate": 0.000482191712431353,
      "loss": 0.0013,
      "step": 3567
    },
    {
      "epoch": 1.781328007988018,
      "grad_norm": 0.6827839016914368,
      "learning_rate": 0.0004821867199201198,
      "loss": 0.0239,
      "step": 3568
    },
    {
      "epoch": 1.781827259111333,
      "grad_norm": 0.2440129667520523,
      "learning_rate": 0.00048218172740888663,
      "loss": 0.0048,
      "step": 3569
    },
    {
      "epoch": 1.7823265102346482,
      "grad_norm": 0.041539162397384644,
      "learning_rate": 0.0004821767348976535,
      "loss": 0.0016,
      "step": 3570
    },
    {
      "epoch": 1.782825761357963,
      "grad_norm": 0.18539094924926758,
      "learning_rate": 0.00048217174238642034,
      "loss": 0.0026,
      "step": 3571
    },
    {
      "epoch": 1.783325012481278,
      "grad_norm": 0.8739383816719055,
      "learning_rate": 0.0004821667498751872,
      "loss": 0.0047,
      "step": 3572
    },
    {
      "epoch": 1.7838242636045931,
      "grad_norm": 0.48786959052085876,
      "learning_rate": 0.00048216175736395404,
      "loss": 0.0049,
      "step": 3573
    },
    {
      "epoch": 1.7843235147279082,
      "grad_norm": 0.2675198018550873,
      "learning_rate": 0.0004821567648527209,
      "loss": 0.0032,
      "step": 3574
    },
    {
      "epoch": 1.784822765851223,
      "grad_norm": 0.4879201650619507,
      "learning_rate": 0.00048215177234148775,
      "loss": 0.0088,
      "step": 3575
    },
    {
      "epoch": 1.785322016974538,
      "grad_norm": 0.17661137878894806,
      "learning_rate": 0.0004821467798302546,
      "loss": 0.003,
      "step": 3576
    },
    {
      "epoch": 1.7858212680978531,
      "grad_norm": 0.19669947028160095,
      "learning_rate": 0.00048214178731902145,
      "loss": 0.0048,
      "step": 3577
    },
    {
      "epoch": 1.7863205192211682,
      "grad_norm": 0.472351610660553,
      "learning_rate": 0.0004821367948077883,
      "loss": 0.0131,
      "step": 3578
    },
    {
      "epoch": 1.7868197703444832,
      "grad_norm": 1.0127636194229126,
      "learning_rate": 0.00048213180229655516,
      "loss": 0.0083,
      "step": 3579
    },
    {
      "epoch": 1.7873190214677983,
      "grad_norm": 0.20274601876735687,
      "learning_rate": 0.000482126809785322,
      "loss": 0.0024,
      "step": 3580
    },
    {
      "epoch": 1.7878182725911134,
      "grad_norm": 0.018747225403785706,
      "learning_rate": 0.00048212181727408886,
      "loss": 0.0011,
      "step": 3581
    },
    {
      "epoch": 1.7883175237144284,
      "grad_norm": 0.04965834319591522,
      "learning_rate": 0.0004821168247628557,
      "loss": 0.0019,
      "step": 3582
    },
    {
      "epoch": 1.7888167748377435,
      "grad_norm": 0.29992926120758057,
      "learning_rate": 0.00048211183225162256,
      "loss": 0.0071,
      "step": 3583
    },
    {
      "epoch": 1.7893160259610585,
      "grad_norm": 0.299889475107193,
      "learning_rate": 0.0004821068397403894,
      "loss": 0.0039,
      "step": 3584
    },
    {
      "epoch": 1.7898152770843736,
      "grad_norm": 0.6048309206962585,
      "learning_rate": 0.00048210184722915627,
      "loss": 0.0103,
      "step": 3585
    },
    {
      "epoch": 1.7903145282076884,
      "grad_norm": 0.09788627177476883,
      "learning_rate": 0.0004820968547179231,
      "loss": 0.0022,
      "step": 3586
    },
    {
      "epoch": 1.7908137793310035,
      "grad_norm": 0.3507659137248993,
      "learning_rate": 0.00048209186220668997,
      "loss": 0.0175,
      "step": 3587
    },
    {
      "epoch": 1.7913130304543186,
      "grad_norm": 2.6787686347961426,
      "learning_rate": 0.0004820868696954568,
      "loss": 0.0639,
      "step": 3588
    },
    {
      "epoch": 1.7918122815776334,
      "grad_norm": 0.1737920492887497,
      "learning_rate": 0.0004820818771842237,
      "loss": 0.0097,
      "step": 3589
    },
    {
      "epoch": 1.7923115327009485,
      "grad_norm": 0.12491382658481598,
      "learning_rate": 0.00048207688467299053,
      "loss": 0.0031,
      "step": 3590
    },
    {
      "epoch": 1.7928107838242635,
      "grad_norm": 0.08792348206043243,
      "learning_rate": 0.0004820718921617574,
      "loss": 0.0027,
      "step": 3591
    },
    {
      "epoch": 1.7933100349475786,
      "grad_norm": 0.10651936382055283,
      "learning_rate": 0.00048206689965052423,
      "loss": 0.0023,
      "step": 3592
    },
    {
      "epoch": 1.7938092860708936,
      "grad_norm": 0.04676460102200508,
      "learning_rate": 0.0004820619071392911,
      "loss": 0.0016,
      "step": 3593
    },
    {
      "epoch": 1.7943085371942087,
      "grad_norm": 0.039616137742996216,
      "learning_rate": 0.0004820569146280579,
      "loss": 0.0015,
      "step": 3594
    },
    {
      "epoch": 1.7948077883175237,
      "grad_norm": 0.07661592215299606,
      "learning_rate": 0.00048205192211682473,
      "loss": 0.0021,
      "step": 3595
    },
    {
      "epoch": 1.7953070394408388,
      "grad_norm": 0.24457773566246033,
      "learning_rate": 0.0004820469296055916,
      "loss": 0.0052,
      "step": 3596
    },
    {
      "epoch": 1.7958062905641539,
      "grad_norm": 0.20004390180110931,
      "learning_rate": 0.00048204193709435844,
      "loss": 0.0033,
      "step": 3597
    },
    {
      "epoch": 1.796305541687469,
      "grad_norm": 0.3394826054573059,
      "learning_rate": 0.0004820369445831253,
      "loss": 0.0081,
      "step": 3598
    },
    {
      "epoch": 1.796804792810784,
      "grad_norm": 0.21940119564533234,
      "learning_rate": 0.00048203195207189214,
      "loss": 0.0028,
      "step": 3599
    },
    {
      "epoch": 1.7973040439340988,
      "grad_norm": 0.06696717441082001,
      "learning_rate": 0.000482026959560659,
      "loss": 0.0016,
      "step": 3600
    },
    {
      "epoch": 1.7978032950574139,
      "grad_norm": 0.16622798144817352,
      "learning_rate": 0.00048202196704942585,
      "loss": 0.0027,
      "step": 3601
    },
    {
      "epoch": 1.798302546180729,
      "grad_norm": 0.17133712768554688,
      "learning_rate": 0.0004820169745381927,
      "loss": 0.0031,
      "step": 3602
    },
    {
      "epoch": 1.7988017973040438,
      "grad_norm": 0.33465924859046936,
      "learning_rate": 0.00048201198202695955,
      "loss": 0.0096,
      "step": 3603
    },
    {
      "epoch": 1.7993010484273588,
      "grad_norm": 0.2472139447927475,
      "learning_rate": 0.0004820069895157264,
      "loss": 0.0046,
      "step": 3604
    },
    {
      "epoch": 1.7998002995506739,
      "grad_norm": 0.058632783591747284,
      "learning_rate": 0.00048200199700449325,
      "loss": 0.0017,
      "step": 3605
    },
    {
      "epoch": 1.800299550673989,
      "grad_norm": 0.07009807229042053,
      "learning_rate": 0.0004819970044932601,
      "loss": 0.0013,
      "step": 3606
    },
    {
      "epoch": 1.800798801797304,
      "grad_norm": 0.2728002369403839,
      "learning_rate": 0.00048199201198202696,
      "loss": 0.0048,
      "step": 3607
    },
    {
      "epoch": 1.801298052920619,
      "grad_norm": 0.03149046376347542,
      "learning_rate": 0.0004819870194707938,
      "loss": 0.0011,
      "step": 3608
    },
    {
      "epoch": 1.8017973040439341,
      "grad_norm": 0.32987359166145325,
      "learning_rate": 0.00048198202695956066,
      "loss": 0.0072,
      "step": 3609
    },
    {
      "epoch": 1.8022965551672492,
      "grad_norm": 0.4552614688873291,
      "learning_rate": 0.0004819770344483275,
      "loss": 0.0132,
      "step": 3610
    },
    {
      "epoch": 1.8027958062905642,
      "grad_norm": 0.061318740248680115,
      "learning_rate": 0.00048197204193709437,
      "loss": 0.0011,
      "step": 3611
    },
    {
      "epoch": 1.8032950574138793,
      "grad_norm": 0.33897361159324646,
      "learning_rate": 0.0004819670494258612,
      "loss": 0.0065,
      "step": 3612
    },
    {
      "epoch": 1.8037943085371944,
      "grad_norm": 0.06392607092857361,
      "learning_rate": 0.00048196205691462807,
      "loss": 0.0014,
      "step": 3613
    },
    {
      "epoch": 1.8042935596605092,
      "grad_norm": 0.055570363998413086,
      "learning_rate": 0.0004819570644033949,
      "loss": 0.0013,
      "step": 3614
    },
    {
      "epoch": 1.8047928107838243,
      "grad_norm": 0.024038279429078102,
      "learning_rate": 0.0004819520718921618,
      "loss": 0.0007,
      "step": 3615
    },
    {
      "epoch": 1.8052920619071393,
      "grad_norm": 1.1719772815704346,
      "learning_rate": 0.00048194707938092863,
      "loss": 0.01,
      "step": 3616
    },
    {
      "epoch": 1.8057913130304544,
      "grad_norm": 0.10816756635904312,
      "learning_rate": 0.0004819420868696955,
      "loss": 0.0018,
      "step": 3617
    },
    {
      "epoch": 1.8062905641537692,
      "grad_norm": 0.49394655227661133,
      "learning_rate": 0.00048193709435846233,
      "loss": 0.0196,
      "step": 3618
    },
    {
      "epoch": 1.8067898152770843,
      "grad_norm": 0.016840027645230293,
      "learning_rate": 0.0004819321018472292,
      "loss": 0.0007,
      "step": 3619
    },
    {
      "epoch": 1.8072890664003993,
      "grad_norm": 0.23310425877571106,
      "learning_rate": 0.00048192710933599604,
      "loss": 0.0029,
      "step": 3620
    },
    {
      "epoch": 1.8077883175237144,
      "grad_norm": 0.10938646644353867,
      "learning_rate": 0.0004819221168247629,
      "loss": 0.0018,
      "step": 3621
    },
    {
      "epoch": 1.8082875686470294,
      "grad_norm": 0.11971645057201385,
      "learning_rate": 0.00048191712431352974,
      "loss": 0.0025,
      "step": 3622
    },
    {
      "epoch": 1.8087868197703445,
      "grad_norm": 0.2108745574951172,
      "learning_rate": 0.00048191213180229654,
      "loss": 0.0038,
      "step": 3623
    },
    {
      "epoch": 1.8092860708936596,
      "grad_norm": 0.12716995179653168,
      "learning_rate": 0.0004819071392910634,
      "loss": 0.0023,
      "step": 3624
    },
    {
      "epoch": 1.8097853220169746,
      "grad_norm": 0.02817436493933201,
      "learning_rate": 0.00048190214677983024,
      "loss": 0.0012,
      "step": 3625
    },
    {
      "epoch": 1.8102845731402897,
      "grad_norm": 1.0204797983169556,
      "learning_rate": 0.0004818971542685971,
      "loss": 0.0072,
      "step": 3626
    },
    {
      "epoch": 1.8107838242636047,
      "grad_norm": 0.3490906059741974,
      "learning_rate": 0.00048189216175736395,
      "loss": 0.0033,
      "step": 3627
    },
    {
      "epoch": 1.8112830753869196,
      "grad_norm": 0.10208148509263992,
      "learning_rate": 0.0004818871692461308,
      "loss": 0.0021,
      "step": 3628
    },
    {
      "epoch": 1.8117823265102346,
      "grad_norm": 0.3781019449234009,
      "learning_rate": 0.00048188217673489765,
      "loss": 0.0033,
      "step": 3629
    },
    {
      "epoch": 1.8122815776335497,
      "grad_norm": 0.24924851953983307,
      "learning_rate": 0.0004818771842236645,
      "loss": 0.0024,
      "step": 3630
    },
    {
      "epoch": 1.8127808287568647,
      "grad_norm": 0.09275048971176147,
      "learning_rate": 0.00048187219171243135,
      "loss": 0.0021,
      "step": 3631
    },
    {
      "epoch": 1.8132800798801796,
      "grad_norm": 0.5190940499305725,
      "learning_rate": 0.0004818671992011982,
      "loss": 0.0137,
      "step": 3632
    },
    {
      "epoch": 1.8137793310034946,
      "grad_norm": 0.5887345671653748,
      "learning_rate": 0.00048186220668996506,
      "loss": 0.0103,
      "step": 3633
    },
    {
      "epoch": 1.8142785821268097,
      "grad_norm": 0.5920940637588501,
      "learning_rate": 0.0004818572141787319,
      "loss": 0.0169,
      "step": 3634
    },
    {
      "epoch": 1.8147778332501248,
      "grad_norm": 0.3875085413455963,
      "learning_rate": 0.00048185222166749876,
      "loss": 0.0311,
      "step": 3635
    },
    {
      "epoch": 1.8152770843734398,
      "grad_norm": 0.5745680928230286,
      "learning_rate": 0.0004818472291562656,
      "loss": 0.0168,
      "step": 3636
    },
    {
      "epoch": 1.8157763354967549,
      "grad_norm": 0.3566536605358124,
      "learning_rate": 0.00048184223664503247,
      "loss": 0.0061,
      "step": 3637
    },
    {
      "epoch": 1.81627558662007,
      "grad_norm": 0.2891833186149597,
      "learning_rate": 0.0004818372441337993,
      "loss": 0.0063,
      "step": 3638
    },
    {
      "epoch": 1.816774837743385,
      "grad_norm": 0.10755759477615356,
      "learning_rate": 0.00048183225162256617,
      "loss": 0.0019,
      "step": 3639
    },
    {
      "epoch": 1.8172740888667,
      "grad_norm": 0.037857770919799805,
      "learning_rate": 0.000481827259111333,
      "loss": 0.0011,
      "step": 3640
    },
    {
      "epoch": 1.8177733399900151,
      "grad_norm": 0.24615360796451569,
      "learning_rate": 0.0004818222666000999,
      "loss": 0.0054,
      "step": 3641
    },
    {
      "epoch": 1.81827259111333,
      "grad_norm": 0.039633024483919144,
      "learning_rate": 0.00048181727408886673,
      "loss": 0.0011,
      "step": 3642
    },
    {
      "epoch": 1.818771842236645,
      "grad_norm": 0.055393896996974945,
      "learning_rate": 0.0004818122815776336,
      "loss": 0.0014,
      "step": 3643
    },
    {
      "epoch": 1.81927109335996,
      "grad_norm": 0.6706510782241821,
      "learning_rate": 0.00048180728906640043,
      "loss": 0.0253,
      "step": 3644
    },
    {
      "epoch": 1.8197703444832751,
      "grad_norm": 0.10788822919130325,
      "learning_rate": 0.0004818022965551673,
      "loss": 0.0016,
      "step": 3645
    },
    {
      "epoch": 1.82026959560659,
      "grad_norm": 0.25532519817352295,
      "learning_rate": 0.00048179730404393414,
      "loss": 0.0042,
      "step": 3646
    },
    {
      "epoch": 1.820768846729905,
      "grad_norm": 0.4267602562904358,
      "learning_rate": 0.000481792311532701,
      "loss": 0.0169,
      "step": 3647
    },
    {
      "epoch": 1.82126809785322,
      "grad_norm": 0.3731241226196289,
      "learning_rate": 0.00048178731902146784,
      "loss": 0.0087,
      "step": 3648
    },
    {
      "epoch": 1.8217673489765351,
      "grad_norm": 0.24230556190013885,
      "learning_rate": 0.0004817823265102347,
      "loss": 0.0035,
      "step": 3649
    },
    {
      "epoch": 1.8222666000998502,
      "grad_norm": 0.11595601588487625,
      "learning_rate": 0.00048177733399900154,
      "loss": 0.0029,
      "step": 3650
    },
    {
      "epoch": 1.8227658512231653,
      "grad_norm": 0.4757157266139984,
      "learning_rate": 0.0004817723414877684,
      "loss": 0.0107,
      "step": 3651
    },
    {
      "epoch": 1.8232651023464803,
      "grad_norm": 0.240147203207016,
      "learning_rate": 0.0004817673489765352,
      "loss": 0.0046,
      "step": 3652
    },
    {
      "epoch": 1.8237643534697954,
      "grad_norm": 0.7730523943901062,
      "learning_rate": 0.00048176235646530205,
      "loss": 0.009,
      "step": 3653
    },
    {
      "epoch": 1.8242636045931104,
      "grad_norm": 0.5376343727111816,
      "learning_rate": 0.0004817573639540689,
      "loss": 0.0177,
      "step": 3654
    },
    {
      "epoch": 1.8247628557164255,
      "grad_norm": 0.5361180305480957,
      "learning_rate": 0.00048175237144283575,
      "loss": 0.0166,
      "step": 3655
    },
    {
      "epoch": 1.8252621068397406,
      "grad_norm": 0.3199942111968994,
      "learning_rate": 0.0004817473789316026,
      "loss": 0.0088,
      "step": 3656
    },
    {
      "epoch": 1.8257613579630554,
      "grad_norm": 0.25116297602653503,
      "learning_rate": 0.00048174238642036945,
      "loss": 0.0064,
      "step": 3657
    },
    {
      "epoch": 1.8262606090863704,
      "grad_norm": 0.38456082344055176,
      "learning_rate": 0.0004817373939091363,
      "loss": 0.0102,
      "step": 3658
    },
    {
      "epoch": 1.8267598602096855,
      "grad_norm": 3.1793015003204346,
      "learning_rate": 0.00048173240139790316,
      "loss": 0.0466,
      "step": 3659
    },
    {
      "epoch": 1.8272591113330003,
      "grad_norm": 0.4215078055858612,
      "learning_rate": 0.00048172740888667,
      "loss": 0.0104,
      "step": 3660
    },
    {
      "epoch": 1.8277583624563154,
      "grad_norm": 0.29458189010620117,
      "learning_rate": 0.00048172241637543686,
      "loss": 0.022,
      "step": 3661
    },
    {
      "epoch": 1.8282576135796305,
      "grad_norm": 0.3499706983566284,
      "learning_rate": 0.0004817174238642037,
      "loss": 0.0169,
      "step": 3662
    },
    {
      "epoch": 1.8287568647029455,
      "grad_norm": 0.11271849274635315,
      "learning_rate": 0.00048171243135297057,
      "loss": 0.0023,
      "step": 3663
    },
    {
      "epoch": 1.8292561158262606,
      "grad_norm": 0.23501186072826385,
      "learning_rate": 0.0004817074388417374,
      "loss": 0.0041,
      "step": 3664
    },
    {
      "epoch": 1.8297553669495756,
      "grad_norm": 0.3786488473415375,
      "learning_rate": 0.00048170244633050427,
      "loss": 0.0064,
      "step": 3665
    },
    {
      "epoch": 1.8302546180728907,
      "grad_norm": 0.10916277021169662,
      "learning_rate": 0.0004816974538192711,
      "loss": 0.0025,
      "step": 3666
    },
    {
      "epoch": 1.8307538691962058,
      "grad_norm": 0.03200792893767357,
      "learning_rate": 0.000481692461308038,
      "loss": 0.0012,
      "step": 3667
    },
    {
      "epoch": 1.8312531203195208,
      "grad_norm": 0.025861291214823723,
      "learning_rate": 0.0004816874687968048,
      "loss": 0.0011,
      "step": 3668
    },
    {
      "epoch": 1.8317523714428359,
      "grad_norm": 0.30123254656791687,
      "learning_rate": 0.0004816824762855717,
      "loss": 0.0221,
      "step": 3669
    },
    {
      "epoch": 1.832251622566151,
      "grad_norm": 0.03690546751022339,
      "learning_rate": 0.00048167748377433853,
      "loss": 0.0014,
      "step": 3670
    },
    {
      "epoch": 1.8327508736894658,
      "grad_norm": 0.029509322717785835,
      "learning_rate": 0.0004816724912631054,
      "loss": 0.0012,
      "step": 3671
    },
    {
      "epoch": 1.8332501248127808,
      "grad_norm": 0.43414589762687683,
      "learning_rate": 0.00048166749875187223,
      "loss": 0.0052,
      "step": 3672
    },
    {
      "epoch": 1.8337493759360959,
      "grad_norm": 0.4444887638092041,
      "learning_rate": 0.0004816625062406391,
      "loss": 0.0271,
      "step": 3673
    },
    {
      "epoch": 1.8342486270594107,
      "grad_norm": 0.35962623357772827,
      "learning_rate": 0.00048165751372940594,
      "loss": 0.0047,
      "step": 3674
    },
    {
      "epoch": 1.8347478781827258,
      "grad_norm": 1.713080883026123,
      "learning_rate": 0.0004816525212181728,
      "loss": 0.0132,
      "step": 3675
    },
    {
      "epoch": 1.8352471293060408,
      "grad_norm": 0.04948774725198746,
      "learning_rate": 0.00048164752870693964,
      "loss": 0.0015,
      "step": 3676
    },
    {
      "epoch": 1.835746380429356,
      "grad_norm": 0.15139643847942352,
      "learning_rate": 0.0004816425361957065,
      "loss": 0.0022,
      "step": 3677
    },
    {
      "epoch": 1.836245631552671,
      "grad_norm": 0.23828622698783875,
      "learning_rate": 0.00048163754368447335,
      "loss": 0.0072,
      "step": 3678
    },
    {
      "epoch": 1.836744882675986,
      "grad_norm": 0.4988018274307251,
      "learning_rate": 0.0004816325511732402,
      "loss": 0.0051,
      "step": 3679
    },
    {
      "epoch": 1.837244133799301,
      "grad_norm": 0.11760514229536057,
      "learning_rate": 0.00048162755866200705,
      "loss": 0.0028,
      "step": 3680
    },
    {
      "epoch": 1.8377433849226161,
      "grad_norm": 0.3580547571182251,
      "learning_rate": 0.0004816225661507738,
      "loss": 0.0117,
      "step": 3681
    },
    {
      "epoch": 1.8382426360459312,
      "grad_norm": 0.7456996440887451,
      "learning_rate": 0.00048161757363954065,
      "loss": 0.0195,
      "step": 3682
    },
    {
      "epoch": 1.8387418871692462,
      "grad_norm": 0.04417260363698006,
      "learning_rate": 0.0004816125811283075,
      "loss": 0.0015,
      "step": 3683
    },
    {
      "epoch": 1.8392411382925613,
      "grad_norm": 0.2047569453716278,
      "learning_rate": 0.00048160758861707435,
      "loss": 0.0033,
      "step": 3684
    },
    {
      "epoch": 1.8397403894158761,
      "grad_norm": 0.5356984734535217,
      "learning_rate": 0.0004816025961058412,
      "loss": 0.0054,
      "step": 3685
    },
    {
      "epoch": 1.8402396405391912,
      "grad_norm": 0.5071126818656921,
      "learning_rate": 0.00048159760359460806,
      "loss": 0.0133,
      "step": 3686
    },
    {
      "epoch": 1.8407388916625063,
      "grad_norm": 0.08595214039087296,
      "learning_rate": 0.0004815926110833749,
      "loss": 0.0022,
      "step": 3687
    },
    {
      "epoch": 1.8412381427858213,
      "grad_norm": 0.1839410811662674,
      "learning_rate": 0.00048158761857214176,
      "loss": 0.0019,
      "step": 3688
    },
    {
      "epoch": 1.8417373939091362,
      "grad_norm": 0.47918614745140076,
      "learning_rate": 0.0004815826260609086,
      "loss": 0.0047,
      "step": 3689
    },
    {
      "epoch": 1.8422366450324512,
      "grad_norm": 0.3324483036994934,
      "learning_rate": 0.00048157763354967546,
      "loss": 0.0042,
      "step": 3690
    },
    {
      "epoch": 1.8427358961557663,
      "grad_norm": 0.6563775539398193,
      "learning_rate": 0.0004815726410384423,
      "loss": 0.0111,
      "step": 3691
    },
    {
      "epoch": 1.8432351472790813,
      "grad_norm": 0.020134152844548225,
      "learning_rate": 0.00048156764852720917,
      "loss": 0.0011,
      "step": 3692
    },
    {
      "epoch": 1.8437343984023964,
      "grad_norm": 0.2907865643501282,
      "learning_rate": 0.000481562656015976,
      "loss": 0.0114,
      "step": 3693
    },
    {
      "epoch": 1.8442336495257114,
      "grad_norm": 0.12885606288909912,
      "learning_rate": 0.00048155766350474287,
      "loss": 0.0025,
      "step": 3694
    },
    {
      "epoch": 1.8447329006490265,
      "grad_norm": 0.08807156980037689,
      "learning_rate": 0.0004815526709935097,
      "loss": 0.0025,
      "step": 3695
    },
    {
      "epoch": 1.8452321517723416,
      "grad_norm": 0.01658736728131771,
      "learning_rate": 0.0004815476784822766,
      "loss": 0.0009,
      "step": 3696
    },
    {
      "epoch": 1.8457314028956566,
      "grad_norm": 1.014663577079773,
      "learning_rate": 0.00048154268597104343,
      "loss": 0.0986,
      "step": 3697
    },
    {
      "epoch": 1.8462306540189717,
      "grad_norm": 1.5522087812423706,
      "learning_rate": 0.0004815376934598103,
      "loss": 0.0182,
      "step": 3698
    },
    {
      "epoch": 1.8467299051422865,
      "grad_norm": 0.20984628796577454,
      "learning_rate": 0.00048153270094857713,
      "loss": 0.0042,
      "step": 3699
    },
    {
      "epoch": 1.8472291562656016,
      "grad_norm": 3.7056732177734375,
      "learning_rate": 0.000481527708437344,
      "loss": 0.0193,
      "step": 3700
    },
    {
      "epoch": 1.8477284073889166,
      "grad_norm": 0.23780547082424164,
      "learning_rate": 0.00048152271592611084,
      "loss": 0.0042,
      "step": 3701
    },
    {
      "epoch": 1.8482276585122317,
      "grad_norm": 0.03888816758990288,
      "learning_rate": 0.0004815177234148777,
      "loss": 0.0018,
      "step": 3702
    },
    {
      "epoch": 1.8487269096355465,
      "grad_norm": 0.7349192500114441,
      "learning_rate": 0.00048151273090364454,
      "loss": 0.012,
      "step": 3703
    },
    {
      "epoch": 1.8492261607588616,
      "grad_norm": 0.379546582698822,
      "learning_rate": 0.0004815077383924114,
      "loss": 0.0193,
      "step": 3704
    },
    {
      "epoch": 1.8497254118821767,
      "grad_norm": 1.005328893661499,
      "learning_rate": 0.00048150274588117824,
      "loss": 0.0119,
      "step": 3705
    },
    {
      "epoch": 1.8502246630054917,
      "grad_norm": 2.1319363117218018,
      "learning_rate": 0.0004814977533699451,
      "loss": 0.0217,
      "step": 3706
    },
    {
      "epoch": 1.8507239141288068,
      "grad_norm": 0.10993427783250809,
      "learning_rate": 0.00048149276085871195,
      "loss": 0.0026,
      "step": 3707
    },
    {
      "epoch": 1.8512231652521218,
      "grad_norm": 2.187706470489502,
      "learning_rate": 0.0004814877683474788,
      "loss": 0.0158,
      "step": 3708
    },
    {
      "epoch": 1.8517224163754369,
      "grad_norm": 0.32136082649230957,
      "learning_rate": 0.00048148277583624565,
      "loss": 0.0041,
      "step": 3709
    },
    {
      "epoch": 1.852221667498752,
      "grad_norm": 0.4726269543170929,
      "learning_rate": 0.00048147778332501245,
      "loss": 0.0155,
      "step": 3710
    },
    {
      "epoch": 1.852720918622067,
      "grad_norm": 0.16032958030700684,
      "learning_rate": 0.0004814727908137793,
      "loss": 0.0032,
      "step": 3711
    },
    {
      "epoch": 1.853220169745382,
      "grad_norm": 0.2950662672519684,
      "learning_rate": 0.00048146779830254616,
      "loss": 0.0091,
      "step": 3712
    },
    {
      "epoch": 1.8537194208686971,
      "grad_norm": 0.16320331394672394,
      "learning_rate": 0.000481462805791313,
      "loss": 0.0028,
      "step": 3713
    },
    {
      "epoch": 1.854218671992012,
      "grad_norm": 0.2262016087770462,
      "learning_rate": 0.00048145781328007986,
      "loss": 0.0048,
      "step": 3714
    },
    {
      "epoch": 1.854717923115327,
      "grad_norm": 5.375776290893555,
      "learning_rate": 0.0004814528207688467,
      "loss": 0.0142,
      "step": 3715
    },
    {
      "epoch": 1.855217174238642,
      "grad_norm": 1.5615090131759644,
      "learning_rate": 0.00048144782825761356,
      "loss": 0.0113,
      "step": 3716
    },
    {
      "epoch": 1.855716425361957,
      "grad_norm": 1.0536946058273315,
      "learning_rate": 0.0004814428357463804,
      "loss": 0.0122,
      "step": 3717
    },
    {
      "epoch": 1.856215676485272,
      "grad_norm": 0.04283235967159271,
      "learning_rate": 0.00048143784323514727,
      "loss": 0.0015,
      "step": 3718
    },
    {
      "epoch": 1.856714927608587,
      "grad_norm": 0.0635610893368721,
      "learning_rate": 0.0004814328507239141,
      "loss": 0.0023,
      "step": 3719
    },
    {
      "epoch": 1.857214178731902,
      "grad_norm": 0.45294302701950073,
      "learning_rate": 0.00048142785821268097,
      "loss": 0.0103,
      "step": 3720
    },
    {
      "epoch": 1.8577134298552171,
      "grad_norm": 0.069477878510952,
      "learning_rate": 0.0004814228657014478,
      "loss": 0.0025,
      "step": 3721
    },
    {
      "epoch": 1.8582126809785322,
      "grad_norm": 0.8629611134529114,
      "learning_rate": 0.0004814178731902147,
      "loss": 0.0127,
      "step": 3722
    },
    {
      "epoch": 1.8587119321018473,
      "grad_norm": 0.1189708486199379,
      "learning_rate": 0.00048141288067898153,
      "loss": 0.0025,
      "step": 3723
    },
    {
      "epoch": 1.8592111832251623,
      "grad_norm": 0.7297744154930115,
      "learning_rate": 0.0004814078881677484,
      "loss": 0.0249,
      "step": 3724
    },
    {
      "epoch": 1.8597104343484774,
      "grad_norm": 0.4136648178100586,
      "learning_rate": 0.00048140289565651523,
      "loss": 0.0051,
      "step": 3725
    },
    {
      "epoch": 1.8602096854717924,
      "grad_norm": 0.3700311481952667,
      "learning_rate": 0.0004813979031452821,
      "loss": 0.0042,
      "step": 3726
    },
    {
      "epoch": 1.8607089365951075,
      "grad_norm": 0.025521568953990936,
      "learning_rate": 0.00048139291063404894,
      "loss": 0.0017,
      "step": 3727
    },
    {
      "epoch": 1.8612081877184223,
      "grad_norm": 0.08624267578125,
      "learning_rate": 0.0004813879181228158,
      "loss": 0.0026,
      "step": 3728
    },
    {
      "epoch": 1.8617074388417374,
      "grad_norm": 0.28539660573005676,
      "learning_rate": 0.00048138292561158264,
      "loss": 0.0061,
      "step": 3729
    },
    {
      "epoch": 1.8622066899650525,
      "grad_norm": 0.20828619599342346,
      "learning_rate": 0.0004813779331003495,
      "loss": 0.0083,
      "step": 3730
    },
    {
      "epoch": 1.8627059410883673,
      "grad_norm": 1.600456953048706,
      "learning_rate": 0.00048137294058911634,
      "loss": 0.015,
      "step": 3731
    },
    {
      "epoch": 1.8632051922116823,
      "grad_norm": 0.6917056441307068,
      "learning_rate": 0.0004813679480778832,
      "loss": 0.0166,
      "step": 3732
    },
    {
      "epoch": 1.8637044433349974,
      "grad_norm": 0.07988867163658142,
      "learning_rate": 0.00048136295556665005,
      "loss": 0.0022,
      "step": 3733
    },
    {
      "epoch": 1.8642036944583125,
      "grad_norm": 0.4470517039299011,
      "learning_rate": 0.0004813579630554169,
      "loss": 0.0251,
      "step": 3734
    },
    {
      "epoch": 1.8647029455816275,
      "grad_norm": 1.2447636127471924,
      "learning_rate": 0.00048135297054418375,
      "loss": 0.0144,
      "step": 3735
    },
    {
      "epoch": 1.8652021967049426,
      "grad_norm": 0.049447573721408844,
      "learning_rate": 0.0004813479780329506,
      "loss": 0.0022,
      "step": 3736
    },
    {
      "epoch": 1.8657014478282576,
      "grad_norm": 0.09052416682243347,
      "learning_rate": 0.00048134298552171746,
      "loss": 0.0035,
      "step": 3737
    },
    {
      "epoch": 1.8662006989515727,
      "grad_norm": 0.27876710891723633,
      "learning_rate": 0.0004813379930104843,
      "loss": 0.0075,
      "step": 3738
    },
    {
      "epoch": 1.8666999500748878,
      "grad_norm": 0.32897621393203735,
      "learning_rate": 0.0004813330004992511,
      "loss": 0.0085,
      "step": 3739
    },
    {
      "epoch": 1.8671992011982028,
      "grad_norm": 0.29103848338127136,
      "learning_rate": 0.00048132800798801796,
      "loss": 0.008,
      "step": 3740
    },
    {
      "epoch": 1.8676984523215179,
      "grad_norm": 0.6909366846084595,
      "learning_rate": 0.0004813230154767848,
      "loss": 0.0098,
      "step": 3741
    },
    {
      "epoch": 1.8681977034448327,
      "grad_norm": 0.1222819983959198,
      "learning_rate": 0.00048131802296555166,
      "loss": 0.0023,
      "step": 3742
    },
    {
      "epoch": 1.8686969545681478,
      "grad_norm": 1.1330915689468384,
      "learning_rate": 0.0004813130304543185,
      "loss": 0.0107,
      "step": 3743
    },
    {
      "epoch": 1.8691962056914628,
      "grad_norm": 0.26158636808395386,
      "learning_rate": 0.00048130803794308537,
      "loss": 0.005,
      "step": 3744
    },
    {
      "epoch": 1.8696954568147777,
      "grad_norm": 1.9339708089828491,
      "learning_rate": 0.0004813030454318522,
      "loss": 0.0209,
      "step": 3745
    },
    {
      "epoch": 1.8701947079380927,
      "grad_norm": 0.18376046419143677,
      "learning_rate": 0.00048129805292061907,
      "loss": 0.0042,
      "step": 3746
    },
    {
      "epoch": 1.8706939590614078,
      "grad_norm": 0.4087066054344177,
      "learning_rate": 0.0004812930604093859,
      "loss": 0.0112,
      "step": 3747
    },
    {
      "epoch": 1.8711932101847228,
      "grad_norm": 0.4752067029476166,
      "learning_rate": 0.0004812880678981528,
      "loss": 0.0107,
      "step": 3748
    },
    {
      "epoch": 1.871692461308038,
      "grad_norm": 0.1956762671470642,
      "learning_rate": 0.00048128307538691963,
      "loss": 0.005,
      "step": 3749
    },
    {
      "epoch": 1.872191712431353,
      "grad_norm": 0.6948685050010681,
      "learning_rate": 0.0004812780828756865,
      "loss": 0.0395,
      "step": 3750
    },
    {
      "epoch": 1.872690963554668,
      "grad_norm": 0.16106155514717102,
      "learning_rate": 0.00048127309036445333,
      "loss": 0.0032,
      "step": 3751
    },
    {
      "epoch": 1.873190214677983,
      "grad_norm": 0.5030436515808105,
      "learning_rate": 0.0004812680978532202,
      "loss": 0.0251,
      "step": 3752
    },
    {
      "epoch": 1.8736894658012981,
      "grad_norm": 0.20398198068141937,
      "learning_rate": 0.00048126310534198704,
      "loss": 0.0037,
      "step": 3753
    },
    {
      "epoch": 1.8741887169246132,
      "grad_norm": 0.2743265926837921,
      "learning_rate": 0.0004812581128307539,
      "loss": 0.0059,
      "step": 3754
    },
    {
      "epoch": 1.8746879680479283,
      "grad_norm": 0.6408389806747437,
      "learning_rate": 0.00048125312031952074,
      "loss": 0.0085,
      "step": 3755
    },
    {
      "epoch": 1.875187219171243,
      "grad_norm": 1.2062008380889893,
      "learning_rate": 0.0004812481278082876,
      "loss": 0.0184,
      "step": 3756
    },
    {
      "epoch": 1.8756864702945582,
      "grad_norm": 0.4206213653087616,
      "learning_rate": 0.00048124313529705444,
      "loss": 0.0154,
      "step": 3757
    },
    {
      "epoch": 1.8761857214178732,
      "grad_norm": 0.020368382334709167,
      "learning_rate": 0.0004812381427858213,
      "loss": 0.0011,
      "step": 3758
    },
    {
      "epoch": 1.8766849725411883,
      "grad_norm": 0.2541786730289459,
      "learning_rate": 0.00048123315027458815,
      "loss": 0.0049,
      "step": 3759
    },
    {
      "epoch": 1.877184223664503,
      "grad_norm": 0.09617730230093002,
      "learning_rate": 0.000481228157763355,
      "loss": 0.0024,
      "step": 3760
    },
    {
      "epoch": 1.8776834747878182,
      "grad_norm": 0.1748165637254715,
      "learning_rate": 0.00048122316525212185,
      "loss": 0.0044,
      "step": 3761
    },
    {
      "epoch": 1.8781827259111332,
      "grad_norm": 0.6865012049674988,
      "learning_rate": 0.0004812181727408887,
      "loss": 0.0111,
      "step": 3762
    },
    {
      "epoch": 1.8786819770344483,
      "grad_norm": 0.5979148745536804,
      "learning_rate": 0.00048121318022965556,
      "loss": 0.009,
      "step": 3763
    },
    {
      "epoch": 1.8791812281577633,
      "grad_norm": 0.22035831212997437,
      "learning_rate": 0.0004812081877184224,
      "loss": 0.0031,
      "step": 3764
    },
    {
      "epoch": 1.8796804792810784,
      "grad_norm": 0.32217714190483093,
      "learning_rate": 0.00048120319520718926,
      "loss": 0.0085,
      "step": 3765
    },
    {
      "epoch": 1.8801797304043935,
      "grad_norm": 0.44624659419059753,
      "learning_rate": 0.0004811982026959561,
      "loss": 0.0186,
      "step": 3766
    },
    {
      "epoch": 1.8806789815277085,
      "grad_norm": 0.03195216879248619,
      "learning_rate": 0.00048119321018472296,
      "loss": 0.0012,
      "step": 3767
    },
    {
      "epoch": 1.8811782326510236,
      "grad_norm": 0.44802096486091614,
      "learning_rate": 0.00048118821767348976,
      "loss": 0.0267,
      "step": 3768
    },
    {
      "epoch": 1.8816774837743386,
      "grad_norm": 0.10410209000110626,
      "learning_rate": 0.0004811832251622566,
      "loss": 0.0022,
      "step": 3769
    },
    {
      "epoch": 1.8821767348976535,
      "grad_norm": 0.4947088658809662,
      "learning_rate": 0.00048117823265102347,
      "loss": 0.008,
      "step": 3770
    },
    {
      "epoch": 1.8826759860209685,
      "grad_norm": 0.03471619635820389,
      "learning_rate": 0.0004811732401397903,
      "loss": 0.0013,
      "step": 3771
    },
    {
      "epoch": 1.8831752371442836,
      "grad_norm": 0.28733256459236145,
      "learning_rate": 0.00048116824762855717,
      "loss": 0.0048,
      "step": 3772
    },
    {
      "epoch": 1.8836744882675986,
      "grad_norm": 0.21340972185134888,
      "learning_rate": 0.000481163255117324,
      "loss": 0.0029,
      "step": 3773
    },
    {
      "epoch": 1.8841737393909135,
      "grad_norm": 0.05052858963608742,
      "learning_rate": 0.0004811582626060909,
      "loss": 0.0014,
      "step": 3774
    },
    {
      "epoch": 1.8846729905142285,
      "grad_norm": 0.15680062770843506,
      "learning_rate": 0.0004811532700948577,
      "loss": 0.0031,
      "step": 3775
    },
    {
      "epoch": 1.8851722416375436,
      "grad_norm": 0.09476617723703384,
      "learning_rate": 0.0004811482775836246,
      "loss": 0.0015,
      "step": 3776
    },
    {
      "epoch": 1.8856714927608587,
      "grad_norm": 0.05185031145811081,
      "learning_rate": 0.00048114328507239143,
      "loss": 0.0015,
      "step": 3777
    },
    {
      "epoch": 1.8861707438841737,
      "grad_norm": 0.40942057967185974,
      "learning_rate": 0.0004811382925611583,
      "loss": 0.0097,
      "step": 3778
    },
    {
      "epoch": 1.8866699950074888,
      "grad_norm": 0.019015049561858177,
      "learning_rate": 0.00048113330004992514,
      "loss": 0.0009,
      "step": 3779
    },
    {
      "epoch": 1.8871692461308038,
      "grad_norm": 0.4205161929130554,
      "learning_rate": 0.000481128307538692,
      "loss": 0.004,
      "step": 3780
    },
    {
      "epoch": 1.887668497254119,
      "grad_norm": 0.24821731448173523,
      "learning_rate": 0.00048112331502745884,
      "loss": 0.0058,
      "step": 3781
    },
    {
      "epoch": 1.888167748377434,
      "grad_norm": 0.29565972089767456,
      "learning_rate": 0.0004811183225162257,
      "loss": 0.015,
      "step": 3782
    },
    {
      "epoch": 1.888666999500749,
      "grad_norm": 0.028260035440325737,
      "learning_rate": 0.00048111333000499254,
      "loss": 0.0008,
      "step": 3783
    },
    {
      "epoch": 1.889166250624064,
      "grad_norm": 0.04984253644943237,
      "learning_rate": 0.0004811083374937594,
      "loss": 0.0014,
      "step": 3784
    },
    {
      "epoch": 1.889665501747379,
      "grad_norm": 0.08997081220149994,
      "learning_rate": 0.00048110334498252625,
      "loss": 0.0016,
      "step": 3785
    },
    {
      "epoch": 1.890164752870694,
      "grad_norm": 0.10080645978450775,
      "learning_rate": 0.0004810983524712931,
      "loss": 0.0023,
      "step": 3786
    },
    {
      "epoch": 1.890664003994009,
      "grad_norm": 0.25123533606529236,
      "learning_rate": 0.00048109335996005995,
      "loss": 0.0093,
      "step": 3787
    },
    {
      "epoch": 1.8911632551173239,
      "grad_norm": 0.03933194652199745,
      "learning_rate": 0.0004810883674488268,
      "loss": 0.0011,
      "step": 3788
    },
    {
      "epoch": 1.891662506240639,
      "grad_norm": 0.17013652622699738,
      "learning_rate": 0.00048108337493759366,
      "loss": 0.0033,
      "step": 3789
    },
    {
      "epoch": 1.892161757363954,
      "grad_norm": 0.5996319651603699,
      "learning_rate": 0.0004810783824263605,
      "loss": 0.0049,
      "step": 3790
    },
    {
      "epoch": 1.892661008487269,
      "grad_norm": 0.15007999539375305,
      "learning_rate": 0.00048107338991512736,
      "loss": 0.002,
      "step": 3791
    },
    {
      "epoch": 1.893160259610584,
      "grad_norm": 0.021116778254508972,
      "learning_rate": 0.0004810683974038942,
      "loss": 0.001,
      "step": 3792
    },
    {
      "epoch": 1.8936595107338992,
      "grad_norm": 0.03683852776885033,
      "learning_rate": 0.00048106340489266106,
      "loss": 0.001,
      "step": 3793
    },
    {
      "epoch": 1.8941587618572142,
      "grad_norm": 0.025401964783668518,
      "learning_rate": 0.0004810584123814279,
      "loss": 0.001,
      "step": 3794
    },
    {
      "epoch": 1.8946580129805293,
      "grad_norm": 0.24645468592643738,
      "learning_rate": 0.00048105341987019477,
      "loss": 0.004,
      "step": 3795
    },
    {
      "epoch": 1.8951572641038443,
      "grad_norm": 0.31544753909111023,
      "learning_rate": 0.0004810484273589616,
      "loss": 0.0041,
      "step": 3796
    },
    {
      "epoch": 1.8956565152271594,
      "grad_norm": 0.6090158224105835,
      "learning_rate": 0.0004810434348477284,
      "loss": 0.0094,
      "step": 3797
    },
    {
      "epoch": 1.8961557663504744,
      "grad_norm": 0.032343678176403046,
      "learning_rate": 0.0004810384423364952,
      "loss": 0.0012,
      "step": 3798
    },
    {
      "epoch": 1.8966550174737893,
      "grad_norm": 0.0716506838798523,
      "learning_rate": 0.00048103344982526207,
      "loss": 0.0014,
      "step": 3799
    },
    {
      "epoch": 1.8971542685971043,
      "grad_norm": 0.10724307596683502,
      "learning_rate": 0.0004810284573140289,
      "loss": 0.0025,
      "step": 3800
    },
    {
      "epoch": 1.8976535197204194,
      "grad_norm": 0.17429353296756744,
      "learning_rate": 0.00048102346480279577,
      "loss": 0.0037,
      "step": 3801
    },
    {
      "epoch": 1.8981527708437342,
      "grad_norm": 0.11969642341136932,
      "learning_rate": 0.0004810184722915626,
      "loss": 0.002,
      "step": 3802
    },
    {
      "epoch": 1.8986520219670493,
      "grad_norm": 0.15974609553813934,
      "learning_rate": 0.0004810134797803295,
      "loss": 0.0032,
      "step": 3803
    },
    {
      "epoch": 1.8991512730903644,
      "grad_norm": 0.021272532641887665,
      "learning_rate": 0.00048100848726909633,
      "loss": 0.0007,
      "step": 3804
    },
    {
      "epoch": 1.8996505242136794,
      "grad_norm": 0.7490372657775879,
      "learning_rate": 0.0004810034947578632,
      "loss": 0.0296,
      "step": 3805
    },
    {
      "epoch": 1.9001497753369945,
      "grad_norm": 0.18675068020820618,
      "learning_rate": 0.00048099850224663003,
      "loss": 0.0017,
      "step": 3806
    },
    {
      "epoch": 1.9006490264603095,
      "grad_norm": 0.10289274156093597,
      "learning_rate": 0.0004809935097353969,
      "loss": 0.0023,
      "step": 3807
    },
    {
      "epoch": 1.9011482775836246,
      "grad_norm": 1.2888925075531006,
      "learning_rate": 0.00048098851722416374,
      "loss": 0.0172,
      "step": 3808
    },
    {
      "epoch": 1.9016475287069396,
      "grad_norm": 0.22054122388362885,
      "learning_rate": 0.0004809835247129306,
      "loss": 0.0022,
      "step": 3809
    },
    {
      "epoch": 1.9021467798302547,
      "grad_norm": 0.16619062423706055,
      "learning_rate": 0.00048097853220169744,
      "loss": 0.0018,
      "step": 3810
    },
    {
      "epoch": 1.9026460309535698,
      "grad_norm": 0.026071373373270035,
      "learning_rate": 0.0004809735396904643,
      "loss": 0.0009,
      "step": 3811
    },
    {
      "epoch": 1.9031452820768848,
      "grad_norm": 0.3126005530357361,
      "learning_rate": 0.00048096854717923114,
      "loss": 0.0051,
      "step": 3812
    },
    {
      "epoch": 1.9036445332001997,
      "grad_norm": 0.9556068181991577,
      "learning_rate": 0.000480963554667998,
      "loss": 0.0061,
      "step": 3813
    },
    {
      "epoch": 1.9041437843235147,
      "grad_norm": 0.5225002765655518,
      "learning_rate": 0.00048095856215676485,
      "loss": 0.0622,
      "step": 3814
    },
    {
      "epoch": 1.9046430354468298,
      "grad_norm": 0.30543118715286255,
      "learning_rate": 0.0004809535696455317,
      "loss": 0.0187,
      "step": 3815
    },
    {
      "epoch": 1.9051422865701446,
      "grad_norm": 0.4333063066005707,
      "learning_rate": 0.00048094857713429855,
      "loss": 0.0147,
      "step": 3816
    },
    {
      "epoch": 1.9056415376934597,
      "grad_norm": 0.030779508873820305,
      "learning_rate": 0.0004809435846230654,
      "loss": 0.0011,
      "step": 3817
    },
    {
      "epoch": 1.9061407888167747,
      "grad_norm": 0.09279976040124893,
      "learning_rate": 0.00048093859211183226,
      "loss": 0.002,
      "step": 3818
    },
    {
      "epoch": 1.9066400399400898,
      "grad_norm": 0.5357505083084106,
      "learning_rate": 0.0004809335996005991,
      "loss": 0.0204,
      "step": 3819
    },
    {
      "epoch": 1.9071392910634049,
      "grad_norm": 0.8779345750808716,
      "learning_rate": 0.00048092860708936596,
      "loss": 0.0313,
      "step": 3820
    },
    {
      "epoch": 1.90763854218672,
      "grad_norm": 0.06483782827854156,
      "learning_rate": 0.0004809236145781328,
      "loss": 0.0022,
      "step": 3821
    },
    {
      "epoch": 1.908137793310035,
      "grad_norm": 0.46486806869506836,
      "learning_rate": 0.00048091862206689967,
      "loss": 0.0274,
      "step": 3822
    },
    {
      "epoch": 1.90863704443335,
      "grad_norm": 0.34573471546173096,
      "learning_rate": 0.0004809136295556665,
      "loss": 0.0192,
      "step": 3823
    },
    {
      "epoch": 1.909136295556665,
      "grad_norm": 0.5036914944648743,
      "learning_rate": 0.00048090863704443337,
      "loss": 0.006,
      "step": 3824
    },
    {
      "epoch": 1.9096355466799801,
      "grad_norm": 0.23956769704818726,
      "learning_rate": 0.0004809036445332002,
      "loss": 0.0064,
      "step": 3825
    },
    {
      "epoch": 1.9101347978032952,
      "grad_norm": 0.5785004496574402,
      "learning_rate": 0.0004808986520219671,
      "loss": 0.0154,
      "step": 3826
    },
    {
      "epoch": 1.91063404892661,
      "grad_norm": 0.21286369860172272,
      "learning_rate": 0.00048089365951073387,
      "loss": 0.0043,
      "step": 3827
    },
    {
      "epoch": 1.911133300049925,
      "grad_norm": 0.1623833030462265,
      "learning_rate": 0.0004808886669995007,
      "loss": 0.0038,
      "step": 3828
    },
    {
      "epoch": 1.9116325511732402,
      "grad_norm": 0.15619906783103943,
      "learning_rate": 0.0004808836744882676,
      "loss": 0.003,
      "step": 3829
    },
    {
      "epoch": 1.9121318022965552,
      "grad_norm": 0.8295655846595764,
      "learning_rate": 0.00048087868197703443,
      "loss": 0.0151,
      "step": 3830
    },
    {
      "epoch": 1.91263105341987,
      "grad_norm": 0.10945191234350204,
      "learning_rate": 0.0004808736894658013,
      "loss": 0.0026,
      "step": 3831
    },
    {
      "epoch": 1.9131303045431851,
      "grad_norm": 0.02521401457488537,
      "learning_rate": 0.00048086869695456813,
      "loss": 0.0015,
      "step": 3832
    },
    {
      "epoch": 1.9136295556665002,
      "grad_norm": 0.2191525101661682,
      "learning_rate": 0.000480863704443335,
      "loss": 0.0043,
      "step": 3833
    },
    {
      "epoch": 1.9141288067898152,
      "grad_norm": 0.7489610910415649,
      "learning_rate": 0.00048085871193210184,
      "loss": 0.0174,
      "step": 3834
    },
    {
      "epoch": 1.9146280579131303,
      "grad_norm": 0.38360875844955444,
      "learning_rate": 0.0004808537194208687,
      "loss": 0.0062,
      "step": 3835
    },
    {
      "epoch": 1.9151273090364453,
      "grad_norm": 0.397828072309494,
      "learning_rate": 0.00048084872690963554,
      "loss": 0.0075,
      "step": 3836
    },
    {
      "epoch": 1.9156265601597604,
      "grad_norm": 0.12236811220645905,
      "learning_rate": 0.0004808437343984024,
      "loss": 0.0025,
      "step": 3837
    },
    {
      "epoch": 1.9161258112830755,
      "grad_norm": 0.060501936823129654,
      "learning_rate": 0.00048083874188716924,
      "loss": 0.002,
      "step": 3838
    },
    {
      "epoch": 1.9166250624063905,
      "grad_norm": 0.2147631198167801,
      "learning_rate": 0.0004808337493759361,
      "loss": 0.0023,
      "step": 3839
    },
    {
      "epoch": 1.9171243135297056,
      "grad_norm": 0.09649219363927841,
      "learning_rate": 0.00048082875686470295,
      "loss": 0.0023,
      "step": 3840
    },
    {
      "epoch": 1.9176235646530204,
      "grad_norm": 1.0161741971969604,
      "learning_rate": 0.0004808237643534698,
      "loss": 0.0157,
      "step": 3841
    },
    {
      "epoch": 1.9181228157763355,
      "grad_norm": 0.07602553069591522,
      "learning_rate": 0.00048081877184223665,
      "loss": 0.0023,
      "step": 3842
    },
    {
      "epoch": 1.9186220668996505,
      "grad_norm": 0.20477652549743652,
      "learning_rate": 0.0004808137793310035,
      "loss": 0.0026,
      "step": 3843
    },
    {
      "epoch": 1.9191213180229656,
      "grad_norm": 0.20586539804935455,
      "learning_rate": 0.00048080878681977036,
      "loss": 0.0028,
      "step": 3844
    },
    {
      "epoch": 1.9196205691462804,
      "grad_norm": 0.0202786922454834,
      "learning_rate": 0.0004808037943085372,
      "loss": 0.0012,
      "step": 3845
    },
    {
      "epoch": 1.9201198202695955,
      "grad_norm": 0.38607698678970337,
      "learning_rate": 0.00048079880179730406,
      "loss": 0.0064,
      "step": 3846
    },
    {
      "epoch": 1.9206190713929105,
      "grad_norm": 0.13922247290611267,
      "learning_rate": 0.0004807938092860709,
      "loss": 0.0029,
      "step": 3847
    },
    {
      "epoch": 1.9211183225162256,
      "grad_norm": 0.5455977320671082,
      "learning_rate": 0.00048078881677483777,
      "loss": 0.0094,
      "step": 3848
    },
    {
      "epoch": 1.9216175736395407,
      "grad_norm": 0.09609794616699219,
      "learning_rate": 0.0004807838242636046,
      "loss": 0.002,
      "step": 3849
    },
    {
      "epoch": 1.9221168247628557,
      "grad_norm": 0.03962384909391403,
      "learning_rate": 0.00048077883175237147,
      "loss": 0.0013,
      "step": 3850
    },
    {
      "epoch": 1.9226160758861708,
      "grad_norm": 0.04093138501048088,
      "learning_rate": 0.0004807738392411383,
      "loss": 0.0012,
      "step": 3851
    },
    {
      "epoch": 1.9231153270094858,
      "grad_norm": 0.46431371569633484,
      "learning_rate": 0.0004807688467299052,
      "loss": 0.0025,
      "step": 3852
    },
    {
      "epoch": 1.923614578132801,
      "grad_norm": 0.03884145990014076,
      "learning_rate": 0.000480763854218672,
      "loss": 0.0015,
      "step": 3853
    },
    {
      "epoch": 1.924113829256116,
      "grad_norm": 0.6730416417121887,
      "learning_rate": 0.0004807588617074389,
      "loss": 0.018,
      "step": 3854
    },
    {
      "epoch": 1.924613080379431,
      "grad_norm": 0.30005791783332825,
      "learning_rate": 0.00048075386919620573,
      "loss": 0.0031,
      "step": 3855
    },
    {
      "epoch": 1.9251123315027459,
      "grad_norm": 0.11636132746934891,
      "learning_rate": 0.00048074887668497253,
      "loss": 0.0017,
      "step": 3856
    },
    {
      "epoch": 1.925611582626061,
      "grad_norm": 0.6939219236373901,
      "learning_rate": 0.0004807438841737394,
      "loss": 0.0034,
      "step": 3857
    },
    {
      "epoch": 1.926110833749376,
      "grad_norm": 0.23889541625976562,
      "learning_rate": 0.00048073889166250623,
      "loss": 0.0268,
      "step": 3858
    },
    {
      "epoch": 1.9266100848726908,
      "grad_norm": 0.04819013550877571,
      "learning_rate": 0.0004807338991512731,
      "loss": 0.0017,
      "step": 3859
    },
    {
      "epoch": 1.9271093359960059,
      "grad_norm": 0.7149842977523804,
      "learning_rate": 0.00048072890664003994,
      "loss": 0.0247,
      "step": 3860
    },
    {
      "epoch": 1.927608587119321,
      "grad_norm": 0.6813237071037292,
      "learning_rate": 0.0004807239141288068,
      "loss": 0.0031,
      "step": 3861
    },
    {
      "epoch": 1.928107838242636,
      "grad_norm": 0.0649917870759964,
      "learning_rate": 0.00048071892161757364,
      "loss": 0.0016,
      "step": 3862
    },
    {
      "epoch": 1.928607089365951,
      "grad_norm": 0.4469715654850006,
      "learning_rate": 0.0004807139291063405,
      "loss": 0.0029,
      "step": 3863
    },
    {
      "epoch": 1.929106340489266,
      "grad_norm": 0.09442689269781113,
      "learning_rate": 0.00048070893659510734,
      "loss": 0.0017,
      "step": 3864
    },
    {
      "epoch": 1.9296055916125812,
      "grad_norm": 1.1961760520935059,
      "learning_rate": 0.0004807039440838742,
      "loss": 0.0224,
      "step": 3865
    },
    {
      "epoch": 1.9301048427358962,
      "grad_norm": 0.2664877474308014,
      "learning_rate": 0.00048069895157264105,
      "loss": 0.0024,
      "step": 3866
    },
    {
      "epoch": 1.9306040938592113,
      "grad_norm": 0.28540679812431335,
      "learning_rate": 0.0004806939590614079,
      "loss": 0.0044,
      "step": 3867
    },
    {
      "epoch": 1.9311033449825263,
      "grad_norm": 0.18148526549339294,
      "learning_rate": 0.00048068896655017475,
      "loss": 0.0036,
      "step": 3868
    },
    {
      "epoch": 1.9316025961058414,
      "grad_norm": 0.4420164227485657,
      "learning_rate": 0.0004806839740389416,
      "loss": 0.0316,
      "step": 3869
    },
    {
      "epoch": 1.9321018472291562,
      "grad_norm": 0.6521635055541992,
      "learning_rate": 0.00048067898152770846,
      "loss": 0.0121,
      "step": 3870
    },
    {
      "epoch": 1.9326010983524713,
      "grad_norm": 0.2529185712337494,
      "learning_rate": 0.0004806739890164753,
      "loss": 0.0065,
      "step": 3871
    },
    {
      "epoch": 1.9331003494757864,
      "grad_norm": 0.04851660132408142,
      "learning_rate": 0.00048066899650524216,
      "loss": 0.0017,
      "step": 3872
    },
    {
      "epoch": 1.9335996005991012,
      "grad_norm": 0.05128815770149231,
      "learning_rate": 0.000480664003994009,
      "loss": 0.0015,
      "step": 3873
    },
    {
      "epoch": 1.9340988517224162,
      "grad_norm": 0.06407041102647781,
      "learning_rate": 0.00048065901148277586,
      "loss": 0.002,
      "step": 3874
    },
    {
      "epoch": 1.9345981028457313,
      "grad_norm": 0.030313290655612946,
      "learning_rate": 0.0004806540189715427,
      "loss": 0.0015,
      "step": 3875
    },
    {
      "epoch": 1.9350973539690464,
      "grad_norm": 0.43887266516685486,
      "learning_rate": 0.00048064902646030957,
      "loss": 0.0096,
      "step": 3876
    },
    {
      "epoch": 1.9355966050923614,
      "grad_norm": 0.1623494178056717,
      "learning_rate": 0.0004806440339490764,
      "loss": 0.0029,
      "step": 3877
    },
    {
      "epoch": 1.9360958562156765,
      "grad_norm": 0.16165924072265625,
      "learning_rate": 0.00048063904143784327,
      "loss": 0.0032,
      "step": 3878
    },
    {
      "epoch": 1.9365951073389915,
      "grad_norm": 0.025094294920563698,
      "learning_rate": 0.0004806340489266101,
      "loss": 0.0016,
      "step": 3879
    },
    {
      "epoch": 1.9370943584623066,
      "grad_norm": 0.2744503617286682,
      "learning_rate": 0.000480629056415377,
      "loss": 0.004,
      "step": 3880
    },
    {
      "epoch": 1.9375936095856217,
      "grad_norm": 0.07757604867219925,
      "learning_rate": 0.00048062406390414383,
      "loss": 0.0022,
      "step": 3881
    },
    {
      "epoch": 1.9380928607089367,
      "grad_norm": 1.5227636098861694,
      "learning_rate": 0.0004806190713929107,
      "loss": 0.0201,
      "step": 3882
    },
    {
      "epoch": 1.9385921118322518,
      "grad_norm": 0.15684780478477478,
      "learning_rate": 0.00048061407888167753,
      "loss": 0.0028,
      "step": 3883
    },
    {
      "epoch": 1.9390913629555666,
      "grad_norm": 0.4885498285293579,
      "learning_rate": 0.0004806090863704444,
      "loss": 0.0056,
      "step": 3884
    },
    {
      "epoch": 1.9395906140788817,
      "grad_norm": 0.1051318421959877,
      "learning_rate": 0.0004806040938592112,
      "loss": 0.0013,
      "step": 3885
    },
    {
      "epoch": 1.9400898652021967,
      "grad_norm": 0.04037357494235039,
      "learning_rate": 0.00048059910134797804,
      "loss": 0.0019,
      "step": 3886
    },
    {
      "epoch": 1.9405891163255118,
      "grad_norm": 0.7870335578918457,
      "learning_rate": 0.0004805941088367449,
      "loss": 0.0276,
      "step": 3887
    },
    {
      "epoch": 1.9410883674488266,
      "grad_norm": 0.5753095746040344,
      "learning_rate": 0.00048058911632551174,
      "loss": 0.0081,
      "step": 3888
    },
    {
      "epoch": 1.9415876185721417,
      "grad_norm": 0.9761804938316345,
      "learning_rate": 0.0004805841238142786,
      "loss": 0.0105,
      "step": 3889
    },
    {
      "epoch": 1.9420868696954567,
      "grad_norm": 0.015112524852156639,
      "learning_rate": 0.00048057913130304544,
      "loss": 0.0011,
      "step": 3890
    },
    {
      "epoch": 1.9425861208187718,
      "grad_norm": 0.015290896408259869,
      "learning_rate": 0.0004805741387918123,
      "loss": 0.001,
      "step": 3891
    },
    {
      "epoch": 1.9430853719420869,
      "grad_norm": 0.11657285690307617,
      "learning_rate": 0.00048056914628057915,
      "loss": 0.0015,
      "step": 3892
    },
    {
      "epoch": 1.943584623065402,
      "grad_norm": 0.2163701206445694,
      "learning_rate": 0.000480564153769346,
      "loss": 0.0037,
      "step": 3893
    },
    {
      "epoch": 1.944083874188717,
      "grad_norm": 0.6871156692504883,
      "learning_rate": 0.00048055916125811285,
      "loss": 0.02,
      "step": 3894
    },
    {
      "epoch": 1.944583125312032,
      "grad_norm": 0.18476314842700958,
      "learning_rate": 0.0004805541687468797,
      "loss": 0.0029,
      "step": 3895
    },
    {
      "epoch": 1.945082376435347,
      "grad_norm": 0.30243706703186035,
      "learning_rate": 0.00048054917623564656,
      "loss": 0.0055,
      "step": 3896
    },
    {
      "epoch": 1.9455816275586622,
      "grad_norm": 0.03747807815670967,
      "learning_rate": 0.0004805441837244134,
      "loss": 0.0013,
      "step": 3897
    },
    {
      "epoch": 1.946080878681977,
      "grad_norm": 0.036558669060468674,
      "learning_rate": 0.00048053919121318026,
      "loss": 0.0015,
      "step": 3898
    },
    {
      "epoch": 1.946580129805292,
      "grad_norm": 0.017966818064451218,
      "learning_rate": 0.0004805341987019471,
      "loss": 0.0011,
      "step": 3899
    },
    {
      "epoch": 1.947079380928607,
      "grad_norm": 0.054196733981370926,
      "learning_rate": 0.00048052920619071396,
      "loss": 0.0018,
      "step": 3900
    },
    {
      "epoch": 1.9475786320519222,
      "grad_norm": 0.12514445185661316,
      "learning_rate": 0.0004805242136794808,
      "loss": 0.0023,
      "step": 3901
    },
    {
      "epoch": 1.948077883175237,
      "grad_norm": 0.27309468388557434,
      "learning_rate": 0.00048051922116824767,
      "loss": 0.0263,
      "step": 3902
    },
    {
      "epoch": 1.948577134298552,
      "grad_norm": 0.32675686478614807,
      "learning_rate": 0.0004805142286570145,
      "loss": 0.0087,
      "step": 3903
    },
    {
      "epoch": 1.9490763854218671,
      "grad_norm": 1.769526720046997,
      "learning_rate": 0.00048050923614578137,
      "loss": 0.0082,
      "step": 3904
    },
    {
      "epoch": 1.9495756365451822,
      "grad_norm": 0.059499695897102356,
      "learning_rate": 0.0004805042436345482,
      "loss": 0.0014,
      "step": 3905
    },
    {
      "epoch": 1.9500748876684972,
      "grad_norm": 0.039007220417261124,
      "learning_rate": 0.0004804992511233151,
      "loss": 0.0014,
      "step": 3906
    },
    {
      "epoch": 1.9505741387918123,
      "grad_norm": 0.07926104217767715,
      "learning_rate": 0.00048049425861208193,
      "loss": 0.002,
      "step": 3907
    },
    {
      "epoch": 1.9510733899151274,
      "grad_norm": 0.3854883909225464,
      "learning_rate": 0.0004804892661008488,
      "loss": 0.0294,
      "step": 3908
    },
    {
      "epoch": 1.9515726410384424,
      "grad_norm": 0.5344531536102295,
      "learning_rate": 0.00048048427358961563,
      "loss": 0.0057,
      "step": 3909
    },
    {
      "epoch": 1.9520718921617575,
      "grad_norm": 0.6007452011108398,
      "learning_rate": 0.00048047928107838243,
      "loss": 0.0144,
      "step": 3910
    },
    {
      "epoch": 1.9525711432850725,
      "grad_norm": 1.431817889213562,
      "learning_rate": 0.0004804742885671493,
      "loss": 0.0248,
      "step": 3911
    },
    {
      "epoch": 1.9530703944083874,
      "grad_norm": 0.3522578179836273,
      "learning_rate": 0.00048046929605591613,
      "loss": 0.0036,
      "step": 3912
    },
    {
      "epoch": 1.9535696455317024,
      "grad_norm": 0.05415929853916168,
      "learning_rate": 0.000480464303544683,
      "loss": 0.0014,
      "step": 3913
    },
    {
      "epoch": 1.9540688966550175,
      "grad_norm": 0.02747468091547489,
      "learning_rate": 0.0004804593110334498,
      "loss": 0.0011,
      "step": 3914
    },
    {
      "epoch": 1.9545681477783325,
      "grad_norm": 0.17290429770946503,
      "learning_rate": 0.00048045431852221664,
      "loss": 0.0035,
      "step": 3915
    },
    {
      "epoch": 1.9550673989016474,
      "grad_norm": 0.16290567815303802,
      "learning_rate": 0.0004804493260109835,
      "loss": 0.0031,
      "step": 3916
    },
    {
      "epoch": 1.9555666500249624,
      "grad_norm": 0.03335289657115936,
      "learning_rate": 0.00048044433349975034,
      "loss": 0.0014,
      "step": 3917
    },
    {
      "epoch": 1.9560659011482775,
      "grad_norm": 0.26905104517936707,
      "learning_rate": 0.0004804393409885172,
      "loss": 0.0042,
      "step": 3918
    },
    {
      "epoch": 1.9565651522715926,
      "grad_norm": 0.3377680778503418,
      "learning_rate": 0.00048043434847728404,
      "loss": 0.0041,
      "step": 3919
    },
    {
      "epoch": 1.9570644033949076,
      "grad_norm": 0.04057895019650459,
      "learning_rate": 0.0004804293559660509,
      "loss": 0.0015,
      "step": 3920
    },
    {
      "epoch": 1.9575636545182227,
      "grad_norm": 0.06894739717245102,
      "learning_rate": 0.00048042436345481775,
      "loss": 0.0019,
      "step": 3921
    },
    {
      "epoch": 1.9580629056415377,
      "grad_norm": 0.11561894416809082,
      "learning_rate": 0.0004804193709435846,
      "loss": 0.0021,
      "step": 3922
    },
    {
      "epoch": 1.9585621567648528,
      "grad_norm": 0.04833581671118736,
      "learning_rate": 0.00048041437843235145,
      "loss": 0.0015,
      "step": 3923
    },
    {
      "epoch": 1.9590614078881678,
      "grad_norm": 0.4400424361228943,
      "learning_rate": 0.0004804093859211183,
      "loss": 0.0019,
      "step": 3924
    },
    {
      "epoch": 1.959560659011483,
      "grad_norm": 0.32500576972961426,
      "learning_rate": 0.00048040439340988516,
      "loss": 0.0045,
      "step": 3925
    },
    {
      "epoch": 1.960059910134798,
      "grad_norm": 0.044513776898384094,
      "learning_rate": 0.000480399400898652,
      "loss": 0.0014,
      "step": 3926
    },
    {
      "epoch": 1.9605591612581128,
      "grad_norm": 0.04508800432085991,
      "learning_rate": 0.00048039440838741886,
      "loss": 0.0011,
      "step": 3927
    },
    {
      "epoch": 1.9610584123814279,
      "grad_norm": 0.2082994282245636,
      "learning_rate": 0.0004803894158761857,
      "loss": 0.0023,
      "step": 3928
    },
    {
      "epoch": 1.961557663504743,
      "grad_norm": 0.25774043798446655,
      "learning_rate": 0.00048038442336495257,
      "loss": 0.0027,
      "step": 3929
    },
    {
      "epoch": 1.9620569146280578,
      "grad_norm": 0.38874730467796326,
      "learning_rate": 0.0004803794308537194,
      "loss": 0.0069,
      "step": 3930
    },
    {
      "epoch": 1.9625561657513728,
      "grad_norm": 1.4937307834625244,
      "learning_rate": 0.00048037443834248627,
      "loss": 0.0028,
      "step": 3931
    },
    {
      "epoch": 1.9630554168746879,
      "grad_norm": 0.3811388909816742,
      "learning_rate": 0.0004803694458312531,
      "loss": 0.0018,
      "step": 3932
    },
    {
      "epoch": 1.963554667998003,
      "grad_norm": 0.3153066337108612,
      "learning_rate": 0.00048036445332002,
      "loss": 0.0099,
      "step": 3933
    },
    {
      "epoch": 1.964053919121318,
      "grad_norm": 0.09232206642627716,
      "learning_rate": 0.0004803594608087868,
      "loss": 0.0018,
      "step": 3934
    },
    {
      "epoch": 1.964553170244633,
      "grad_norm": 0.12830734252929688,
      "learning_rate": 0.0004803544682975537,
      "loss": 0.0013,
      "step": 3935
    },
    {
      "epoch": 1.965052421367948,
      "grad_norm": 0.8927769064903259,
      "learning_rate": 0.00048034947578632053,
      "loss": 0.0122,
      "step": 3936
    },
    {
      "epoch": 1.9655516724912632,
      "grad_norm": 1.0415180921554565,
      "learning_rate": 0.0004803444832750874,
      "loss": 0.0222,
      "step": 3937
    },
    {
      "epoch": 1.9660509236145782,
      "grad_norm": 0.06400439888238907,
      "learning_rate": 0.00048033949076385423,
      "loss": 0.0011,
      "step": 3938
    },
    {
      "epoch": 1.9665501747378933,
      "grad_norm": 0.01669371873140335,
      "learning_rate": 0.0004803344982526211,
      "loss": 0.0008,
      "step": 3939
    },
    {
      "epoch": 1.9670494258612083,
      "grad_norm": 0.3944104015827179,
      "learning_rate": 0.00048032950574138794,
      "loss": 0.0029,
      "step": 3940
    },
    {
      "epoch": 1.9675486769845232,
      "grad_norm": 0.06473925709724426,
      "learning_rate": 0.0004803245132301548,
      "loss": 0.001,
      "step": 3941
    },
    {
      "epoch": 1.9680479281078382,
      "grad_norm": 0.33273646235466003,
      "learning_rate": 0.00048031952071892164,
      "loss": 0.0261,
      "step": 3942
    },
    {
      "epoch": 1.9685471792311533,
      "grad_norm": 0.3105463683605194,
      "learning_rate": 0.00048031452820768844,
      "loss": 0.0042,
      "step": 3943
    },
    {
      "epoch": 1.9690464303544681,
      "grad_norm": 0.11573746055364609,
      "learning_rate": 0.0004803095356964553,
      "loss": 0.002,
      "step": 3944
    },
    {
      "epoch": 1.9695456814777832,
      "grad_norm": 0.0834161564707756,
      "learning_rate": 0.00048030454318522214,
      "loss": 0.0018,
      "step": 3945
    },
    {
      "epoch": 1.9700449326010983,
      "grad_norm": 0.2925702631473541,
      "learning_rate": 0.000480299550673989,
      "loss": 0.0061,
      "step": 3946
    },
    {
      "epoch": 1.9705441837244133,
      "grad_norm": 0.37035754323005676,
      "learning_rate": 0.00048029455816275585,
      "loss": 0.0056,
      "step": 3947
    },
    {
      "epoch": 1.9710434348477284,
      "grad_norm": 0.06571030616760254,
      "learning_rate": 0.0004802895656515227,
      "loss": 0.002,
      "step": 3948
    },
    {
      "epoch": 1.9715426859710434,
      "grad_norm": 0.6761942505836487,
      "learning_rate": 0.00048028457314028955,
      "loss": 0.014,
      "step": 3949
    },
    {
      "epoch": 1.9720419370943585,
      "grad_norm": 0.04142873361706734,
      "learning_rate": 0.0004802795806290564,
      "loss": 0.0018,
      "step": 3950
    },
    {
      "epoch": 1.9725411882176735,
      "grad_norm": 0.25463956594467163,
      "learning_rate": 0.00048027458811782326,
      "loss": 0.0025,
      "step": 3951
    },
    {
      "epoch": 1.9730404393409886,
      "grad_norm": 0.020274439826607704,
      "learning_rate": 0.0004802695956065901,
      "loss": 0.0007,
      "step": 3952
    },
    {
      "epoch": 1.9735396904643037,
      "grad_norm": 0.22577667236328125,
      "learning_rate": 0.00048026460309535696,
      "loss": 0.005,
      "step": 3953
    },
    {
      "epoch": 1.9740389415876187,
      "grad_norm": 0.8168784976005554,
      "learning_rate": 0.0004802596105841238,
      "loss": 0.0041,
      "step": 3954
    },
    {
      "epoch": 1.9745381927109336,
      "grad_norm": 0.1205383762717247,
      "learning_rate": 0.00048025461807289067,
      "loss": 0.0016,
      "step": 3955
    },
    {
      "epoch": 1.9750374438342486,
      "grad_norm": 0.24251405894756317,
      "learning_rate": 0.0004802496255616575,
      "loss": 0.0041,
      "step": 3956
    },
    {
      "epoch": 1.9755366949575637,
      "grad_norm": 0.060229454189538956,
      "learning_rate": 0.00048024463305042437,
      "loss": 0.0011,
      "step": 3957
    },
    {
      "epoch": 1.9760359460808787,
      "grad_norm": 0.12040986120700836,
      "learning_rate": 0.0004802396405391912,
      "loss": 0.0024,
      "step": 3958
    },
    {
      "epoch": 1.9765351972041936,
      "grad_norm": 0.1444094181060791,
      "learning_rate": 0.0004802346480279581,
      "loss": 0.0029,
      "step": 3959
    },
    {
      "epoch": 1.9770344483275086,
      "grad_norm": 0.022309942170977592,
      "learning_rate": 0.0004802296555167249,
      "loss": 0.0009,
      "step": 3960
    },
    {
      "epoch": 1.9775336994508237,
      "grad_norm": 0.03362879529595375,
      "learning_rate": 0.0004802246630054918,
      "loss": 0.0008,
      "step": 3961
    },
    {
      "epoch": 1.9780329505741387,
      "grad_norm": 0.19045503437519073,
      "learning_rate": 0.00048021967049425863,
      "loss": 0.002,
      "step": 3962
    },
    {
      "epoch": 1.9785322016974538,
      "grad_norm": 0.6646015048027039,
      "learning_rate": 0.0004802146779830255,
      "loss": 0.0292,
      "step": 3963
    },
    {
      "epoch": 1.9790314528207689,
      "grad_norm": 0.010499685071408749,
      "learning_rate": 0.00048020968547179233,
      "loss": 0.0006,
      "step": 3964
    },
    {
      "epoch": 1.979530703944084,
      "grad_norm": 0.2668319046497345,
      "learning_rate": 0.0004802046929605592,
      "loss": 0.0165,
      "step": 3965
    },
    {
      "epoch": 1.980029955067399,
      "grad_norm": 0.1301153004169464,
      "learning_rate": 0.00048019970044932604,
      "loss": 0.0016,
      "step": 3966
    },
    {
      "epoch": 1.980529206190714,
      "grad_norm": 0.44893816113471985,
      "learning_rate": 0.0004801947079380929,
      "loss": 0.0032,
      "step": 3967
    },
    {
      "epoch": 1.981028457314029,
      "grad_norm": 0.011155102401971817,
      "learning_rate": 0.00048018971542685974,
      "loss": 0.0005,
      "step": 3968
    },
    {
      "epoch": 1.981527708437344,
      "grad_norm": 0.3550621569156647,
      "learning_rate": 0.0004801847229156266,
      "loss": 0.0043,
      "step": 3969
    },
    {
      "epoch": 1.982026959560659,
      "grad_norm": 0.09735730290412903,
      "learning_rate": 0.00048017973040439345,
      "loss": 0.0016,
      "step": 3970
    },
    {
      "epoch": 1.982526210683974,
      "grad_norm": 0.04122201353311539,
      "learning_rate": 0.0004801747378931603,
      "loss": 0.0008,
      "step": 3971
    },
    {
      "epoch": 1.9830254618072891,
      "grad_norm": 0.18939979374408722,
      "learning_rate": 0.0004801697453819271,
      "loss": 0.0014,
      "step": 3972
    },
    {
      "epoch": 1.983524712930604,
      "grad_norm": 0.7223553657531738,
      "learning_rate": 0.00048016475287069395,
      "loss": 0.0181,
      "step": 3973
    },
    {
      "epoch": 1.984023964053919,
      "grad_norm": 0.2950001060962677,
      "learning_rate": 0.0004801597603594608,
      "loss": 0.0036,
      "step": 3974
    },
    {
      "epoch": 1.984523215177234,
      "grad_norm": 0.28746744990348816,
      "learning_rate": 0.00048015476784822765,
      "loss": 0.0032,
      "step": 3975
    },
    {
      "epoch": 1.9850224663005491,
      "grad_norm": 0.3931017816066742,
      "learning_rate": 0.0004801497753369945,
      "loss": 0.0063,
      "step": 3976
    },
    {
      "epoch": 1.9855217174238642,
      "grad_norm": 0.5860705375671387,
      "learning_rate": 0.00048014478282576136,
      "loss": 0.0114,
      "step": 3977
    },
    {
      "epoch": 1.9860209685471792,
      "grad_norm": 0.9828218817710876,
      "learning_rate": 0.0004801397903145282,
      "loss": 0.0026,
      "step": 3978
    },
    {
      "epoch": 1.9865202196704943,
      "grad_norm": 0.16355125606060028,
      "learning_rate": 0.00048013479780329506,
      "loss": 0.0035,
      "step": 3979
    },
    {
      "epoch": 1.9870194707938094,
      "grad_norm": 0.13313612341880798,
      "learning_rate": 0.0004801298052920619,
      "loss": 0.0026,
      "step": 3980
    },
    {
      "epoch": 1.9875187219171244,
      "grad_norm": 0.6707600355148315,
      "learning_rate": 0.00048012481278082876,
      "loss": 0.0097,
      "step": 3981
    },
    {
      "epoch": 1.9880179730404395,
      "grad_norm": 0.048019830137491226,
      "learning_rate": 0.0004801198202695956,
      "loss": 0.0008,
      "step": 3982
    },
    {
      "epoch": 1.9885172241637543,
      "grad_norm": 0.18180927634239197,
      "learning_rate": 0.00048011482775836247,
      "loss": 0.0034,
      "step": 3983
    },
    {
      "epoch": 1.9890164752870694,
      "grad_norm": 0.13841727375984192,
      "learning_rate": 0.0004801098352471293,
      "loss": 0.0017,
      "step": 3984
    },
    {
      "epoch": 1.9895157264103844,
      "grad_norm": 0.05548921972513199,
      "learning_rate": 0.0004801048427358962,
      "loss": 0.0011,
      "step": 3985
    },
    {
      "epoch": 1.9900149775336995,
      "grad_norm": 0.20513533055782318,
      "learning_rate": 0.000480099850224663,
      "loss": 0.0214,
      "step": 3986
    },
    {
      "epoch": 1.9905142286570143,
      "grad_norm": 0.2641230523586273,
      "learning_rate": 0.0004800948577134299,
      "loss": 0.0022,
      "step": 3987
    },
    {
      "epoch": 1.9910134797803294,
      "grad_norm": 2.0238428115844727,
      "learning_rate": 0.00048008986520219673,
      "loss": 0.0109,
      "step": 3988
    },
    {
      "epoch": 1.9915127309036444,
      "grad_norm": 0.020005429163575172,
      "learning_rate": 0.0004800848726909636,
      "loss": 0.0008,
      "step": 3989
    },
    {
      "epoch": 1.9920119820269595,
      "grad_norm": 0.018830396234989166,
      "learning_rate": 0.00048007988017973043,
      "loss": 0.0006,
      "step": 3990
    },
    {
      "epoch": 1.9925112331502746,
      "grad_norm": 0.130360409617424,
      "learning_rate": 0.0004800748876684973,
      "loss": 0.0022,
      "step": 3991
    },
    {
      "epoch": 1.9930104842735896,
      "grad_norm": 0.10992375016212463,
      "learning_rate": 0.00048006989515726414,
      "loss": 0.0019,
      "step": 3992
    },
    {
      "epoch": 1.9935097353969047,
      "grad_norm": 0.347903311252594,
      "learning_rate": 0.000480064902646031,
      "loss": 0.021,
      "step": 3993
    },
    {
      "epoch": 1.9940089865202197,
      "grad_norm": 0.056882891803979874,
      "learning_rate": 0.00048005991013479784,
      "loss": 0.0015,
      "step": 3994
    },
    {
      "epoch": 1.9945082376435348,
      "grad_norm": 0.21789419651031494,
      "learning_rate": 0.0004800549176235647,
      "loss": 0.0028,
      "step": 3995
    },
    {
      "epoch": 1.9950074887668499,
      "grad_norm": 0.060853131115436554,
      "learning_rate": 0.00048004992511233155,
      "loss": 0.0011,
      "step": 3996
    },
    {
      "epoch": 1.995506739890165,
      "grad_norm": 0.05420350655913353,
      "learning_rate": 0.0004800449326010984,
      "loss": 0.0012,
      "step": 3997
    },
    {
      "epoch": 1.9960059910134798,
      "grad_norm": 0.5222618579864502,
      "learning_rate": 0.00048003994008986525,
      "loss": 0.0293,
      "step": 3998
    },
    {
      "epoch": 1.9965052421367948,
      "grad_norm": 0.024764560163021088,
      "learning_rate": 0.0004800349475786321,
      "loss": 0.001,
      "step": 3999
    },
    {
      "epoch": 1.9970044932601099,
      "grad_norm": 0.1859295666217804,
      "learning_rate": 0.00048002995506739895,
      "loss": 0.0048,
      "step": 4000
    },
    {
      "epoch": 1.9975037443834247,
      "grad_norm": 0.09770026803016663,
      "learning_rate": 0.00048002496255616575,
      "loss": 0.0017,
      "step": 4001
    },
    {
      "epoch": 1.9980029955067398,
      "grad_norm": 0.14061839878559113,
      "learning_rate": 0.0004800199700449326,
      "loss": 0.0015,
      "step": 4002
    },
    {
      "epoch": 1.9985022466300548,
      "grad_norm": 0.12056996673345566,
      "learning_rate": 0.00048001497753369946,
      "loss": 0.0022,
      "step": 4003
    },
    {
      "epoch": 1.9990014977533699,
      "grad_norm": 0.6070502996444702,
      "learning_rate": 0.0004800099850224663,
      "loss": 0.0048,
      "step": 4004
    },
    {
      "epoch": 1.999500748876685,
      "grad_norm": 0.1410224884748459,
      "learning_rate": 0.00048000499251123316,
      "loss": 0.0019,
      "step": 4005
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.3535457253456116,
      "learning_rate": 0.00048,
      "loss": 0.0029,
      "step": 4006
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.2111537903547287,
      "eval_runtime": 2072.0523,
      "eval_samples_per_second": 2.829,
      "eval_steps_per_second": 0.236,
      "step": 4006
    },
    {
      "epoch": 2.000499251123315,
      "grad_norm": 0.017110703513026237,
      "learning_rate": 0.00047999500748876686,
      "loss": 0.0009,
      "step": 4007
    },
    {
      "epoch": 2.00099850224663,
      "grad_norm": 0.10882025957107544,
      "learning_rate": 0.0004799900149775337,
      "loss": 0.0016,
      "step": 4008
    },
    {
      "epoch": 2.001497753369945,
      "grad_norm": 0.5306007266044617,
      "learning_rate": 0.00047998502246630057,
      "loss": 0.0066,
      "step": 4009
    },
    {
      "epoch": 2.0019970044932602,
      "grad_norm": 1.526044487953186,
      "learning_rate": 0.0004799800299550674,
      "loss": 0.0131,
      "step": 4010
    },
    {
      "epoch": 2.0024962556165753,
      "grad_norm": 0.03944038972258568,
      "learning_rate": 0.00047997503744383427,
      "loss": 0.0012,
      "step": 4011
    },
    {
      "epoch": 2.0029955067398904,
      "grad_norm": 0.012117868289351463,
      "learning_rate": 0.0004799700449326011,
      "loss": 0.0008,
      "step": 4012
    },
    {
      "epoch": 2.0034947578632054,
      "grad_norm": 0.033250559121370316,
      "learning_rate": 0.000479965052421368,
      "loss": 0.001,
      "step": 4013
    },
    {
      "epoch": 2.00399400898652,
      "grad_norm": 0.1920216977596283,
      "learning_rate": 0.00047996005991013483,
      "loss": 0.0041,
      "step": 4014
    },
    {
      "epoch": 2.004493260109835,
      "grad_norm": 0.020290233194828033,
      "learning_rate": 0.0004799550673989017,
      "loss": 0.0009,
      "step": 4015
    },
    {
      "epoch": 2.00499251123315,
      "grad_norm": 0.27248963713645935,
      "learning_rate": 0.00047995007488766853,
      "loss": 0.0032,
      "step": 4016
    },
    {
      "epoch": 2.005491762356465,
      "grad_norm": 0.8235182762145996,
      "learning_rate": 0.0004799450823764354,
      "loss": 0.006,
      "step": 4017
    },
    {
      "epoch": 2.0059910134797803,
      "grad_norm": 1.5866433382034302,
      "learning_rate": 0.00047994008986520224,
      "loss": 0.026,
      "step": 4018
    },
    {
      "epoch": 2.0064902646030953,
      "grad_norm": 0.2201855182647705,
      "learning_rate": 0.0004799350973539691,
      "loss": 0.0026,
      "step": 4019
    },
    {
      "epoch": 2.0069895157264104,
      "grad_norm": 0.7409124374389648,
      "learning_rate": 0.00047993010484273594,
      "loss": 0.0206,
      "step": 4020
    },
    {
      "epoch": 2.0074887668497254,
      "grad_norm": 0.10457604378461838,
      "learning_rate": 0.0004799251123315028,
      "loss": 0.0024,
      "step": 4021
    },
    {
      "epoch": 2.0079880179730405,
      "grad_norm": 1.0660642385482788,
      "learning_rate": 0.00047992011982026965,
      "loss": 0.0054,
      "step": 4022
    },
    {
      "epoch": 2.0084872690963556,
      "grad_norm": 0.09359912574291229,
      "learning_rate": 0.00047991512730903644,
      "loss": 0.0021,
      "step": 4023
    },
    {
      "epoch": 2.0089865202196706,
      "grad_norm": 0.05907992273569107,
      "learning_rate": 0.0004799101347978033,
      "loss": 0.0011,
      "step": 4024
    },
    {
      "epoch": 2.0094857713429857,
      "grad_norm": 0.08222323656082153,
      "learning_rate": 0.00047990514228657015,
      "loss": 0.0018,
      "step": 4025
    },
    {
      "epoch": 2.0099850224663007,
      "grad_norm": 0.10273106396198273,
      "learning_rate": 0.000479900149775337,
      "loss": 0.0014,
      "step": 4026
    },
    {
      "epoch": 2.010484273589616,
      "grad_norm": 0.4602643549442291,
      "learning_rate": 0.00047989515726410385,
      "loss": 0.0305,
      "step": 4027
    },
    {
      "epoch": 2.0109835247129304,
      "grad_norm": 1.479437232017517,
      "learning_rate": 0.0004798901647528707,
      "loss": 0.0209,
      "step": 4028
    },
    {
      "epoch": 2.0114827758362455,
      "grad_norm": 0.6119838356971741,
      "learning_rate": 0.00047988517224163756,
      "loss": 0.0082,
      "step": 4029
    },
    {
      "epoch": 2.0119820269595605,
      "grad_norm": 0.9700179696083069,
      "learning_rate": 0.00047988017973040435,
      "loss": 0.0046,
      "step": 4030
    },
    {
      "epoch": 2.0124812780828756,
      "grad_norm": 1.2102798223495483,
      "learning_rate": 0.0004798751872191712,
      "loss": 0.0152,
      "step": 4031
    },
    {
      "epoch": 2.0129805292061906,
      "grad_norm": 0.2743605077266693,
      "learning_rate": 0.00047987019470793806,
      "loss": 0.002,
      "step": 4032
    },
    {
      "epoch": 2.0134797803295057,
      "grad_norm": 0.027280133217573166,
      "learning_rate": 0.0004798652021967049,
      "loss": 0.0014,
      "step": 4033
    },
    {
      "epoch": 2.0139790314528208,
      "grad_norm": 0.5258426666259766,
      "learning_rate": 0.00047986020968547176,
      "loss": 0.0038,
      "step": 4034
    },
    {
      "epoch": 2.014478282576136,
      "grad_norm": 0.04080238938331604,
      "learning_rate": 0.0004798552171742386,
      "loss": 0.001,
      "step": 4035
    },
    {
      "epoch": 2.014977533699451,
      "grad_norm": 1.0920789241790771,
      "learning_rate": 0.00047985022466300547,
      "loss": 0.0068,
      "step": 4036
    },
    {
      "epoch": 2.015476784822766,
      "grad_norm": 0.5020147562026978,
      "learning_rate": 0.0004798452321517723,
      "loss": 0.0116,
      "step": 4037
    },
    {
      "epoch": 2.015976035946081,
      "grad_norm": 0.41402724385261536,
      "learning_rate": 0.00047984023964053917,
      "loss": 0.008,
      "step": 4038
    },
    {
      "epoch": 2.016475287069396,
      "grad_norm": 0.3001719117164612,
      "learning_rate": 0.000479835247129306,
      "loss": 0.0075,
      "step": 4039
    },
    {
      "epoch": 2.016974538192711,
      "grad_norm": 0.700268030166626,
      "learning_rate": 0.0004798302546180729,
      "loss": 0.0144,
      "step": 4040
    },
    {
      "epoch": 2.017473789316026,
      "grad_norm": 0.9514129161834717,
      "learning_rate": 0.0004798252621068397,
      "loss": 0.0095,
      "step": 4041
    },
    {
      "epoch": 2.017973040439341,
      "grad_norm": 0.1155327558517456,
      "learning_rate": 0.0004798202695956066,
      "loss": 0.0032,
      "step": 4042
    },
    {
      "epoch": 2.018472291562656,
      "grad_norm": 0.048237867653369904,
      "learning_rate": 0.00047981527708437343,
      "loss": 0.0013,
      "step": 4043
    },
    {
      "epoch": 2.018971542685971,
      "grad_norm": 0.058704253286123276,
      "learning_rate": 0.0004798102845731403,
      "loss": 0.0018,
      "step": 4044
    },
    {
      "epoch": 2.019470793809286,
      "grad_norm": 0.43443918228149414,
      "learning_rate": 0.00047980529206190713,
      "loss": 0.011,
      "step": 4045
    },
    {
      "epoch": 2.019970044932601,
      "grad_norm": 0.5184845328330994,
      "learning_rate": 0.000479800299550674,
      "loss": 0.0048,
      "step": 4046
    },
    {
      "epoch": 2.020469296055916,
      "grad_norm": 0.013256244361400604,
      "learning_rate": 0.00047979530703944084,
      "loss": 0.0009,
      "step": 4047
    },
    {
      "epoch": 2.020968547179231,
      "grad_norm": 1.2762500047683716,
      "learning_rate": 0.0004797903145282077,
      "loss": 0.0111,
      "step": 4048
    },
    {
      "epoch": 2.021467798302546,
      "grad_norm": 0.14987218379974365,
      "learning_rate": 0.00047978532201697454,
      "loss": 0.0036,
      "step": 4049
    },
    {
      "epoch": 2.0219670494258613,
      "grad_norm": 0.049940284341573715,
      "learning_rate": 0.0004797803295057414,
      "loss": 0.0019,
      "step": 4050
    },
    {
      "epoch": 2.0224663005491763,
      "grad_norm": 0.2563113272190094,
      "learning_rate": 0.00047977533699450825,
      "loss": 0.0038,
      "step": 4051
    },
    {
      "epoch": 2.0229655516724914,
      "grad_norm": 1.025428056716919,
      "learning_rate": 0.0004797703444832751,
      "loss": 0.035,
      "step": 4052
    },
    {
      "epoch": 2.0234648027958064,
      "grad_norm": 0.6651381254196167,
      "learning_rate": 0.00047976535197204195,
      "loss": 0.015,
      "step": 4053
    },
    {
      "epoch": 2.0239640539191215,
      "grad_norm": 0.11275437474250793,
      "learning_rate": 0.0004797603594608088,
      "loss": 0.0033,
      "step": 4054
    },
    {
      "epoch": 2.0244633050424365,
      "grad_norm": 0.9893163442611694,
      "learning_rate": 0.00047975536694957566,
      "loss": 0.0046,
      "step": 4055
    },
    {
      "epoch": 2.024962556165751,
      "grad_norm": 0.2517881393432617,
      "learning_rate": 0.0004797503744383425,
      "loss": 0.0038,
      "step": 4056
    },
    {
      "epoch": 2.025461807289066,
      "grad_norm": 0.37132665514945984,
      "learning_rate": 0.00047974538192710936,
      "loss": 0.0038,
      "step": 4057
    },
    {
      "epoch": 2.0259610584123813,
      "grad_norm": 0.084510937333107,
      "learning_rate": 0.0004797403894158762,
      "loss": 0.0018,
      "step": 4058
    },
    {
      "epoch": 2.0264603095356963,
      "grad_norm": 0.8911846280097961,
      "learning_rate": 0.000479735396904643,
      "loss": 0.0113,
      "step": 4059
    },
    {
      "epoch": 2.0269595606590114,
      "grad_norm": 0.046832125633955,
      "learning_rate": 0.00047973040439340986,
      "loss": 0.0016,
      "step": 4060
    },
    {
      "epoch": 2.0274588117823265,
      "grad_norm": 0.5156230330467224,
      "learning_rate": 0.0004797254118821767,
      "loss": 0.0113,
      "step": 4061
    },
    {
      "epoch": 2.0279580629056415,
      "grad_norm": 0.49059823155403137,
      "learning_rate": 0.00047972041937094357,
      "loss": 0.0313,
      "step": 4062
    },
    {
      "epoch": 2.0284573140289566,
      "grad_norm": 0.1021210178732872,
      "learning_rate": 0.0004797154268597104,
      "loss": 0.0024,
      "step": 4063
    },
    {
      "epoch": 2.0289565651522716,
      "grad_norm": 0.2956770360469818,
      "learning_rate": 0.00047971043434847727,
      "loss": 0.0214,
      "step": 4064
    },
    {
      "epoch": 2.0294558162755867,
      "grad_norm": 0.17353542149066925,
      "learning_rate": 0.0004797054418372441,
      "loss": 0.0029,
      "step": 4065
    },
    {
      "epoch": 2.0299550673989017,
      "grad_norm": 0.13138391077518463,
      "learning_rate": 0.000479700449326011,
      "loss": 0.0025,
      "step": 4066
    },
    {
      "epoch": 2.030454318522217,
      "grad_norm": 0.3939555287361145,
      "learning_rate": 0.0004796954568147778,
      "loss": 0.0166,
      "step": 4067
    },
    {
      "epoch": 2.030953569645532,
      "grad_norm": 1.090133786201477,
      "learning_rate": 0.0004796904643035447,
      "loss": 0.0142,
      "step": 4068
    },
    {
      "epoch": 2.031452820768847,
      "grad_norm": 0.2543337643146515,
      "learning_rate": 0.00047968547179231153,
      "loss": 0.0042,
      "step": 4069
    },
    {
      "epoch": 2.031952071892162,
      "grad_norm": 0.4530984163284302,
      "learning_rate": 0.0004796804792810784,
      "loss": 0.0114,
      "step": 4070
    },
    {
      "epoch": 2.0324513230154766,
      "grad_norm": 0.19033300876617432,
      "learning_rate": 0.00047967548676984523,
      "loss": 0.0037,
      "step": 4071
    },
    {
      "epoch": 2.0329505741387917,
      "grad_norm": 0.3624039888381958,
      "learning_rate": 0.0004796704942586121,
      "loss": 0.0055,
      "step": 4072
    },
    {
      "epoch": 2.0334498252621067,
      "grad_norm": 0.02762700244784355,
      "learning_rate": 0.00047966550174737894,
      "loss": 0.0014,
      "step": 4073
    },
    {
      "epoch": 2.0339490763854218,
      "grad_norm": 0.05498436093330383,
      "learning_rate": 0.0004796605092361458,
      "loss": 0.0015,
      "step": 4074
    },
    {
      "epoch": 2.034448327508737,
      "grad_norm": 0.08347906172275543,
      "learning_rate": 0.00047965551672491264,
      "loss": 0.0026,
      "step": 4075
    },
    {
      "epoch": 2.034947578632052,
      "grad_norm": 0.22611598670482635,
      "learning_rate": 0.0004796505242136795,
      "loss": 0.004,
      "step": 4076
    },
    {
      "epoch": 2.035446829755367,
      "grad_norm": 0.3143165707588196,
      "learning_rate": 0.00047964553170244635,
      "loss": 0.0023,
      "step": 4077
    },
    {
      "epoch": 2.035946080878682,
      "grad_norm": 0.45764729380607605,
      "learning_rate": 0.0004796405391912132,
      "loss": 0.0045,
      "step": 4078
    },
    {
      "epoch": 2.036445332001997,
      "grad_norm": 2.7007687091827393,
      "learning_rate": 0.00047963554667998005,
      "loss": 0.0124,
      "step": 4079
    },
    {
      "epoch": 2.036944583125312,
      "grad_norm": 13.316133499145508,
      "learning_rate": 0.0004796305541687469,
      "loss": 0.0052,
      "step": 4080
    },
    {
      "epoch": 2.037443834248627,
      "grad_norm": 0.4791715145111084,
      "learning_rate": 0.00047962556165751375,
      "loss": 0.0054,
      "step": 4081
    },
    {
      "epoch": 2.0379430853719422,
      "grad_norm": 0.6554825901985168,
      "learning_rate": 0.0004796205691462806,
      "loss": 0.0207,
      "step": 4082
    },
    {
      "epoch": 2.0384423364952573,
      "grad_norm": 0.031863510608673096,
      "learning_rate": 0.00047961557663504746,
      "loss": 0.0012,
      "step": 4083
    },
    {
      "epoch": 2.0389415876185724,
      "grad_norm": 0.013963788747787476,
      "learning_rate": 0.0004796105841238143,
      "loss": 0.0007,
      "step": 4084
    },
    {
      "epoch": 2.039440838741887,
      "grad_norm": 0.5308322310447693,
      "learning_rate": 0.00047960559161258116,
      "loss": 0.0089,
      "step": 4085
    },
    {
      "epoch": 2.039940089865202,
      "grad_norm": 0.06184431165456772,
      "learning_rate": 0.000479600599101348,
      "loss": 0.0012,
      "step": 4086
    },
    {
      "epoch": 2.040439340988517,
      "grad_norm": 0.012535505928099155,
      "learning_rate": 0.00047959560659011487,
      "loss": 0.0008,
      "step": 4087
    },
    {
      "epoch": 2.040938592111832,
      "grad_norm": 0.3573664426803589,
      "learning_rate": 0.0004795906140788817,
      "loss": 0.0053,
      "step": 4088
    },
    {
      "epoch": 2.041437843235147,
      "grad_norm": 0.15661059319972992,
      "learning_rate": 0.0004795856215676485,
      "loss": 0.0018,
      "step": 4089
    },
    {
      "epoch": 2.0419370943584623,
      "grad_norm": 0.15198366343975067,
      "learning_rate": 0.00047958062905641537,
      "loss": 0.0033,
      "step": 4090
    },
    {
      "epoch": 2.0424363454817773,
      "grad_norm": 0.3741779327392578,
      "learning_rate": 0.0004795756365451822,
      "loss": 0.0066,
      "step": 4091
    },
    {
      "epoch": 2.0429355966050924,
      "grad_norm": 0.0761650875210762,
      "learning_rate": 0.0004795706440339491,
      "loss": 0.0014,
      "step": 4092
    },
    {
      "epoch": 2.0434348477284074,
      "grad_norm": 0.4966983497142792,
      "learning_rate": 0.0004795656515227159,
      "loss": 0.0102,
      "step": 4093
    },
    {
      "epoch": 2.0439340988517225,
      "grad_norm": 0.7458798289299011,
      "learning_rate": 0.0004795606590114828,
      "loss": 0.01,
      "step": 4094
    },
    {
      "epoch": 2.0444333499750376,
      "grad_norm": 0.9683207869529724,
      "learning_rate": 0.00047955566650024963,
      "loss": 0.0141,
      "step": 4095
    },
    {
      "epoch": 2.0449326010983526,
      "grad_norm": 0.371344655752182,
      "learning_rate": 0.0004795506739890165,
      "loss": 0.0129,
      "step": 4096
    },
    {
      "epoch": 2.0454318522216677,
      "grad_norm": 0.11377962678670883,
      "learning_rate": 0.00047954568147778333,
      "loss": 0.001,
      "step": 4097
    },
    {
      "epoch": 2.0459311033449827,
      "grad_norm": 0.014766079373657703,
      "learning_rate": 0.0004795406889665502,
      "loss": 0.0008,
      "step": 4098
    },
    {
      "epoch": 2.0464303544682974,
      "grad_norm": 0.023114653304219246,
      "learning_rate": 0.00047953569645531704,
      "loss": 0.001,
      "step": 4099
    },
    {
      "epoch": 2.0469296055916124,
      "grad_norm": 0.3748287260532379,
      "learning_rate": 0.0004795307039440839,
      "loss": 0.0082,
      "step": 4100
    },
    {
      "epoch": 2.0474288567149275,
      "grad_norm": 1.1063494682312012,
      "learning_rate": 0.00047952571143285074,
      "loss": 0.0195,
      "step": 4101
    },
    {
      "epoch": 2.0479281078382425,
      "grad_norm": 0.5168731212615967,
      "learning_rate": 0.0004795207189216176,
      "loss": 0.0273,
      "step": 4102
    },
    {
      "epoch": 2.0484273589615576,
      "grad_norm": 0.4591951370239258,
      "learning_rate": 0.00047951572641038445,
      "loss": 0.0145,
      "step": 4103
    },
    {
      "epoch": 2.0489266100848726,
      "grad_norm": 1.0329031944274902,
      "learning_rate": 0.0004795107338991513,
      "loss": 0.0193,
      "step": 4104
    },
    {
      "epoch": 2.0494258612081877,
      "grad_norm": 0.44262632727622986,
      "learning_rate": 0.00047950574138791815,
      "loss": 0.0229,
      "step": 4105
    },
    {
      "epoch": 2.0499251123315028,
      "grad_norm": 0.17182791233062744,
      "learning_rate": 0.000479500748876685,
      "loss": 0.0032,
      "step": 4106
    },
    {
      "epoch": 2.050424363454818,
      "grad_norm": 0.0453992635011673,
      "learning_rate": 0.00047949575636545185,
      "loss": 0.0016,
      "step": 4107
    },
    {
      "epoch": 2.050923614578133,
      "grad_norm": 0.8787567615509033,
      "learning_rate": 0.0004794907638542187,
      "loss": 0.0109,
      "step": 4108
    },
    {
      "epoch": 2.051422865701448,
      "grad_norm": 0.6055806875228882,
      "learning_rate": 0.00047948577134298556,
      "loss": 0.0132,
      "step": 4109
    },
    {
      "epoch": 2.051922116824763,
      "grad_norm": 0.5327054262161255,
      "learning_rate": 0.0004794807788317524,
      "loss": 0.0489,
      "step": 4110
    },
    {
      "epoch": 2.052421367948078,
      "grad_norm": 0.42591285705566406,
      "learning_rate": 0.00047947578632051926,
      "loss": 0.0084,
      "step": 4111
    },
    {
      "epoch": 2.052920619071393,
      "grad_norm": 1.3093754053115845,
      "learning_rate": 0.0004794707938092861,
      "loss": 0.035,
      "step": 4112
    },
    {
      "epoch": 2.0534198701947077,
      "grad_norm": 1.0789687633514404,
      "learning_rate": 0.00047946580129805297,
      "loss": 0.0148,
      "step": 4113
    },
    {
      "epoch": 2.053919121318023,
      "grad_norm": 0.08625742048025131,
      "learning_rate": 0.0004794608087868198,
      "loss": 0.0028,
      "step": 4114
    },
    {
      "epoch": 2.054418372441338,
      "grad_norm": 0.24237416684627533,
      "learning_rate": 0.00047945581627558667,
      "loss": 0.0037,
      "step": 4115
    },
    {
      "epoch": 2.054917623564653,
      "grad_norm": 0.6102145910263062,
      "learning_rate": 0.0004794508237643535,
      "loss": 0.0192,
      "step": 4116
    },
    {
      "epoch": 2.055416874687968,
      "grad_norm": 0.30458351969718933,
      "learning_rate": 0.0004794458312531204,
      "loss": 0.0079,
      "step": 4117
    },
    {
      "epoch": 2.055916125811283,
      "grad_norm": 0.6677407622337341,
      "learning_rate": 0.00047944083874188717,
      "loss": 0.0113,
      "step": 4118
    },
    {
      "epoch": 2.056415376934598,
      "grad_norm": 0.41714534163475037,
      "learning_rate": 0.000479435846230654,
      "loss": 0.0134,
      "step": 4119
    },
    {
      "epoch": 2.056914628057913,
      "grad_norm": 0.3573504388332367,
      "learning_rate": 0.0004794308537194209,
      "loss": 0.0108,
      "step": 4120
    },
    {
      "epoch": 2.057413879181228,
      "grad_norm": 0.5229579210281372,
      "learning_rate": 0.00047942586120818773,
      "loss": 0.0078,
      "step": 4121
    },
    {
      "epoch": 2.0579131303045433,
      "grad_norm": 0.20230023562908173,
      "learning_rate": 0.0004794208686969546,
      "loss": 0.0033,
      "step": 4122
    },
    {
      "epoch": 2.0584123814278583,
      "grad_norm": 0.02953183650970459,
      "learning_rate": 0.00047941587618572143,
      "loss": 0.0012,
      "step": 4123
    },
    {
      "epoch": 2.0589116325511734,
      "grad_norm": 0.4441116154193878,
      "learning_rate": 0.0004794108836744883,
      "loss": 0.0176,
      "step": 4124
    },
    {
      "epoch": 2.0594108836744884,
      "grad_norm": 0.0652848407626152,
      "learning_rate": 0.00047940589116325514,
      "loss": 0.0023,
      "step": 4125
    },
    {
      "epoch": 2.0599101347978035,
      "grad_norm": 0.22145938873291016,
      "learning_rate": 0.000479400898652022,
      "loss": 0.0041,
      "step": 4126
    },
    {
      "epoch": 2.060409385921118,
      "grad_norm": 0.25944164395332336,
      "learning_rate": 0.00047939590614078884,
      "loss": 0.0048,
      "step": 4127
    },
    {
      "epoch": 2.060908637044433,
      "grad_norm": 0.9217107892036438,
      "learning_rate": 0.0004793909136295557,
      "loss": 0.0287,
      "step": 4128
    },
    {
      "epoch": 2.0614078881677482,
      "grad_norm": 0.43317607045173645,
      "learning_rate": 0.00047938592111832255,
      "loss": 0.0099,
      "step": 4129
    },
    {
      "epoch": 2.0619071392910633,
      "grad_norm": 0.41649261116981506,
      "learning_rate": 0.0004793809286070894,
      "loss": 0.0041,
      "step": 4130
    },
    {
      "epoch": 2.0624063904143783,
      "grad_norm": 0.16381603479385376,
      "learning_rate": 0.00047937593609585625,
      "loss": 0.0027,
      "step": 4131
    },
    {
      "epoch": 2.0629056415376934,
      "grad_norm": 1.1362648010253906,
      "learning_rate": 0.0004793709435846231,
      "loss": 0.0391,
      "step": 4132
    },
    {
      "epoch": 2.0634048926610085,
      "grad_norm": 0.07820047438144684,
      "learning_rate": 0.00047936595107338995,
      "loss": 0.0018,
      "step": 4133
    },
    {
      "epoch": 2.0639041437843235,
      "grad_norm": 0.3387241065502167,
      "learning_rate": 0.0004793609585621568,
      "loss": 0.0072,
      "step": 4134
    },
    {
      "epoch": 2.0644033949076386,
      "grad_norm": 0.23649118840694427,
      "learning_rate": 0.00047935596605092366,
      "loss": 0.0039,
      "step": 4135
    },
    {
      "epoch": 2.0649026460309536,
      "grad_norm": 0.05911310389637947,
      "learning_rate": 0.00047935097353969046,
      "loss": 0.0019,
      "step": 4136
    },
    {
      "epoch": 2.0654018971542687,
      "grad_norm": 0.18809598684310913,
      "learning_rate": 0.0004793459810284573,
      "loss": 0.0046,
      "step": 4137
    },
    {
      "epoch": 2.0659011482775838,
      "grad_norm": 0.1688622534275055,
      "learning_rate": 0.00047934098851722416,
      "loss": 0.0035,
      "step": 4138
    },
    {
      "epoch": 2.066400399400899,
      "grad_norm": 0.08617391437292099,
      "learning_rate": 0.000479335996005991,
      "loss": 0.0024,
      "step": 4139
    },
    {
      "epoch": 2.066899650524214,
      "grad_norm": 0.618772029876709,
      "learning_rate": 0.00047933100349475786,
      "loss": 0.0059,
      "step": 4140
    },
    {
      "epoch": 2.067398901647529,
      "grad_norm": 0.2751348614692688,
      "learning_rate": 0.0004793260109835247,
      "loss": 0.0057,
      "step": 4141
    },
    {
      "epoch": 2.0678981527708435,
      "grad_norm": 1.1535576581954956,
      "learning_rate": 0.00047932101847229157,
      "loss": 0.0314,
      "step": 4142
    },
    {
      "epoch": 2.0683974038941586,
      "grad_norm": 0.21088676154613495,
      "learning_rate": 0.0004793160259610584,
      "loss": 0.006,
      "step": 4143
    },
    {
      "epoch": 2.0688966550174737,
      "grad_norm": 0.20278680324554443,
      "learning_rate": 0.00047931103344982527,
      "loss": 0.004,
      "step": 4144
    },
    {
      "epoch": 2.0693959061407887,
      "grad_norm": 0.43945664167404175,
      "learning_rate": 0.0004793060409385921,
      "loss": 0.0136,
      "step": 4145
    },
    {
      "epoch": 2.069895157264104,
      "grad_norm": 0.6289063692092896,
      "learning_rate": 0.000479301048427359,
      "loss": 0.0044,
      "step": 4146
    },
    {
      "epoch": 2.070394408387419,
      "grad_norm": 0.7214049696922302,
      "learning_rate": 0.0004792960559161258,
      "loss": 0.0113,
      "step": 4147
    },
    {
      "epoch": 2.070893659510734,
      "grad_norm": 0.03342685475945473,
      "learning_rate": 0.0004792910634048926,
      "loss": 0.0013,
      "step": 4148
    },
    {
      "epoch": 2.071392910634049,
      "grad_norm": 0.032662052661180496,
      "learning_rate": 0.0004792860708936595,
      "loss": 0.0016,
      "step": 4149
    },
    {
      "epoch": 2.071892161757364,
      "grad_norm": 1.0163838863372803,
      "learning_rate": 0.00047928107838242633,
      "loss": 0.0077,
      "step": 4150
    },
    {
      "epoch": 2.072391412880679,
      "grad_norm": 0.03155193105340004,
      "learning_rate": 0.0004792760858711932,
      "loss": 0.0013,
      "step": 4151
    },
    {
      "epoch": 2.072890664003994,
      "grad_norm": 0.3575563132762909,
      "learning_rate": 0.00047927109335996003,
      "loss": 0.0085,
      "step": 4152
    },
    {
      "epoch": 2.073389915127309,
      "grad_norm": 0.4247704744338989,
      "learning_rate": 0.0004792661008487269,
      "loss": 0.0106,
      "step": 4153
    },
    {
      "epoch": 2.0738891662506242,
      "grad_norm": 0.43270742893218994,
      "learning_rate": 0.00047926110833749374,
      "loss": 0.0051,
      "step": 4154
    },
    {
      "epoch": 2.074388417373939,
      "grad_norm": 1.182888388633728,
      "learning_rate": 0.0004792561158262606,
      "loss": 0.0083,
      "step": 4155
    },
    {
      "epoch": 2.074887668497254,
      "grad_norm": 0.23003171384334564,
      "learning_rate": 0.00047925112331502744,
      "loss": 0.0339,
      "step": 4156
    },
    {
      "epoch": 2.075386919620569,
      "grad_norm": 0.21599280834197998,
      "learning_rate": 0.0004792461308037943,
      "loss": 0.0036,
      "step": 4157
    },
    {
      "epoch": 2.075886170743884,
      "grad_norm": 0.12239139527082443,
      "learning_rate": 0.00047924113829256115,
      "loss": 0.0022,
      "step": 4158
    },
    {
      "epoch": 2.076385421867199,
      "grad_norm": 0.5689714550971985,
      "learning_rate": 0.000479236145781328,
      "loss": 0.0042,
      "step": 4159
    },
    {
      "epoch": 2.076884672990514,
      "grad_norm": 0.28151044249534607,
      "learning_rate": 0.00047923115327009485,
      "loss": 0.0041,
      "step": 4160
    },
    {
      "epoch": 2.077383924113829,
      "grad_norm": 0.637300968170166,
      "learning_rate": 0.0004792261607588617,
      "loss": 0.0042,
      "step": 4161
    },
    {
      "epoch": 2.0778831752371443,
      "grad_norm": 0.3115273714065552,
      "learning_rate": 0.00047922116824762856,
      "loss": 0.0033,
      "step": 4162
    },
    {
      "epoch": 2.0783824263604593,
      "grad_norm": 0.2571837902069092,
      "learning_rate": 0.0004792161757363954,
      "loss": 0.0167,
      "step": 4163
    },
    {
      "epoch": 2.0788816774837744,
      "grad_norm": 0.14546354115009308,
      "learning_rate": 0.00047921118322516226,
      "loss": 0.0034,
      "step": 4164
    },
    {
      "epoch": 2.0793809286070895,
      "grad_norm": 0.17240269482135773,
      "learning_rate": 0.0004792061907139291,
      "loss": 0.0036,
      "step": 4165
    },
    {
      "epoch": 2.0798801797304045,
      "grad_norm": 0.22609993815422058,
      "learning_rate": 0.00047920119820269596,
      "loss": 0.0045,
      "step": 4166
    },
    {
      "epoch": 2.0803794308537196,
      "grad_norm": 0.15760810673236847,
      "learning_rate": 0.0004791962056914628,
      "loss": 0.0034,
      "step": 4167
    },
    {
      "epoch": 2.0808786819770346,
      "grad_norm": 0.27872464060783386,
      "learning_rate": 0.00047919121318022967,
      "loss": 0.0115,
      "step": 4168
    },
    {
      "epoch": 2.0813779331003497,
      "grad_norm": 0.27688318490982056,
      "learning_rate": 0.0004791862206689965,
      "loss": 0.0043,
      "step": 4169
    },
    {
      "epoch": 2.0818771842236643,
      "grad_norm": 0.047796010971069336,
      "learning_rate": 0.00047918122815776337,
      "loss": 0.0012,
      "step": 4170
    },
    {
      "epoch": 2.0823764353469794,
      "grad_norm": 0.045953039079904556,
      "learning_rate": 0.0004791762356465302,
      "loss": 0.0013,
      "step": 4171
    },
    {
      "epoch": 2.0828756864702944,
      "grad_norm": 0.027856091037392616,
      "learning_rate": 0.0004791712431352971,
      "loss": 0.0011,
      "step": 4172
    },
    {
      "epoch": 2.0833749375936095,
      "grad_norm": 0.0802415981888771,
      "learning_rate": 0.00047916625062406393,
      "loss": 0.0022,
      "step": 4173
    },
    {
      "epoch": 2.0838741887169245,
      "grad_norm": 0.8629623055458069,
      "learning_rate": 0.0004791612581128308,
      "loss": 0.014,
      "step": 4174
    },
    {
      "epoch": 2.0843734398402396,
      "grad_norm": 0.3994506299495697,
      "learning_rate": 0.00047915626560159763,
      "loss": 0.003,
      "step": 4175
    },
    {
      "epoch": 2.0848726909635547,
      "grad_norm": 0.20633564889431,
      "learning_rate": 0.00047915127309036443,
      "loss": 0.0044,
      "step": 4176
    },
    {
      "epoch": 2.0853719420868697,
      "grad_norm": 0.15933316946029663,
      "learning_rate": 0.0004791462805791313,
      "loss": 0.0024,
      "step": 4177
    },
    {
      "epoch": 2.0858711932101848,
      "grad_norm": 1.129379391670227,
      "learning_rate": 0.00047914128806789813,
      "loss": 0.0237,
      "step": 4178
    },
    {
      "epoch": 2.0863704443335,
      "grad_norm": 0.22740450501441956,
      "learning_rate": 0.000479136295556665,
      "loss": 0.0022,
      "step": 4179
    },
    {
      "epoch": 2.086869695456815,
      "grad_norm": 0.22982704639434814,
      "learning_rate": 0.00047913130304543184,
      "loss": 0.0153,
      "step": 4180
    },
    {
      "epoch": 2.08736894658013,
      "grad_norm": 0.44808387756347656,
      "learning_rate": 0.0004791263105341987,
      "loss": 0.006,
      "step": 4181
    },
    {
      "epoch": 2.087868197703445,
      "grad_norm": 0.6333979964256287,
      "learning_rate": 0.00047912131802296554,
      "loss": 0.0097,
      "step": 4182
    },
    {
      "epoch": 2.08836744882676,
      "grad_norm": 0.4186096787452698,
      "learning_rate": 0.0004791163255117324,
      "loss": 0.0157,
      "step": 4183
    },
    {
      "epoch": 2.0888666999500747,
      "grad_norm": 0.6083394885063171,
      "learning_rate": 0.00047911133300049925,
      "loss": 0.0075,
      "step": 4184
    },
    {
      "epoch": 2.0893659510733897,
      "grad_norm": 0.029591267928481102,
      "learning_rate": 0.0004791063404892661,
      "loss": 0.001,
      "step": 4185
    },
    {
      "epoch": 2.089865202196705,
      "grad_norm": 0.3204606771469116,
      "learning_rate": 0.00047910134797803295,
      "loss": 0.0031,
      "step": 4186
    },
    {
      "epoch": 2.09036445332002,
      "grad_norm": 1.0596448183059692,
      "learning_rate": 0.0004790963554667998,
      "loss": 0.0554,
      "step": 4187
    },
    {
      "epoch": 2.090863704443335,
      "grad_norm": 2.5880048274993896,
      "learning_rate": 0.00047909136295556665,
      "loss": 0.0079,
      "step": 4188
    },
    {
      "epoch": 2.09136295556665,
      "grad_norm": 0.03431089222431183,
      "learning_rate": 0.0004790863704443335,
      "loss": 0.0014,
      "step": 4189
    },
    {
      "epoch": 2.091862206689965,
      "grad_norm": 2.366218090057373,
      "learning_rate": 0.00047908137793310036,
      "loss": 0.0092,
      "step": 4190
    },
    {
      "epoch": 2.09236145781328,
      "grad_norm": 0.13541759550571442,
      "learning_rate": 0.0004790763854218672,
      "loss": 0.0027,
      "step": 4191
    },
    {
      "epoch": 2.092860708936595,
      "grad_norm": 0.7369152903556824,
      "learning_rate": 0.00047907139291063406,
      "loss": 0.008,
      "step": 4192
    },
    {
      "epoch": 2.09335996005991,
      "grad_norm": 0.1047133281826973,
      "learning_rate": 0.0004790664003994009,
      "loss": 0.0028,
      "step": 4193
    },
    {
      "epoch": 2.0938592111832253,
      "grad_norm": 0.5922781825065613,
      "learning_rate": 0.00047906140788816777,
      "loss": 0.0276,
      "step": 4194
    },
    {
      "epoch": 2.0943584623065403,
      "grad_norm": 0.2810475826263428,
      "learning_rate": 0.0004790564153769346,
      "loss": 0.0065,
      "step": 4195
    },
    {
      "epoch": 2.0948577134298554,
      "grad_norm": 0.6004721522331238,
      "learning_rate": 0.00047905142286570147,
      "loss": 0.0194,
      "step": 4196
    },
    {
      "epoch": 2.0953569645531704,
      "grad_norm": 0.12669458985328674,
      "learning_rate": 0.0004790464303544683,
      "loss": 0.0032,
      "step": 4197
    },
    {
      "epoch": 2.095856215676485,
      "grad_norm": 0.1704285889863968,
      "learning_rate": 0.0004790414378432352,
      "loss": 0.0035,
      "step": 4198
    },
    {
      "epoch": 2.0963554667998,
      "grad_norm": 0.2693943679332733,
      "learning_rate": 0.00047903644533200203,
      "loss": 0.0147,
      "step": 4199
    },
    {
      "epoch": 2.096854717923115,
      "grad_norm": 0.5445719361305237,
      "learning_rate": 0.0004790314528207689,
      "loss": 0.0164,
      "step": 4200
    },
    {
      "epoch": 2.0973539690464302,
      "grad_norm": 0.3344060480594635,
      "learning_rate": 0.00047902646030953573,
      "loss": 0.0041,
      "step": 4201
    },
    {
      "epoch": 2.0978532201697453,
      "grad_norm": 0.016462190076708794,
      "learning_rate": 0.0004790214677983026,
      "loss": 0.0009,
      "step": 4202
    },
    {
      "epoch": 2.0983524712930604,
      "grad_norm": 0.22583073377609253,
      "learning_rate": 0.00047901647528706944,
      "loss": 0.0024,
      "step": 4203
    },
    {
      "epoch": 2.0988517224163754,
      "grad_norm": 0.05186684429645538,
      "learning_rate": 0.0004790114827758363,
      "loss": 0.0009,
      "step": 4204
    },
    {
      "epoch": 2.0993509735396905,
      "grad_norm": 0.2333008050918579,
      "learning_rate": 0.0004790064902646031,
      "loss": 0.0049,
      "step": 4205
    },
    {
      "epoch": 2.0998502246630055,
      "grad_norm": 0.4685685932636261,
      "learning_rate": 0.00047900149775336994,
      "loss": 0.0147,
      "step": 4206
    },
    {
      "epoch": 2.1003494757863206,
      "grad_norm": 0.48186492919921875,
      "learning_rate": 0.0004789965052421368,
      "loss": 0.0187,
      "step": 4207
    },
    {
      "epoch": 2.1008487269096356,
      "grad_norm": 0.07937933504581451,
      "learning_rate": 0.00047899151273090364,
      "loss": 0.0021,
      "step": 4208
    },
    {
      "epoch": 2.1013479780329507,
      "grad_norm": 0.13594909012317657,
      "learning_rate": 0.0004789865202196705,
      "loss": 0.0033,
      "step": 4209
    },
    {
      "epoch": 2.1018472291562658,
      "grad_norm": 0.4388132691383362,
      "learning_rate": 0.00047898152770843735,
      "loss": 0.0116,
      "step": 4210
    },
    {
      "epoch": 2.102346480279581,
      "grad_norm": 0.06941938400268555,
      "learning_rate": 0.0004789765351972042,
      "loss": 0.0024,
      "step": 4211
    },
    {
      "epoch": 2.102845731402896,
      "grad_norm": 0.15185849368572235,
      "learning_rate": 0.00047897154268597105,
      "loss": 0.0038,
      "step": 4212
    },
    {
      "epoch": 2.1033449825262105,
      "grad_norm": 0.8405464291572571,
      "learning_rate": 0.0004789665501747379,
      "loss": 0.0077,
      "step": 4213
    },
    {
      "epoch": 2.1038442336495256,
      "grad_norm": 0.02354852668941021,
      "learning_rate": 0.00047896155766350475,
      "loss": 0.0009,
      "step": 4214
    },
    {
      "epoch": 2.1043434847728406,
      "grad_norm": 0.3854008913040161,
      "learning_rate": 0.0004789565651522716,
      "loss": 0.0081,
      "step": 4215
    },
    {
      "epoch": 2.1048427358961557,
      "grad_norm": 0.023927345871925354,
      "learning_rate": 0.00047895157264103846,
      "loss": 0.0011,
      "step": 4216
    },
    {
      "epoch": 2.1053419870194707,
      "grad_norm": 0.20074261724948883,
      "learning_rate": 0.0004789465801298053,
      "loss": 0.0035,
      "step": 4217
    },
    {
      "epoch": 2.105841238142786,
      "grad_norm": 0.12384046614170074,
      "learning_rate": 0.00047894158761857216,
      "loss": 0.003,
      "step": 4218
    },
    {
      "epoch": 2.106340489266101,
      "grad_norm": 0.33324357867240906,
      "learning_rate": 0.000478936595107339,
      "loss": 0.006,
      "step": 4219
    },
    {
      "epoch": 2.106839740389416,
      "grad_norm": 0.27616995573043823,
      "learning_rate": 0.00047893160259610587,
      "loss": 0.004,
      "step": 4220
    },
    {
      "epoch": 2.107338991512731,
      "grad_norm": 0.12162356078624725,
      "learning_rate": 0.0004789266100848727,
      "loss": 0.0012,
      "step": 4221
    },
    {
      "epoch": 2.107838242636046,
      "grad_norm": 0.4984302222728729,
      "learning_rate": 0.00047892161757363957,
      "loss": 0.0193,
      "step": 4222
    },
    {
      "epoch": 2.108337493759361,
      "grad_norm": 0.1260649561882019,
      "learning_rate": 0.0004789166250624064,
      "loss": 0.0036,
      "step": 4223
    },
    {
      "epoch": 2.108836744882676,
      "grad_norm": 0.16708993911743164,
      "learning_rate": 0.0004789116325511733,
      "loss": 0.0032,
      "step": 4224
    },
    {
      "epoch": 2.109335996005991,
      "grad_norm": 0.23619402945041656,
      "learning_rate": 0.0004789066400399401,
      "loss": 0.0041,
      "step": 4225
    },
    {
      "epoch": 2.109835247129306,
      "grad_norm": 0.08719122409820557,
      "learning_rate": 0.000478901647528707,
      "loss": 0.0023,
      "step": 4226
    },
    {
      "epoch": 2.110334498252621,
      "grad_norm": 0.2644976079463959,
      "learning_rate": 0.00047889665501747383,
      "loss": 0.0032,
      "step": 4227
    },
    {
      "epoch": 2.110833749375936,
      "grad_norm": 0.35046207904815674,
      "learning_rate": 0.0004788916625062407,
      "loss": 0.0084,
      "step": 4228
    },
    {
      "epoch": 2.111333000499251,
      "grad_norm": 0.1324446052312851,
      "learning_rate": 0.00047888666999500754,
      "loss": 0.004,
      "step": 4229
    },
    {
      "epoch": 2.111832251622566,
      "grad_norm": 0.16784389317035675,
      "learning_rate": 0.0004788816774837744,
      "loss": 0.0026,
      "step": 4230
    },
    {
      "epoch": 2.112331502745881,
      "grad_norm": 0.028632020577788353,
      "learning_rate": 0.00047887668497254124,
      "loss": 0.0009,
      "step": 4231
    },
    {
      "epoch": 2.112830753869196,
      "grad_norm": 0.01676456443965435,
      "learning_rate": 0.0004788716924613081,
      "loss": 0.0008,
      "step": 4232
    },
    {
      "epoch": 2.1133300049925112,
      "grad_norm": 0.6056499481201172,
      "learning_rate": 0.00047886669995007494,
      "loss": 0.0094,
      "step": 4233
    },
    {
      "epoch": 2.1138292561158263,
      "grad_norm": 0.12662957608699799,
      "learning_rate": 0.00047886170743884174,
      "loss": 0.0016,
      "step": 4234
    },
    {
      "epoch": 2.1143285072391413,
      "grad_norm": 0.16045325994491577,
      "learning_rate": 0.0004788567149276086,
      "loss": 0.0022,
      "step": 4235
    },
    {
      "epoch": 2.1148277583624564,
      "grad_norm": 0.5149704217910767,
      "learning_rate": 0.00047885172241637545,
      "loss": 0.0075,
      "step": 4236
    },
    {
      "epoch": 2.1153270094857715,
      "grad_norm": 0.9472490549087524,
      "learning_rate": 0.0004788467299051423,
      "loss": 0.0119,
      "step": 4237
    },
    {
      "epoch": 2.1158262606090865,
      "grad_norm": 0.6617457270622253,
      "learning_rate": 0.00047884173739390915,
      "loss": 0.028,
      "step": 4238
    },
    {
      "epoch": 2.1163255117324016,
      "grad_norm": 0.18570001423358917,
      "learning_rate": 0.000478836744882676,
      "loss": 0.0051,
      "step": 4239
    },
    {
      "epoch": 2.1168247628557166,
      "grad_norm": 0.12061212211847305,
      "learning_rate": 0.00047883175237144285,
      "loss": 0.0018,
      "step": 4240
    },
    {
      "epoch": 2.1173240139790312,
      "grad_norm": 0.34429091215133667,
      "learning_rate": 0.0004788267598602097,
      "loss": 0.0055,
      "step": 4241
    },
    {
      "epoch": 2.1178232651023463,
      "grad_norm": 0.5477898120880127,
      "learning_rate": 0.00047882176734897656,
      "loss": 0.0112,
      "step": 4242
    },
    {
      "epoch": 2.1183225162256614,
      "grad_norm": 0.6271489858627319,
      "learning_rate": 0.0004788167748377434,
      "loss": 0.0078,
      "step": 4243
    },
    {
      "epoch": 2.1188217673489764,
      "grad_norm": 0.8686397671699524,
      "learning_rate": 0.00047881178232651026,
      "loss": 0.0065,
      "step": 4244
    },
    {
      "epoch": 2.1193210184722915,
      "grad_norm": 0.026434611529111862,
      "learning_rate": 0.0004788067898152771,
      "loss": 0.0009,
      "step": 4245
    },
    {
      "epoch": 2.1198202695956065,
      "grad_norm": 0.13131433725357056,
      "learning_rate": 0.00047880179730404397,
      "loss": 0.0029,
      "step": 4246
    },
    {
      "epoch": 2.1203195207189216,
      "grad_norm": 0.43309286236763,
      "learning_rate": 0.0004787968047928108,
      "loss": 0.0057,
      "step": 4247
    },
    {
      "epoch": 2.1208187718422367,
      "grad_norm": 0.06669566035270691,
      "learning_rate": 0.00047879181228157767,
      "loss": 0.0012,
      "step": 4248
    },
    {
      "epoch": 2.1213180229655517,
      "grad_norm": 0.40630364418029785,
      "learning_rate": 0.00047878681977034447,
      "loss": 0.0056,
      "step": 4249
    },
    {
      "epoch": 2.121817274088867,
      "grad_norm": 0.0886572077870369,
      "learning_rate": 0.0004787818272591113,
      "loss": 0.0023,
      "step": 4250
    },
    {
      "epoch": 2.122316525212182,
      "grad_norm": 0.03446483239531517,
      "learning_rate": 0.00047877683474787817,
      "loss": 0.0011,
      "step": 4251
    },
    {
      "epoch": 2.122815776335497,
      "grad_norm": 0.1454792022705078,
      "learning_rate": 0.000478771842236645,
      "loss": 0.0031,
      "step": 4252
    },
    {
      "epoch": 2.123315027458812,
      "grad_norm": 0.28079092502593994,
      "learning_rate": 0.0004787668497254119,
      "loss": 0.007,
      "step": 4253
    },
    {
      "epoch": 2.123814278582127,
      "grad_norm": 0.21956288814544678,
      "learning_rate": 0.00047876185721417873,
      "loss": 0.0057,
      "step": 4254
    },
    {
      "epoch": 2.1243135297054416,
      "grad_norm": 0.3038505017757416,
      "learning_rate": 0.0004787568647029456,
      "loss": 0.0045,
      "step": 4255
    },
    {
      "epoch": 2.1248127808287567,
      "grad_norm": 0.8200212717056274,
      "learning_rate": 0.00047875187219171243,
      "loss": 0.0037,
      "step": 4256
    },
    {
      "epoch": 2.1253120319520717,
      "grad_norm": 0.9965682625770569,
      "learning_rate": 0.0004787468796804793,
      "loss": 0.0052,
      "step": 4257
    },
    {
      "epoch": 2.125811283075387,
      "grad_norm": 0.6980131268501282,
      "learning_rate": 0.00047874188716924614,
      "loss": 0.0089,
      "step": 4258
    },
    {
      "epoch": 2.126310534198702,
      "grad_norm": 0.20148210227489471,
      "learning_rate": 0.000478736894658013,
      "loss": 0.0027,
      "step": 4259
    },
    {
      "epoch": 2.126809785322017,
      "grad_norm": 0.20005199313163757,
      "learning_rate": 0.00047873190214677984,
      "loss": 0.0049,
      "step": 4260
    },
    {
      "epoch": 2.127309036445332,
      "grad_norm": 0.4563153386116028,
      "learning_rate": 0.0004787269096355467,
      "loss": 0.008,
      "step": 4261
    },
    {
      "epoch": 2.127808287568647,
      "grad_norm": 0.4669989347457886,
      "learning_rate": 0.00047872191712431354,
      "loss": 0.0246,
      "step": 4262
    },
    {
      "epoch": 2.128307538691962,
      "grad_norm": 0.6743307709693909,
      "learning_rate": 0.00047871692461308034,
      "loss": 0.0136,
      "step": 4263
    },
    {
      "epoch": 2.128806789815277,
      "grad_norm": 0.5335791110992432,
      "learning_rate": 0.0004787119321018472,
      "loss": 0.0132,
      "step": 4264
    },
    {
      "epoch": 2.129306040938592,
      "grad_norm": 0.03739175572991371,
      "learning_rate": 0.00047870693959061405,
      "loss": 0.0013,
      "step": 4265
    },
    {
      "epoch": 2.1298052920619073,
      "grad_norm": 1.45094633102417,
      "learning_rate": 0.0004787019470793809,
      "loss": 0.0131,
      "step": 4266
    },
    {
      "epoch": 2.1303045431852223,
      "grad_norm": 0.05584190785884857,
      "learning_rate": 0.00047869695456814775,
      "loss": 0.0018,
      "step": 4267
    },
    {
      "epoch": 2.1308037943085374,
      "grad_norm": 0.8178831934928894,
      "learning_rate": 0.0004786919620569146,
      "loss": 0.0049,
      "step": 4268
    },
    {
      "epoch": 2.131303045431852,
      "grad_norm": 0.07865826040506363,
      "learning_rate": 0.00047868696954568146,
      "loss": 0.0018,
      "step": 4269
    },
    {
      "epoch": 2.131802296555167,
      "grad_norm": 0.2472466677427292,
      "learning_rate": 0.0004786819770344483,
      "loss": 0.0031,
      "step": 4270
    },
    {
      "epoch": 2.132301547678482,
      "grad_norm": 0.12760327756404877,
      "learning_rate": 0.00047867698452321516,
      "loss": 0.0035,
      "step": 4271
    },
    {
      "epoch": 2.132800798801797,
      "grad_norm": 0.5373837947845459,
      "learning_rate": 0.000478671992011982,
      "loss": 0.0032,
      "step": 4272
    },
    {
      "epoch": 2.1333000499251122,
      "grad_norm": 0.16661660373210907,
      "learning_rate": 0.00047866699950074886,
      "loss": 0.0028,
      "step": 4273
    },
    {
      "epoch": 2.1337993010484273,
      "grad_norm": 0.09731712192296982,
      "learning_rate": 0.0004786620069895157,
      "loss": 0.0021,
      "step": 4274
    },
    {
      "epoch": 2.1342985521717424,
      "grad_norm": 0.27594372630119324,
      "learning_rate": 0.00047865701447828257,
      "loss": 0.0146,
      "step": 4275
    },
    {
      "epoch": 2.1347978032950574,
      "grad_norm": 0.5343478918075562,
      "learning_rate": 0.0004786520219670494,
      "loss": 0.0094,
      "step": 4276
    },
    {
      "epoch": 2.1352970544183725,
      "grad_norm": 0.5430163145065308,
      "learning_rate": 0.00047864702945581627,
      "loss": 0.0083,
      "step": 4277
    },
    {
      "epoch": 2.1357963055416875,
      "grad_norm": 0.23989886045455933,
      "learning_rate": 0.0004786420369445831,
      "loss": 0.0044,
      "step": 4278
    },
    {
      "epoch": 2.1362955566650026,
      "grad_norm": 0.40404215455055237,
      "learning_rate": 0.00047863704443335,
      "loss": 0.0051,
      "step": 4279
    },
    {
      "epoch": 2.1367948077883177,
      "grad_norm": 0.3896756172180176,
      "learning_rate": 0.00047863205192211683,
      "loss": 0.0063,
      "step": 4280
    },
    {
      "epoch": 2.1372940589116327,
      "grad_norm": 0.018093150109052658,
      "learning_rate": 0.0004786270594108837,
      "loss": 0.001,
      "step": 4281
    },
    {
      "epoch": 2.1377933100349478,
      "grad_norm": 0.17880931496620178,
      "learning_rate": 0.00047862206689965053,
      "loss": 0.0018,
      "step": 4282
    },
    {
      "epoch": 2.138292561158263,
      "grad_norm": 0.8820660710334778,
      "learning_rate": 0.0004786170743884174,
      "loss": 0.0111,
      "step": 4283
    },
    {
      "epoch": 2.1387918122815774,
      "grad_norm": 0.2463408261537552,
      "learning_rate": 0.00047861208187718424,
      "loss": 0.0263,
      "step": 4284
    },
    {
      "epoch": 2.1392910634048925,
      "grad_norm": 0.30413100123405457,
      "learning_rate": 0.0004786070893659511,
      "loss": 0.0029,
      "step": 4285
    },
    {
      "epoch": 2.1397903145282076,
      "grad_norm": 0.27921563386917114,
      "learning_rate": 0.00047860209685471794,
      "loss": 0.0032,
      "step": 4286
    },
    {
      "epoch": 2.1402895656515226,
      "grad_norm": 0.030622929334640503,
      "learning_rate": 0.0004785971043434848,
      "loss": 0.0011,
      "step": 4287
    },
    {
      "epoch": 2.1407888167748377,
      "grad_norm": 0.2624208629131317,
      "learning_rate": 0.00047859211183225164,
      "loss": 0.0024,
      "step": 4288
    },
    {
      "epoch": 2.1412880678981527,
      "grad_norm": 0.7222398519515991,
      "learning_rate": 0.0004785871193210185,
      "loss": 0.005,
      "step": 4289
    },
    {
      "epoch": 2.141787319021468,
      "grad_norm": 0.3510355055332184,
      "learning_rate": 0.00047858212680978535,
      "loss": 0.0066,
      "step": 4290
    },
    {
      "epoch": 2.142286570144783,
      "grad_norm": 1.0316746234893799,
      "learning_rate": 0.0004785771342985522,
      "loss": 0.0038,
      "step": 4291
    },
    {
      "epoch": 2.142785821268098,
      "grad_norm": 0.03541725501418114,
      "learning_rate": 0.000478572141787319,
      "loss": 0.001,
      "step": 4292
    },
    {
      "epoch": 2.143285072391413,
      "grad_norm": 0.05536981299519539,
      "learning_rate": 0.00047856714927608585,
      "loss": 0.0013,
      "step": 4293
    },
    {
      "epoch": 2.143784323514728,
      "grad_norm": 0.4301287829875946,
      "learning_rate": 0.0004785621567648527,
      "loss": 0.0184,
      "step": 4294
    },
    {
      "epoch": 2.144283574638043,
      "grad_norm": 0.34849900007247925,
      "learning_rate": 0.00047855716425361955,
      "loss": 0.0304,
      "step": 4295
    },
    {
      "epoch": 2.144782825761358,
      "grad_norm": 1.1359922885894775,
      "learning_rate": 0.0004785521717423864,
      "loss": 0.0075,
      "step": 4296
    },
    {
      "epoch": 2.1452820768846728,
      "grad_norm": 0.49650856852531433,
      "learning_rate": 0.00047854717923115326,
      "loss": 0.0079,
      "step": 4297
    },
    {
      "epoch": 2.145781328007988,
      "grad_norm": 0.3099532127380371,
      "learning_rate": 0.0004785421867199201,
      "loss": 0.0114,
      "step": 4298
    },
    {
      "epoch": 2.146280579131303,
      "grad_norm": 0.027268391102552414,
      "learning_rate": 0.00047853719420868696,
      "loss": 0.0011,
      "step": 4299
    },
    {
      "epoch": 2.146779830254618,
      "grad_norm": 0.4174255430698395,
      "learning_rate": 0.0004785322016974538,
      "loss": 0.0029,
      "step": 4300
    },
    {
      "epoch": 2.147279081377933,
      "grad_norm": 0.09622964262962341,
      "learning_rate": 0.00047852720918622067,
      "loss": 0.002,
      "step": 4301
    },
    {
      "epoch": 2.147778332501248,
      "grad_norm": 0.19657185673713684,
      "learning_rate": 0.0004785222166749875,
      "loss": 0.0044,
      "step": 4302
    },
    {
      "epoch": 2.148277583624563,
      "grad_norm": 0.12340713292360306,
      "learning_rate": 0.00047851722416375437,
      "loss": 0.0017,
      "step": 4303
    },
    {
      "epoch": 2.148776834747878,
      "grad_norm": 0.08015483617782593,
      "learning_rate": 0.0004785122316525212,
      "loss": 0.0021,
      "step": 4304
    },
    {
      "epoch": 2.1492760858711932,
      "grad_norm": 0.04630036652088165,
      "learning_rate": 0.0004785072391412881,
      "loss": 0.0011,
      "step": 4305
    },
    {
      "epoch": 2.1497753369945083,
      "grad_norm": 0.14662225544452667,
      "learning_rate": 0.00047850224663005493,
      "loss": 0.0028,
      "step": 4306
    },
    {
      "epoch": 2.1502745881178233,
      "grad_norm": 0.024425925686955452,
      "learning_rate": 0.0004784972541188218,
      "loss": 0.0012,
      "step": 4307
    },
    {
      "epoch": 2.1507738392411384,
      "grad_norm": 0.056119974702596664,
      "learning_rate": 0.00047849226160758863,
      "loss": 0.0019,
      "step": 4308
    },
    {
      "epoch": 2.1512730903644535,
      "grad_norm": 0.589147686958313,
      "learning_rate": 0.0004784872690963555,
      "loss": 0.0072,
      "step": 4309
    },
    {
      "epoch": 2.1517723414877685,
      "grad_norm": 0.29125916957855225,
      "learning_rate": 0.00047848227658512234,
      "loss": 0.0064,
      "step": 4310
    },
    {
      "epoch": 2.1522715926110836,
      "grad_norm": 0.04808275029063225,
      "learning_rate": 0.0004784772840738892,
      "loss": 0.0013,
      "step": 4311
    },
    {
      "epoch": 2.152770843734398,
      "grad_norm": 0.059287745505571365,
      "learning_rate": 0.00047847229156265604,
      "loss": 0.0012,
      "step": 4312
    },
    {
      "epoch": 2.1532700948577133,
      "grad_norm": 0.22346797585487366,
      "learning_rate": 0.0004784672990514229,
      "loss": 0.0022,
      "step": 4313
    },
    {
      "epoch": 2.1537693459810283,
      "grad_norm": 0.12320678681135178,
      "learning_rate": 0.00047846230654018974,
      "loss": 0.0022,
      "step": 4314
    },
    {
      "epoch": 2.1542685971043434,
      "grad_norm": 0.1065756231546402,
      "learning_rate": 0.0004784573140289566,
      "loss": 0.0015,
      "step": 4315
    },
    {
      "epoch": 2.1547678482276584,
      "grad_norm": 0.5780422687530518,
      "learning_rate": 0.00047845232151772345,
      "loss": 0.0082,
      "step": 4316
    },
    {
      "epoch": 2.1552670993509735,
      "grad_norm": 0.013472560793161392,
      "learning_rate": 0.0004784473290064903,
      "loss": 0.0007,
      "step": 4317
    },
    {
      "epoch": 2.1557663504742886,
      "grad_norm": 0.40060630440711975,
      "learning_rate": 0.00047844233649525715,
      "loss": 0.0064,
      "step": 4318
    },
    {
      "epoch": 2.1562656015976036,
      "grad_norm": 0.1107880100607872,
      "learning_rate": 0.000478437343984024,
      "loss": 0.002,
      "step": 4319
    },
    {
      "epoch": 2.1567648527209187,
      "grad_norm": 2.162489891052246,
      "learning_rate": 0.00047843235147279086,
      "loss": 0.0103,
      "step": 4320
    },
    {
      "epoch": 2.1572641038442337,
      "grad_norm": 0.3861478269100189,
      "learning_rate": 0.00047842735896155765,
      "loss": 0.0057,
      "step": 4321
    },
    {
      "epoch": 2.157763354967549,
      "grad_norm": 0.9938423037528992,
      "learning_rate": 0.0004784223664503245,
      "loss": 0.0183,
      "step": 4322
    },
    {
      "epoch": 2.158262606090864,
      "grad_norm": 0.338181734085083,
      "learning_rate": 0.00047841737393909136,
      "loss": 0.0036,
      "step": 4323
    },
    {
      "epoch": 2.158761857214179,
      "grad_norm": 0.2588639557361603,
      "learning_rate": 0.0004784123814278582,
      "loss": 0.0028,
      "step": 4324
    },
    {
      "epoch": 2.159261108337494,
      "grad_norm": 0.011310534551739693,
      "learning_rate": 0.00047840738891662506,
      "loss": 0.0007,
      "step": 4325
    },
    {
      "epoch": 2.159760359460809,
      "grad_norm": 0.010910009033977985,
      "learning_rate": 0.0004784023964053919,
      "loss": 0.0007,
      "step": 4326
    },
    {
      "epoch": 2.1602596105841236,
      "grad_norm": 0.18162888288497925,
      "learning_rate": 0.00047839740389415877,
      "loss": 0.0021,
      "step": 4327
    },
    {
      "epoch": 2.1607588617074387,
      "grad_norm": 0.14627765119075775,
      "learning_rate": 0.0004783924113829256,
      "loss": 0.0022,
      "step": 4328
    },
    {
      "epoch": 2.1612581128307538,
      "grad_norm": 0.07474380731582642,
      "learning_rate": 0.00047838741887169247,
      "loss": 0.0016,
      "step": 4329
    },
    {
      "epoch": 2.161757363954069,
      "grad_norm": 0.281381219625473,
      "learning_rate": 0.0004783824263604593,
      "loss": 0.0039,
      "step": 4330
    },
    {
      "epoch": 2.162256615077384,
      "grad_norm": 0.22313562035560608,
      "learning_rate": 0.0004783774338492262,
      "loss": 0.0033,
      "step": 4331
    },
    {
      "epoch": 2.162755866200699,
      "grad_norm": 0.21564601361751556,
      "learning_rate": 0.000478372441337993,
      "loss": 0.0029,
      "step": 4332
    },
    {
      "epoch": 2.163255117324014,
      "grad_norm": 0.25843754410743713,
      "learning_rate": 0.0004783674488267599,
      "loss": 0.0242,
      "step": 4333
    },
    {
      "epoch": 2.163754368447329,
      "grad_norm": 0.021693045273423195,
      "learning_rate": 0.00047836245631552673,
      "loss": 0.0005,
      "step": 4334
    },
    {
      "epoch": 2.164253619570644,
      "grad_norm": 0.18790851533412933,
      "learning_rate": 0.0004783574638042936,
      "loss": 0.0022,
      "step": 4335
    },
    {
      "epoch": 2.164752870693959,
      "grad_norm": 0.038409989327192307,
      "learning_rate": 0.00047835247129306044,
      "loss": 0.0009,
      "step": 4336
    },
    {
      "epoch": 2.1652521218172742,
      "grad_norm": 0.036302682012319565,
      "learning_rate": 0.0004783474787818273,
      "loss": 0.0008,
      "step": 4337
    },
    {
      "epoch": 2.1657513729405893,
      "grad_norm": 0.10830170661211014,
      "learning_rate": 0.00047834248627059414,
      "loss": 0.0017,
      "step": 4338
    },
    {
      "epoch": 2.1662506240639043,
      "grad_norm": 0.022875512018799782,
      "learning_rate": 0.000478337493759361,
      "loss": 0.0007,
      "step": 4339
    },
    {
      "epoch": 2.166749875187219,
      "grad_norm": 0.12410533428192139,
      "learning_rate": 0.00047833250124812784,
      "loss": 0.0024,
      "step": 4340
    },
    {
      "epoch": 2.167249126310534,
      "grad_norm": 0.623773455619812,
      "learning_rate": 0.0004783275087368947,
      "loss": 0.022,
      "step": 4341
    },
    {
      "epoch": 2.167748377433849,
      "grad_norm": 0.029021412134170532,
      "learning_rate": 0.00047832251622566155,
      "loss": 0.0008,
      "step": 4342
    },
    {
      "epoch": 2.168247628557164,
      "grad_norm": 0.06420499086380005,
      "learning_rate": 0.0004783175237144284,
      "loss": 0.0012,
      "step": 4343
    },
    {
      "epoch": 2.168746879680479,
      "grad_norm": 0.14081932604312897,
      "learning_rate": 0.00047831253120319525,
      "loss": 0.0014,
      "step": 4344
    },
    {
      "epoch": 2.1692461308037942,
      "grad_norm": 0.0292204562574625,
      "learning_rate": 0.0004783075386919621,
      "loss": 0.0007,
      "step": 4345
    },
    {
      "epoch": 2.1697453819271093,
      "grad_norm": 0.05259101465344429,
      "learning_rate": 0.00047830254618072896,
      "loss": 0.0009,
      "step": 4346
    },
    {
      "epoch": 2.1702446330504244,
      "grad_norm": 0.016651932150125504,
      "learning_rate": 0.0004782975536694958,
      "loss": 0.0005,
      "step": 4347
    },
    {
      "epoch": 2.1707438841737394,
      "grad_norm": 0.48225709795951843,
      "learning_rate": 0.00047829256115826266,
      "loss": 0.0031,
      "step": 4348
    },
    {
      "epoch": 2.1712431352970545,
      "grad_norm": 0.0062134298495948315,
      "learning_rate": 0.0004782875686470295,
      "loss": 0.0003,
      "step": 4349
    },
    {
      "epoch": 2.1717423864203695,
      "grad_norm": 0.01625058799982071,
      "learning_rate": 0.0004782825761357963,
      "loss": 0.0006,
      "step": 4350
    },
    {
      "epoch": 2.1722416375436846,
      "grad_norm": 0.6605690717697144,
      "learning_rate": 0.00047827758362456316,
      "loss": 0.0118,
      "step": 4351
    },
    {
      "epoch": 2.1727408886669997,
      "grad_norm": 1.3358523845672607,
      "learning_rate": 0.00047827259111333,
      "loss": 0.0081,
      "step": 4352
    },
    {
      "epoch": 2.1732401397903147,
      "grad_norm": 0.0592166893184185,
      "learning_rate": 0.00047826759860209687,
      "loss": 0.0008,
      "step": 4353
    },
    {
      "epoch": 2.1737393909136298,
      "grad_norm": 0.5740062594413757,
      "learning_rate": 0.0004782626060908637,
      "loss": 0.0033,
      "step": 4354
    },
    {
      "epoch": 2.1742386420369444,
      "grad_norm": 4.268550872802734,
      "learning_rate": 0.00047825761357963057,
      "loss": 0.0087,
      "step": 4355
    },
    {
      "epoch": 2.1747378931602594,
      "grad_norm": 0.2413114756345749,
      "learning_rate": 0.0004782526210683974,
      "loss": 0.012,
      "step": 4356
    },
    {
      "epoch": 2.1752371442835745,
      "grad_norm": 0.16238483786582947,
      "learning_rate": 0.0004782476285571643,
      "loss": 0.003,
      "step": 4357
    },
    {
      "epoch": 2.1757363954068896,
      "grad_norm": 0.6456405520439148,
      "learning_rate": 0.0004782426360459311,
      "loss": 0.0107,
      "step": 4358
    },
    {
      "epoch": 2.1762356465302046,
      "grad_norm": 0.5276737809181213,
      "learning_rate": 0.000478237643534698,
      "loss": 0.0062,
      "step": 4359
    },
    {
      "epoch": 2.1767348976535197,
      "grad_norm": 0.6924970149993896,
      "learning_rate": 0.00047823265102346483,
      "loss": 0.031,
      "step": 4360
    },
    {
      "epoch": 2.1772341487768347,
      "grad_norm": 0.17867763340473175,
      "learning_rate": 0.0004782276585122317,
      "loss": 0.0035,
      "step": 4361
    },
    {
      "epoch": 2.17773339990015,
      "grad_norm": 0.7880324721336365,
      "learning_rate": 0.0004782226660009985,
      "loss": 0.0097,
      "step": 4362
    },
    {
      "epoch": 2.178232651023465,
      "grad_norm": 1.718157410621643,
      "learning_rate": 0.00047821767348976533,
      "loss": 0.0171,
      "step": 4363
    },
    {
      "epoch": 2.17873190214678,
      "grad_norm": 0.11584270000457764,
      "learning_rate": 0.0004782126809785322,
      "loss": 0.0013,
      "step": 4364
    },
    {
      "epoch": 2.179231153270095,
      "grad_norm": 0.011006333865225315,
      "learning_rate": 0.00047820768846729904,
      "loss": 0.0007,
      "step": 4365
    },
    {
      "epoch": 2.17973040439341,
      "grad_norm": 0.016767021268606186,
      "learning_rate": 0.0004782026959560659,
      "loss": 0.0007,
      "step": 4366
    },
    {
      "epoch": 2.180229655516725,
      "grad_norm": 0.4060986638069153,
      "learning_rate": 0.00047819770344483274,
      "loss": 0.0081,
      "step": 4367
    },
    {
      "epoch": 2.1807289066400397,
      "grad_norm": 0.03098992258310318,
      "learning_rate": 0.0004781927109335996,
      "loss": 0.0008,
      "step": 4368
    },
    {
      "epoch": 2.1812281577633548,
      "grad_norm": 1.0209492444992065,
      "learning_rate": 0.00047818771842236645,
      "loss": 0.0262,
      "step": 4369
    },
    {
      "epoch": 2.18172740888667,
      "grad_norm": 0.2046678215265274,
      "learning_rate": 0.0004781827259111333,
      "loss": 0.0017,
      "step": 4370
    },
    {
      "epoch": 2.182226660009985,
      "grad_norm": 1.2497872114181519,
      "learning_rate": 0.00047817773339990015,
      "loss": 0.0105,
      "step": 4371
    },
    {
      "epoch": 2.1827259111333,
      "grad_norm": 0.26212167739868164,
      "learning_rate": 0.000478172740888667,
      "loss": 0.0345,
      "step": 4372
    },
    {
      "epoch": 2.183225162256615,
      "grad_norm": 0.038246095180511475,
      "learning_rate": 0.00047816774837743385,
      "loss": 0.0011,
      "step": 4373
    },
    {
      "epoch": 2.18372441337993,
      "grad_norm": 0.29448309540748596,
      "learning_rate": 0.0004781627558662007,
      "loss": 0.0035,
      "step": 4374
    },
    {
      "epoch": 2.184223664503245,
      "grad_norm": 0.03743436187505722,
      "learning_rate": 0.00047815776335496756,
      "loss": 0.0015,
      "step": 4375
    },
    {
      "epoch": 2.18472291562656,
      "grad_norm": 0.019592756405472755,
      "learning_rate": 0.0004781527708437344,
      "loss": 0.001,
      "step": 4376
    },
    {
      "epoch": 2.1852221667498752,
      "grad_norm": 0.05204881727695465,
      "learning_rate": 0.00047814777833250126,
      "loss": 0.0015,
      "step": 4377
    },
    {
      "epoch": 2.1857214178731903,
      "grad_norm": 2.5543503761291504,
      "learning_rate": 0.0004781427858212681,
      "loss": 0.0245,
      "step": 4378
    },
    {
      "epoch": 2.1862206689965054,
      "grad_norm": 1.6446675062179565,
      "learning_rate": 0.00047813779331003497,
      "loss": 0.0459,
      "step": 4379
    },
    {
      "epoch": 2.1867199201198204,
      "grad_norm": 0.2319282740354538,
      "learning_rate": 0.00047813280079880176,
      "loss": 0.0038,
      "step": 4380
    },
    {
      "epoch": 2.1872191712431355,
      "grad_norm": 0.09655681997537613,
      "learning_rate": 0.0004781278082875686,
      "loss": 0.0019,
      "step": 4381
    },
    {
      "epoch": 2.1877184223664505,
      "grad_norm": 0.05626525357365608,
      "learning_rate": 0.00047812281577633547,
      "loss": 0.0019,
      "step": 4382
    },
    {
      "epoch": 2.188217673489765,
      "grad_norm": 0.33052581548690796,
      "learning_rate": 0.0004781178232651023,
      "loss": 0.0062,
      "step": 4383
    },
    {
      "epoch": 2.18871692461308,
      "grad_norm": 0.054601527750492096,
      "learning_rate": 0.00047811283075386917,
      "loss": 0.0018,
      "step": 4384
    },
    {
      "epoch": 2.1892161757363953,
      "grad_norm": 0.23638631403446198,
      "learning_rate": 0.000478107838242636,
      "loss": 0.0029,
      "step": 4385
    },
    {
      "epoch": 2.1897154268597103,
      "grad_norm": 0.3898029327392578,
      "learning_rate": 0.0004781028457314029,
      "loss": 0.0038,
      "step": 4386
    },
    {
      "epoch": 2.1902146779830254,
      "grad_norm": 3.9718618392944336,
      "learning_rate": 0.00047809785322016973,
      "loss": 0.0694,
      "step": 4387
    },
    {
      "epoch": 2.1907139291063404,
      "grad_norm": 0.061756860464811325,
      "learning_rate": 0.0004780928607089366,
      "loss": 0.002,
      "step": 4388
    },
    {
      "epoch": 2.1912131802296555,
      "grad_norm": 0.11894054710865021,
      "learning_rate": 0.00047808786819770343,
      "loss": 0.0018,
      "step": 4389
    },
    {
      "epoch": 2.1917124313529706,
      "grad_norm": 0.9170957207679749,
      "learning_rate": 0.0004780828756864703,
      "loss": 0.0131,
      "step": 4390
    },
    {
      "epoch": 2.1922116824762856,
      "grad_norm": 0.02086392045021057,
      "learning_rate": 0.00047807788317523714,
      "loss": 0.0011,
      "step": 4391
    },
    {
      "epoch": 2.1927109335996007,
      "grad_norm": 0.3750669062137604,
      "learning_rate": 0.000478072890664004,
      "loss": 0.0041,
      "step": 4392
    },
    {
      "epoch": 2.1932101847229157,
      "grad_norm": 0.10018301755189896,
      "learning_rate": 0.00047806789815277084,
      "loss": 0.0024,
      "step": 4393
    },
    {
      "epoch": 2.193709435846231,
      "grad_norm": 0.3799329102039337,
      "learning_rate": 0.0004780629056415377,
      "loss": 0.0046,
      "step": 4394
    },
    {
      "epoch": 2.194208686969546,
      "grad_norm": 0.0971231758594513,
      "learning_rate": 0.00047805791313030454,
      "loss": 0.0019,
      "step": 4395
    },
    {
      "epoch": 2.194707938092861,
      "grad_norm": 0.6354756951332092,
      "learning_rate": 0.0004780529206190714,
      "loss": 0.0386,
      "step": 4396
    },
    {
      "epoch": 2.195207189216176,
      "grad_norm": 0.24016977846622467,
      "learning_rate": 0.00047804792810783825,
      "loss": 0.008,
      "step": 4397
    },
    {
      "epoch": 2.1957064403394906,
      "grad_norm": 1.254878282546997,
      "learning_rate": 0.0004780429355966051,
      "loss": 0.0066,
      "step": 4398
    },
    {
      "epoch": 2.1962056914628056,
      "grad_norm": 0.6062874794006348,
      "learning_rate": 0.00047803794308537195,
      "loss": 0.021,
      "step": 4399
    },
    {
      "epoch": 2.1967049425861207,
      "grad_norm": 0.4618426561355591,
      "learning_rate": 0.0004780329505741388,
      "loss": 0.0193,
      "step": 4400
    },
    {
      "epoch": 2.1972041937094358,
      "grad_norm": 0.02349887415766716,
      "learning_rate": 0.00047802795806290566,
      "loss": 0.0008,
      "step": 4401
    },
    {
      "epoch": 2.197703444832751,
      "grad_norm": 0.7977231740951538,
      "learning_rate": 0.0004780229655516725,
      "loss": 0.0412,
      "step": 4402
    },
    {
      "epoch": 2.198202695956066,
      "grad_norm": 0.1874067783355713,
      "learning_rate": 0.00047801797304043936,
      "loss": 0.0037,
      "step": 4403
    },
    {
      "epoch": 2.198701947079381,
      "grad_norm": 0.6899314522743225,
      "learning_rate": 0.0004780129805292062,
      "loss": 0.0081,
      "step": 4404
    },
    {
      "epoch": 2.199201198202696,
      "grad_norm": 0.17176197469234467,
      "learning_rate": 0.00047800798801797307,
      "loss": 0.0039,
      "step": 4405
    },
    {
      "epoch": 2.199700449326011,
      "grad_norm": 0.2886560261249542,
      "learning_rate": 0.0004780029955067399,
      "loss": 0.0045,
      "step": 4406
    },
    {
      "epoch": 2.200199700449326,
      "grad_norm": 0.036021146923303604,
      "learning_rate": 0.00047799800299550677,
      "loss": 0.0014,
      "step": 4407
    },
    {
      "epoch": 2.200698951572641,
      "grad_norm": 0.18100601434707642,
      "learning_rate": 0.0004779930104842736,
      "loss": 0.0028,
      "step": 4408
    },
    {
      "epoch": 2.2011982026959562,
      "grad_norm": 0.20205093920230865,
      "learning_rate": 0.0004779880179730404,
      "loss": 0.0041,
      "step": 4409
    },
    {
      "epoch": 2.2016974538192713,
      "grad_norm": 0.7826411128044128,
      "learning_rate": 0.00047798302546180727,
      "loss": 0.0633,
      "step": 4410
    },
    {
      "epoch": 2.202196704942586,
      "grad_norm": 0.434578001499176,
      "learning_rate": 0.0004779780329505741,
      "loss": 0.0089,
      "step": 4411
    },
    {
      "epoch": 2.202695956065901,
      "grad_norm": 0.2064998894929886,
      "learning_rate": 0.000477973040439341,
      "loss": 0.0035,
      "step": 4412
    },
    {
      "epoch": 2.203195207189216,
      "grad_norm": 0.6036725640296936,
      "learning_rate": 0.00047796804792810783,
      "loss": 0.0088,
      "step": 4413
    },
    {
      "epoch": 2.203694458312531,
      "grad_norm": 0.473583459854126,
      "learning_rate": 0.0004779630554168747,
      "loss": 0.0138,
      "step": 4414
    },
    {
      "epoch": 2.204193709435846,
      "grad_norm": 0.7857224345207214,
      "learning_rate": 0.00047795806290564153,
      "loss": 0.0056,
      "step": 4415
    },
    {
      "epoch": 2.204692960559161,
      "grad_norm": 0.07904133945703506,
      "learning_rate": 0.0004779530703944084,
      "loss": 0.0024,
      "step": 4416
    },
    {
      "epoch": 2.2051922116824763,
      "grad_norm": 0.3785204589366913,
      "learning_rate": 0.00047794807788317524,
      "loss": 0.0059,
      "step": 4417
    },
    {
      "epoch": 2.2056914628057913,
      "grad_norm": 0.30388233065605164,
      "learning_rate": 0.0004779430853719421,
      "loss": 0.0063,
      "step": 4418
    },
    {
      "epoch": 2.2061907139291064,
      "grad_norm": 0.06875245273113251,
      "learning_rate": 0.00047793809286070894,
      "loss": 0.0022,
      "step": 4419
    },
    {
      "epoch": 2.2066899650524214,
      "grad_norm": 0.2958652079105377,
      "learning_rate": 0.0004779331003494758,
      "loss": 0.0068,
      "step": 4420
    },
    {
      "epoch": 2.2071892161757365,
      "grad_norm": 0.2707538604736328,
      "learning_rate": 0.00047792810783824264,
      "loss": 0.003,
      "step": 4421
    },
    {
      "epoch": 2.2076884672990515,
      "grad_norm": 0.08338098973035812,
      "learning_rate": 0.0004779231153270095,
      "loss": 0.0024,
      "step": 4422
    },
    {
      "epoch": 2.2081877184223666,
      "grad_norm": 0.43626487255096436,
      "learning_rate": 0.00047791812281577635,
      "loss": 0.005,
      "step": 4423
    },
    {
      "epoch": 2.2086869695456817,
      "grad_norm": 0.1846284717321396,
      "learning_rate": 0.0004779131303045432,
      "loss": 0.0037,
      "step": 4424
    },
    {
      "epoch": 2.2091862206689967,
      "grad_norm": 0.2157427966594696,
      "learning_rate": 0.00047790813779331005,
      "loss": 0.0025,
      "step": 4425
    },
    {
      "epoch": 2.2096854717923113,
      "grad_norm": 0.2249668538570404,
      "learning_rate": 0.0004779031452820769,
      "loss": 0.0041,
      "step": 4426
    },
    {
      "epoch": 2.2101847229156264,
      "grad_norm": 0.14354437589645386,
      "learning_rate": 0.00047789815277084376,
      "loss": 0.0023,
      "step": 4427
    },
    {
      "epoch": 2.2106839740389415,
      "grad_norm": 0.40131664276123047,
      "learning_rate": 0.0004778931602596106,
      "loss": 0.005,
      "step": 4428
    },
    {
      "epoch": 2.2111832251622565,
      "grad_norm": 0.032937027513980865,
      "learning_rate": 0.00047788816774837746,
      "loss": 0.0009,
      "step": 4429
    },
    {
      "epoch": 2.2116824762855716,
      "grad_norm": 0.49114418029785156,
      "learning_rate": 0.0004778831752371443,
      "loss": 0.0063,
      "step": 4430
    },
    {
      "epoch": 2.2121817274088866,
      "grad_norm": 0.06895379722118378,
      "learning_rate": 0.00047787818272591116,
      "loss": 0.0017,
      "step": 4431
    },
    {
      "epoch": 2.2126809785322017,
      "grad_norm": 0.017715994268655777,
      "learning_rate": 0.000477873190214678,
      "loss": 0.0007,
      "step": 4432
    },
    {
      "epoch": 2.2131802296555168,
      "grad_norm": 0.04571763426065445,
      "learning_rate": 0.00047786819770344487,
      "loss": 0.0014,
      "step": 4433
    },
    {
      "epoch": 2.213679480778832,
      "grad_norm": 0.7107499241828918,
      "learning_rate": 0.0004778632051922117,
      "loss": 0.0051,
      "step": 4434
    },
    {
      "epoch": 2.214178731902147,
      "grad_norm": 0.11833549290895462,
      "learning_rate": 0.0004778582126809786,
      "loss": 0.002,
      "step": 4435
    },
    {
      "epoch": 2.214677983025462,
      "grad_norm": 0.02123541571199894,
      "learning_rate": 0.0004778532201697454,
      "loss": 0.0008,
      "step": 4436
    },
    {
      "epoch": 2.215177234148777,
      "grad_norm": 0.01960308477282524,
      "learning_rate": 0.0004778482276585123,
      "loss": 0.0009,
      "step": 4437
    },
    {
      "epoch": 2.215676485272092,
      "grad_norm": 0.06115199625492096,
      "learning_rate": 0.0004778432351472791,
      "loss": 0.0012,
      "step": 4438
    },
    {
      "epoch": 2.2161757363954067,
      "grad_norm": 0.03448658436536789,
      "learning_rate": 0.00047783824263604593,
      "loss": 0.0009,
      "step": 4439
    },
    {
      "epoch": 2.2166749875187217,
      "grad_norm": 0.17720359563827515,
      "learning_rate": 0.0004778332501248128,
      "loss": 0.003,
      "step": 4440
    },
    {
      "epoch": 2.2171742386420368,
      "grad_norm": 0.25240322947502136,
      "learning_rate": 0.00047782825761357963,
      "loss": 0.0043,
      "step": 4441
    },
    {
      "epoch": 2.217673489765352,
      "grad_norm": 0.3181026577949524,
      "learning_rate": 0.0004778232651023465,
      "loss": 0.0057,
      "step": 4442
    },
    {
      "epoch": 2.218172740888667,
      "grad_norm": 0.4800296723842621,
      "learning_rate": 0.00047781827259111334,
      "loss": 0.0011,
      "step": 4443
    },
    {
      "epoch": 2.218671992011982,
      "grad_norm": 0.5182430744171143,
      "learning_rate": 0.0004778132800798802,
      "loss": 0.0102,
      "step": 4444
    },
    {
      "epoch": 2.219171243135297,
      "grad_norm": 0.011078896000981331,
      "learning_rate": 0.00047780828756864704,
      "loss": 0.0006,
      "step": 4445
    },
    {
      "epoch": 2.219670494258612,
      "grad_norm": 0.11750512570142746,
      "learning_rate": 0.0004778032950574139,
      "loss": 0.0013,
      "step": 4446
    },
    {
      "epoch": 2.220169745381927,
      "grad_norm": 0.08057703077793121,
      "learning_rate": 0.00047779830254618074,
      "loss": 0.0017,
      "step": 4447
    },
    {
      "epoch": 2.220668996505242,
      "grad_norm": 0.25182318687438965,
      "learning_rate": 0.0004777933100349476,
      "loss": 0.003,
      "step": 4448
    },
    {
      "epoch": 2.2211682476285572,
      "grad_norm": 0.006998990662395954,
      "learning_rate": 0.00047778831752371445,
      "loss": 0.0005,
      "step": 4449
    },
    {
      "epoch": 2.2216674987518723,
      "grad_norm": 0.03400591388344765,
      "learning_rate": 0.0004777833250124813,
      "loss": 0.001,
      "step": 4450
    },
    {
      "epoch": 2.2221667498751874,
      "grad_norm": 0.2864464223384857,
      "learning_rate": 0.00047777833250124815,
      "loss": 0.0029,
      "step": 4451
    },
    {
      "epoch": 2.2226660009985024,
      "grad_norm": 0.4494730532169342,
      "learning_rate": 0.000477773339990015,
      "loss": 0.012,
      "step": 4452
    },
    {
      "epoch": 2.2231652521218175,
      "grad_norm": 0.0610848069190979,
      "learning_rate": 0.00047776834747878186,
      "loss": 0.0008,
      "step": 4453
    },
    {
      "epoch": 2.223664503245132,
      "grad_norm": 0.009393168613314629,
      "learning_rate": 0.0004777633549675487,
      "loss": 0.0005,
      "step": 4454
    },
    {
      "epoch": 2.224163754368447,
      "grad_norm": 0.18801723420619965,
      "learning_rate": 0.00047775836245631556,
      "loss": 0.0006,
      "step": 4455
    },
    {
      "epoch": 2.224663005491762,
      "grad_norm": 1.8125563859939575,
      "learning_rate": 0.0004777533699450824,
      "loss": 0.0133,
      "step": 4456
    },
    {
      "epoch": 2.2251622566150773,
      "grad_norm": 0.26955559849739075,
      "learning_rate": 0.00047774837743384926,
      "loss": 0.0024,
      "step": 4457
    },
    {
      "epoch": 2.2256615077383923,
      "grad_norm": 0.5679190158843994,
      "learning_rate": 0.0004777433849226161,
      "loss": 0.0058,
      "step": 4458
    },
    {
      "epoch": 2.2261607588617074,
      "grad_norm": 0.7851067781448364,
      "learning_rate": 0.00047773839241138297,
      "loss": 0.0135,
      "step": 4459
    },
    {
      "epoch": 2.2266600099850224,
      "grad_norm": 0.09233426302671432,
      "learning_rate": 0.0004777333999001498,
      "loss": 0.0012,
      "step": 4460
    },
    {
      "epoch": 2.2271592611083375,
      "grad_norm": 0.006876061204820871,
      "learning_rate": 0.00047772840738891667,
      "loss": 0.0005,
      "step": 4461
    },
    {
      "epoch": 2.2276585122316526,
      "grad_norm": 0.18590082228183746,
      "learning_rate": 0.0004777234148776835,
      "loss": 0.0041,
      "step": 4462
    },
    {
      "epoch": 2.2281577633549676,
      "grad_norm": 0.5125687122344971,
      "learning_rate": 0.0004777184223664504,
      "loss": 0.0199,
      "step": 4463
    },
    {
      "epoch": 2.2286570144782827,
      "grad_norm": 0.007511173840612173,
      "learning_rate": 0.00047771342985521723,
      "loss": 0.0005,
      "step": 4464
    },
    {
      "epoch": 2.2291562656015977,
      "grad_norm": 0.4374066889286041,
      "learning_rate": 0.0004777084373439841,
      "loss": 0.0167,
      "step": 4465
    },
    {
      "epoch": 2.229655516724913,
      "grad_norm": 0.01190941408276558,
      "learning_rate": 0.00047770344483275093,
      "loss": 0.0007,
      "step": 4466
    },
    {
      "epoch": 2.230154767848228,
      "grad_norm": 0.24056273698806763,
      "learning_rate": 0.00047769845232151773,
      "loss": 0.0023,
      "step": 4467
    },
    {
      "epoch": 2.230654018971543,
      "grad_norm": 0.056789953261613846,
      "learning_rate": 0.0004776934598102846,
      "loss": 0.001,
      "step": 4468
    },
    {
      "epoch": 2.2311532700948575,
      "grad_norm": 0.1568153351545334,
      "learning_rate": 0.00047768846729905143,
      "loss": 0.0026,
      "step": 4469
    },
    {
      "epoch": 2.2316525212181726,
      "grad_norm": 0.33322790265083313,
      "learning_rate": 0.0004776834747878183,
      "loss": 0.0044,
      "step": 4470
    },
    {
      "epoch": 2.2321517723414876,
      "grad_norm": 0.6350115537643433,
      "learning_rate": 0.00047767848227658514,
      "loss": 0.0067,
      "step": 4471
    },
    {
      "epoch": 2.2326510234648027,
      "grad_norm": 0.11287099868059158,
      "learning_rate": 0.000477673489765352,
      "loss": 0.0014,
      "step": 4472
    },
    {
      "epoch": 2.2331502745881178,
      "grad_norm": 0.023790545761585236,
      "learning_rate": 0.00047766849725411884,
      "loss": 0.001,
      "step": 4473
    },
    {
      "epoch": 2.233649525711433,
      "grad_norm": 0.09502826631069183,
      "learning_rate": 0.0004776635047428857,
      "loss": 0.0019,
      "step": 4474
    },
    {
      "epoch": 2.234148776834748,
      "grad_norm": 0.3622949719429016,
      "learning_rate": 0.0004776585122316525,
      "loss": 0.0042,
      "step": 4475
    },
    {
      "epoch": 2.234648027958063,
      "grad_norm": 0.541401743888855,
      "learning_rate": 0.00047765351972041935,
      "loss": 0.0073,
      "step": 4476
    },
    {
      "epoch": 2.235147279081378,
      "grad_norm": 0.1826191395521164,
      "learning_rate": 0.0004776485272091862,
      "loss": 0.0029,
      "step": 4477
    },
    {
      "epoch": 2.235646530204693,
      "grad_norm": 0.7989879250526428,
      "learning_rate": 0.00047764353469795305,
      "loss": 0.0126,
      "step": 4478
    },
    {
      "epoch": 2.236145781328008,
      "grad_norm": 0.31491819024086,
      "learning_rate": 0.0004776385421867199,
      "loss": 0.0027,
      "step": 4479
    },
    {
      "epoch": 2.236645032451323,
      "grad_norm": 0.09468790143728256,
      "learning_rate": 0.00047763354967548675,
      "loss": 0.0018,
      "step": 4480
    },
    {
      "epoch": 2.2371442835746382,
      "grad_norm": 0.6189523935317993,
      "learning_rate": 0.0004776285571642536,
      "loss": 0.0262,
      "step": 4481
    },
    {
      "epoch": 2.237643534697953,
      "grad_norm": 0.27698901295661926,
      "learning_rate": 0.00047762356465302046,
      "loss": 0.0074,
      "step": 4482
    },
    {
      "epoch": 2.238142785821268,
      "grad_norm": 0.652092456817627,
      "learning_rate": 0.0004776185721417873,
      "loss": 0.0061,
      "step": 4483
    },
    {
      "epoch": 2.238642036944583,
      "grad_norm": 0.14442481100559235,
      "learning_rate": 0.00047761357963055416,
      "loss": 0.0017,
      "step": 4484
    },
    {
      "epoch": 2.239141288067898,
      "grad_norm": 0.300597220659256,
      "learning_rate": 0.000477608587119321,
      "loss": 0.0044,
      "step": 4485
    },
    {
      "epoch": 2.239640539191213,
      "grad_norm": 0.14132855832576752,
      "learning_rate": 0.00047760359460808787,
      "loss": 0.0028,
      "step": 4486
    },
    {
      "epoch": 2.240139790314528,
      "grad_norm": 0.8344214558601379,
      "learning_rate": 0.0004775986020968547,
      "loss": 0.0063,
      "step": 4487
    },
    {
      "epoch": 2.240639041437843,
      "grad_norm": 0.007660923525691032,
      "learning_rate": 0.00047759360958562157,
      "loss": 0.0006,
      "step": 4488
    },
    {
      "epoch": 2.2411382925611583,
      "grad_norm": 0.08532805740833282,
      "learning_rate": 0.0004775886170743884,
      "loss": 0.0012,
      "step": 4489
    },
    {
      "epoch": 2.2416375436844733,
      "grad_norm": 0.02226915955543518,
      "learning_rate": 0.0004775836245631553,
      "loss": 0.0008,
      "step": 4490
    },
    {
      "epoch": 2.2421367948077884,
      "grad_norm": 0.406475305557251,
      "learning_rate": 0.0004775786320519221,
      "loss": 0.0082,
      "step": 4491
    },
    {
      "epoch": 2.2426360459311034,
      "grad_norm": 0.2081853300333023,
      "learning_rate": 0.000477573639540689,
      "loss": 0.0034,
      "step": 4492
    },
    {
      "epoch": 2.2431352970544185,
      "grad_norm": 0.18320950865745544,
      "learning_rate": 0.00047756864702945583,
      "loss": 0.0027,
      "step": 4493
    },
    {
      "epoch": 2.2436345481777336,
      "grad_norm": 0.16841693222522736,
      "learning_rate": 0.0004775636545182227,
      "loss": 0.0021,
      "step": 4494
    },
    {
      "epoch": 2.2441337993010486,
      "grad_norm": 0.14026609063148499,
      "learning_rate": 0.00047755866200698953,
      "loss": 0.0021,
      "step": 4495
    },
    {
      "epoch": 2.2446330504243637,
      "grad_norm": 0.5218458771705627,
      "learning_rate": 0.00047755366949575633,
      "loss": 0.0076,
      "step": 4496
    },
    {
      "epoch": 2.2451323015476783,
      "grad_norm": 0.09334062784910202,
      "learning_rate": 0.0004775486769845232,
      "loss": 0.0015,
      "step": 4497
    },
    {
      "epoch": 2.2456315526709933,
      "grad_norm": 0.14750681817531586,
      "learning_rate": 0.00047754368447329004,
      "loss": 0.0019,
      "step": 4498
    },
    {
      "epoch": 2.2461308037943084,
      "grad_norm": 0.6544865369796753,
      "learning_rate": 0.0004775386919620569,
      "loss": 0.0084,
      "step": 4499
    },
    {
      "epoch": 2.2466300549176235,
      "grad_norm": 1.2000559568405151,
      "learning_rate": 0.00047753369945082374,
      "loss": 0.0058,
      "step": 4500
    },
    {
      "epoch": 2.2471293060409385,
      "grad_norm": 0.3229200839996338,
      "learning_rate": 0.0004775287069395906,
      "loss": 0.0106,
      "step": 4501
    },
    {
      "epoch": 2.2476285571642536,
      "grad_norm": 0.08684242516756058,
      "learning_rate": 0.00047752371442835744,
      "loss": 0.0017,
      "step": 4502
    },
    {
      "epoch": 2.2481278082875686,
      "grad_norm": 0.3576187491416931,
      "learning_rate": 0.0004775187219171243,
      "loss": 0.0184,
      "step": 4503
    },
    {
      "epoch": 2.2486270594108837,
      "grad_norm": 0.05386527255177498,
      "learning_rate": 0.00047751372940589115,
      "loss": 0.0011,
      "step": 4504
    },
    {
      "epoch": 2.2491263105341988,
      "grad_norm": 0.03138796612620354,
      "learning_rate": 0.000477508736894658,
      "loss": 0.0008,
      "step": 4505
    },
    {
      "epoch": 2.249625561657514,
      "grad_norm": 0.2111666053533554,
      "learning_rate": 0.00047750374438342485,
      "loss": 0.0038,
      "step": 4506
    },
    {
      "epoch": 2.250124812780829,
      "grad_norm": 1.2864985466003418,
      "learning_rate": 0.0004774987518721917,
      "loss": 0.0253,
      "step": 4507
    },
    {
      "epoch": 2.250624063904144,
      "grad_norm": 0.04846775159239769,
      "learning_rate": 0.00047749375936095856,
      "loss": 0.001,
      "step": 4508
    },
    {
      "epoch": 2.251123315027459,
      "grad_norm": 0.03731231391429901,
      "learning_rate": 0.0004774887668497254,
      "loss": 0.001,
      "step": 4509
    },
    {
      "epoch": 2.2516225661507736,
      "grad_norm": 0.14721235632896423,
      "learning_rate": 0.00047748377433849226,
      "loss": 0.003,
      "step": 4510
    },
    {
      "epoch": 2.252121817274089,
      "grad_norm": 0.39570990204811096,
      "learning_rate": 0.0004774787818272591,
      "loss": 0.0062,
      "step": 4511
    },
    {
      "epoch": 2.2526210683974037,
      "grad_norm": 0.47113314270973206,
      "learning_rate": 0.00047747378931602597,
      "loss": 0.0057,
      "step": 4512
    },
    {
      "epoch": 2.253120319520719,
      "grad_norm": 0.46920478343963623,
      "learning_rate": 0.0004774687968047928,
      "loss": 0.0056,
      "step": 4513
    },
    {
      "epoch": 2.253619570644034,
      "grad_norm": 0.39452603459358215,
      "learning_rate": 0.00047746380429355967,
      "loss": 0.0068,
      "step": 4514
    },
    {
      "epoch": 2.254118821767349,
      "grad_norm": 0.026869775727391243,
      "learning_rate": 0.0004774588117823265,
      "loss": 0.0011,
      "step": 4515
    },
    {
      "epoch": 2.254618072890664,
      "grad_norm": 0.38415175676345825,
      "learning_rate": 0.0004774538192710934,
      "loss": 0.0043,
      "step": 4516
    },
    {
      "epoch": 2.255117324013979,
      "grad_norm": 0.02582770213484764,
      "learning_rate": 0.0004774488267598602,
      "loss": 0.0012,
      "step": 4517
    },
    {
      "epoch": 2.255616575137294,
      "grad_norm": 0.04027814045548439,
      "learning_rate": 0.0004774438342486271,
      "loss": 0.001,
      "step": 4518
    },
    {
      "epoch": 2.256115826260609,
      "grad_norm": 0.15648391842842102,
      "learning_rate": 0.00047743884173739393,
      "loss": 0.0019,
      "step": 4519
    },
    {
      "epoch": 2.256615077383924,
      "grad_norm": 0.014292177744209766,
      "learning_rate": 0.0004774338492261608,
      "loss": 0.0008,
      "step": 4520
    },
    {
      "epoch": 2.2571143285072393,
      "grad_norm": 0.25690433382987976,
      "learning_rate": 0.00047742885671492763,
      "loss": 0.0031,
      "step": 4521
    },
    {
      "epoch": 2.2576135796305543,
      "grad_norm": 0.03389791399240494,
      "learning_rate": 0.0004774238642036945,
      "loss": 0.001,
      "step": 4522
    },
    {
      "epoch": 2.2581128307538694,
      "grad_norm": 0.19401836395263672,
      "learning_rate": 0.00047741887169246134,
      "loss": 0.0016,
      "step": 4523
    },
    {
      "epoch": 2.2586120818771844,
      "grad_norm": 0.8455109000205994,
      "learning_rate": 0.0004774138791812282,
      "loss": 0.0116,
      "step": 4524
    },
    {
      "epoch": 2.259111333000499,
      "grad_norm": 0.4652881622314453,
      "learning_rate": 0.000477408886669995,
      "loss": 0.0087,
      "step": 4525
    },
    {
      "epoch": 2.259610584123814,
      "grad_norm": 0.04199404641985893,
      "learning_rate": 0.00047740389415876184,
      "loss": 0.0011,
      "step": 4526
    },
    {
      "epoch": 2.260109835247129,
      "grad_norm": 0.020307106897234917,
      "learning_rate": 0.0004773989016475287,
      "loss": 0.0007,
      "step": 4527
    },
    {
      "epoch": 2.260609086370444,
      "grad_norm": 0.022373998537659645,
      "learning_rate": 0.00047739390913629554,
      "loss": 0.0008,
      "step": 4528
    },
    {
      "epoch": 2.2611083374937593,
      "grad_norm": 0.8720560669898987,
      "learning_rate": 0.0004773889166250624,
      "loss": 0.0369,
      "step": 4529
    },
    {
      "epoch": 2.2616075886170743,
      "grad_norm": 0.013965165242552757,
      "learning_rate": 0.00047738392411382925,
      "loss": 0.0006,
      "step": 4530
    },
    {
      "epoch": 2.2621068397403894,
      "grad_norm": 0.17706583440303802,
      "learning_rate": 0.0004773789316025961,
      "loss": 0.002,
      "step": 4531
    },
    {
      "epoch": 2.2626060908637045,
      "grad_norm": 0.6057366132736206,
      "learning_rate": 0.00047737393909136295,
      "loss": 0.0054,
      "step": 4532
    },
    {
      "epoch": 2.2631053419870195,
      "grad_norm": 0.8650466799736023,
      "learning_rate": 0.0004773689465801298,
      "loss": 0.0126,
      "step": 4533
    },
    {
      "epoch": 2.2636045931103346,
      "grad_norm": 0.22553642094135284,
      "learning_rate": 0.00047736395406889666,
      "loss": 0.004,
      "step": 4534
    },
    {
      "epoch": 2.2641038442336496,
      "grad_norm": 0.02332848124206066,
      "learning_rate": 0.0004773589615576635,
      "loss": 0.0008,
      "step": 4535
    },
    {
      "epoch": 2.2646030953569647,
      "grad_norm": 0.02306956611573696,
      "learning_rate": 0.00047735396904643036,
      "loss": 0.001,
      "step": 4536
    },
    {
      "epoch": 2.2651023464802797,
      "grad_norm": 0.08660321682691574,
      "learning_rate": 0.0004773489765351972,
      "loss": 0.0019,
      "step": 4537
    },
    {
      "epoch": 2.2656015976035944,
      "grad_norm": 0.3481021523475647,
      "learning_rate": 0.00047734398402396406,
      "loss": 0.0115,
      "step": 4538
    },
    {
      "epoch": 2.26610084872691,
      "grad_norm": 0.028189027681946754,
      "learning_rate": 0.0004773389915127309,
      "loss": 0.0011,
      "step": 4539
    },
    {
      "epoch": 2.2666000998502245,
      "grad_norm": 0.3641867935657501,
      "learning_rate": 0.00047733399900149777,
      "loss": 0.0157,
      "step": 4540
    },
    {
      "epoch": 2.2670993509735395,
      "grad_norm": 0.2737976014614105,
      "learning_rate": 0.0004773290064902646,
      "loss": 0.004,
      "step": 4541
    },
    {
      "epoch": 2.2675986020968546,
      "grad_norm": 0.5632948875427246,
      "learning_rate": 0.0004773240139790315,
      "loss": 0.004,
      "step": 4542
    },
    {
      "epoch": 2.2680978532201697,
      "grad_norm": 0.6029441356658936,
      "learning_rate": 0.0004773190214677983,
      "loss": 0.0161,
      "step": 4543
    },
    {
      "epoch": 2.2685971043434847,
      "grad_norm": 0.15292510390281677,
      "learning_rate": 0.0004773140289565652,
      "loss": 0.0027,
      "step": 4544
    },
    {
      "epoch": 2.2690963554667998,
      "grad_norm": 1.0833663940429688,
      "learning_rate": 0.00047730903644533203,
      "loss": 0.0115,
      "step": 4545
    },
    {
      "epoch": 2.269595606590115,
      "grad_norm": 0.34826546907424927,
      "learning_rate": 0.0004773040439340989,
      "loss": 0.0088,
      "step": 4546
    },
    {
      "epoch": 2.27009485771343,
      "grad_norm": 0.5540973544120789,
      "learning_rate": 0.00047729905142286573,
      "loss": 0.006,
      "step": 4547
    },
    {
      "epoch": 2.270594108836745,
      "grad_norm": 0.18005309998989105,
      "learning_rate": 0.0004772940589116326,
      "loss": 0.0023,
      "step": 4548
    },
    {
      "epoch": 2.27109335996006,
      "grad_norm": 0.024223104119300842,
      "learning_rate": 0.00047728906640039944,
      "loss": 0.0008,
      "step": 4549
    },
    {
      "epoch": 2.271592611083375,
      "grad_norm": 0.2475801557302475,
      "learning_rate": 0.0004772840738891663,
      "loss": 0.0027,
      "step": 4550
    },
    {
      "epoch": 2.27209186220669,
      "grad_norm": 0.08040980249643326,
      "learning_rate": 0.00047727908137793314,
      "loss": 0.0014,
      "step": 4551
    },
    {
      "epoch": 2.272591113330005,
      "grad_norm": 0.3274344205856323,
      "learning_rate": 0.0004772740888667,
      "loss": 0.0068,
      "step": 4552
    },
    {
      "epoch": 2.27309036445332,
      "grad_norm": 0.831894040107727,
      "learning_rate": 0.00047726909635546685,
      "loss": 0.0066,
      "step": 4553
    },
    {
      "epoch": 2.273589615576635,
      "grad_norm": 0.07685882598161697,
      "learning_rate": 0.00047726410384423364,
      "loss": 0.001,
      "step": 4554
    },
    {
      "epoch": 2.27408886669995,
      "grad_norm": 0.013546610251069069,
      "learning_rate": 0.0004772591113330005,
      "loss": 0.0008,
      "step": 4555
    },
    {
      "epoch": 2.274588117823265,
      "grad_norm": 0.07390786707401276,
      "learning_rate": 0.00047725411882176735,
      "loss": 0.0012,
      "step": 4556
    },
    {
      "epoch": 2.27508736894658,
      "grad_norm": 0.4698414206504822,
      "learning_rate": 0.0004772491263105342,
      "loss": 0.0139,
      "step": 4557
    },
    {
      "epoch": 2.275586620069895,
      "grad_norm": 0.07231176644563675,
      "learning_rate": 0.00047724413379930105,
      "loss": 0.0012,
      "step": 4558
    },
    {
      "epoch": 2.27608587119321,
      "grad_norm": 0.05269012972712517,
      "learning_rate": 0.0004772391412880679,
      "loss": 0.0013,
      "step": 4559
    },
    {
      "epoch": 2.276585122316525,
      "grad_norm": 0.043006811290979385,
      "learning_rate": 0.00047723414877683476,
      "loss": 0.0012,
      "step": 4560
    },
    {
      "epoch": 2.2770843734398403,
      "grad_norm": 0.37268781661987305,
      "learning_rate": 0.0004772291562656016,
      "loss": 0.008,
      "step": 4561
    },
    {
      "epoch": 2.2775836245631553,
      "grad_norm": 0.15449681878089905,
      "learning_rate": 0.00047722416375436846,
      "loss": 0.0026,
      "step": 4562
    },
    {
      "epoch": 2.2780828756864704,
      "grad_norm": 0.9824182987213135,
      "learning_rate": 0.0004772191712431353,
      "loss": 0.0156,
      "step": 4563
    },
    {
      "epoch": 2.2785821268097854,
      "grad_norm": 0.3830913007259369,
      "learning_rate": 0.00047721417873190216,
      "loss": 0.0103,
      "step": 4564
    },
    {
      "epoch": 2.2790813779331005,
      "grad_norm": 0.558514416217804,
      "learning_rate": 0.000477209186220669,
      "loss": 0.0085,
      "step": 4565
    },
    {
      "epoch": 2.2795806290564156,
      "grad_norm": 0.8178282976150513,
      "learning_rate": 0.00047720419370943587,
      "loss": 0.0047,
      "step": 4566
    },
    {
      "epoch": 2.2800798801797306,
      "grad_norm": 0.41949301958084106,
      "learning_rate": 0.0004771992011982027,
      "loss": 0.0052,
      "step": 4567
    },
    {
      "epoch": 2.2805791313030452,
      "grad_norm": 0.026433173567056656,
      "learning_rate": 0.00047719420868696957,
      "loss": 0.0008,
      "step": 4568
    },
    {
      "epoch": 2.2810783824263603,
      "grad_norm": 1.5890816450119019,
      "learning_rate": 0.0004771892161757364,
      "loss": 0.0051,
      "step": 4569
    },
    {
      "epoch": 2.2815776335496754,
      "grad_norm": 0.10172712802886963,
      "learning_rate": 0.0004771842236645033,
      "loss": 0.0023,
      "step": 4570
    },
    {
      "epoch": 2.2820768846729904,
      "grad_norm": 0.10439596325159073,
      "learning_rate": 0.00047717923115327013,
      "loss": 0.002,
      "step": 4571
    },
    {
      "epoch": 2.2825761357963055,
      "grad_norm": 0.359578400850296,
      "learning_rate": 0.000477174238642037,
      "loss": 0.0053,
      "step": 4572
    },
    {
      "epoch": 2.2830753869196205,
      "grad_norm": 0.259048193693161,
      "learning_rate": 0.00047716924613080383,
      "loss": 0.0047,
      "step": 4573
    },
    {
      "epoch": 2.2835746380429356,
      "grad_norm": 0.05174429714679718,
      "learning_rate": 0.0004771642536195707,
      "loss": 0.0015,
      "step": 4574
    },
    {
      "epoch": 2.2840738891662506,
      "grad_norm": 0.28756895661354065,
      "learning_rate": 0.00047715926110833754,
      "loss": 0.0038,
      "step": 4575
    },
    {
      "epoch": 2.2845731402895657,
      "grad_norm": 0.5562307834625244,
      "learning_rate": 0.0004771542685971044,
      "loss": 0.0153,
      "step": 4576
    },
    {
      "epoch": 2.2850723914128808,
      "grad_norm": 1.4528309106826782,
      "learning_rate": 0.00047714927608587124,
      "loss": 0.0288,
      "step": 4577
    },
    {
      "epoch": 2.285571642536196,
      "grad_norm": 0.6650498509407043,
      "learning_rate": 0.0004771442835746381,
      "loss": 0.0229,
      "step": 4578
    },
    {
      "epoch": 2.286070893659511,
      "grad_norm": 0.06680573523044586,
      "learning_rate": 0.00047713929106340495,
      "loss": 0.0012,
      "step": 4579
    },
    {
      "epoch": 2.286570144782826,
      "grad_norm": 0.1552339494228363,
      "learning_rate": 0.0004771342985521718,
      "loss": 0.0129,
      "step": 4580
    },
    {
      "epoch": 2.2870693959061406,
      "grad_norm": 1.0738469362258911,
      "learning_rate": 0.00047712930604093865,
      "loss": 0.0345,
      "step": 4581
    },
    {
      "epoch": 2.287568647029456,
      "grad_norm": 1.4760836362838745,
      "learning_rate": 0.0004771243135297055,
      "loss": 0.0501,
      "step": 4582
    },
    {
      "epoch": 2.2880678981527707,
      "grad_norm": 0.9047952890396118,
      "learning_rate": 0.0004771193210184723,
      "loss": 0.0312,
      "step": 4583
    },
    {
      "epoch": 2.2885671492760857,
      "grad_norm": 0.10258632898330688,
      "learning_rate": 0.00047711432850723915,
      "loss": 0.0029,
      "step": 4584
    },
    {
      "epoch": 2.289066400399401,
      "grad_norm": 0.09870276600122452,
      "learning_rate": 0.000477109335996006,
      "loss": 0.0026,
      "step": 4585
    },
    {
      "epoch": 2.289565651522716,
      "grad_norm": 0.5894501805305481,
      "learning_rate": 0.00047710434348477286,
      "loss": 0.0137,
      "step": 4586
    },
    {
      "epoch": 2.290064902646031,
      "grad_norm": 0.2935585081577301,
      "learning_rate": 0.0004770993509735397,
      "loss": 0.005,
      "step": 4587
    },
    {
      "epoch": 2.290564153769346,
      "grad_norm": 0.45975062251091003,
      "learning_rate": 0.0004770943584623065,
      "loss": 0.0213,
      "step": 4588
    },
    {
      "epoch": 2.291063404892661,
      "grad_norm": 1.8804285526275635,
      "learning_rate": 0.00047708936595107336,
      "loss": 0.0117,
      "step": 4589
    },
    {
      "epoch": 2.291562656015976,
      "grad_norm": 0.329977422952652,
      "learning_rate": 0.0004770843734398402,
      "loss": 0.0069,
      "step": 4590
    },
    {
      "epoch": 2.292061907139291,
      "grad_norm": 0.1826951950788498,
      "learning_rate": 0.00047707938092860706,
      "loss": 0.0035,
      "step": 4591
    },
    {
      "epoch": 2.292561158262606,
      "grad_norm": 0.6554567813873291,
      "learning_rate": 0.0004770743884173739,
      "loss": 0.0133,
      "step": 4592
    },
    {
      "epoch": 2.2930604093859213,
      "grad_norm": 0.39364945888519287,
      "learning_rate": 0.00047706939590614077,
      "loss": 0.0088,
      "step": 4593
    },
    {
      "epoch": 2.2935596605092363,
      "grad_norm": 0.5636951327323914,
      "learning_rate": 0.0004770644033949076,
      "loss": 0.0073,
      "step": 4594
    },
    {
      "epoch": 2.2940589116325514,
      "grad_norm": 0.45017507672309875,
      "learning_rate": 0.00047705941088367447,
      "loss": 0.0118,
      "step": 4595
    },
    {
      "epoch": 2.294558162755866,
      "grad_norm": 0.16646069288253784,
      "learning_rate": 0.0004770544183724413,
      "loss": 0.003,
      "step": 4596
    },
    {
      "epoch": 2.295057413879181,
      "grad_norm": 0.17663057148456573,
      "learning_rate": 0.0004770494258612082,
      "loss": 0.0033,
      "step": 4597
    },
    {
      "epoch": 2.295556665002496,
      "grad_norm": 1.0390849113464355,
      "learning_rate": 0.000477044433349975,
      "loss": 0.0133,
      "step": 4598
    },
    {
      "epoch": 2.296055916125811,
      "grad_norm": 0.06029614806175232,
      "learning_rate": 0.0004770394408387419,
      "loss": 0.0018,
      "step": 4599
    },
    {
      "epoch": 2.2965551672491262,
      "grad_norm": 0.0896603912115097,
      "learning_rate": 0.00047703444832750873,
      "loss": 0.0025,
      "step": 4600
    },
    {
      "epoch": 2.2970544183724413,
      "grad_norm": 0.058320194482803345,
      "learning_rate": 0.0004770294558162756,
      "loss": 0.002,
      "step": 4601
    },
    {
      "epoch": 2.2975536694957563,
      "grad_norm": 0.25227388739585876,
      "learning_rate": 0.00047702446330504243,
      "loss": 0.0029,
      "step": 4602
    },
    {
      "epoch": 2.2980529206190714,
      "grad_norm": 0.4856920838356018,
      "learning_rate": 0.0004770194707938093,
      "loss": 0.0186,
      "step": 4603
    },
    {
      "epoch": 2.2985521717423865,
      "grad_norm": 0.6752426028251648,
      "learning_rate": 0.00047701447828257614,
      "loss": 0.0155,
      "step": 4604
    },
    {
      "epoch": 2.2990514228657015,
      "grad_norm": 0.29093945026397705,
      "learning_rate": 0.000477009485771343,
      "loss": 0.0058,
      "step": 4605
    },
    {
      "epoch": 2.2995506739890166,
      "grad_norm": 0.6842076182365417,
      "learning_rate": 0.00047700449326010984,
      "loss": 0.0197,
      "step": 4606
    },
    {
      "epoch": 2.3000499251123316,
      "grad_norm": 0.047346822917461395,
      "learning_rate": 0.0004769995007488767,
      "loss": 0.0013,
      "step": 4607
    },
    {
      "epoch": 2.3005491762356467,
      "grad_norm": 1.4270602464675903,
      "learning_rate": 0.00047699450823764355,
      "loss": 0.0174,
      "step": 4608
    },
    {
      "epoch": 2.3010484273589613,
      "grad_norm": 0.061689287424087524,
      "learning_rate": 0.0004769895157264104,
      "loss": 0.0016,
      "step": 4609
    },
    {
      "epoch": 2.301547678482277,
      "grad_norm": 0.050100065767765045,
      "learning_rate": 0.00047698452321517725,
      "loss": 0.0014,
      "step": 4610
    },
    {
      "epoch": 2.3020469296055914,
      "grad_norm": 0.3804493248462677,
      "learning_rate": 0.0004769795307039441,
      "loss": 0.0054,
      "step": 4611
    },
    {
      "epoch": 2.3025461807289065,
      "grad_norm": 0.9532291293144226,
      "learning_rate": 0.0004769745381927109,
      "loss": 0.0106,
      "step": 4612
    },
    {
      "epoch": 2.3030454318522215,
      "grad_norm": 0.6711781620979309,
      "learning_rate": 0.00047696954568147775,
      "loss": 0.0106,
      "step": 4613
    },
    {
      "epoch": 2.3035446829755366,
      "grad_norm": 0.1312636286020279,
      "learning_rate": 0.0004769645531702446,
      "loss": 0.0024,
      "step": 4614
    },
    {
      "epoch": 2.3040439340988517,
      "grad_norm": 0.02182028628885746,
      "learning_rate": 0.00047695956065901146,
      "loss": 0.0008,
      "step": 4615
    },
    {
      "epoch": 2.3045431852221667,
      "grad_norm": 0.08171384781599045,
      "learning_rate": 0.0004769545681477783,
      "loss": 0.0014,
      "step": 4616
    },
    {
      "epoch": 2.305042436345482,
      "grad_norm": 0.10023380070924759,
      "learning_rate": 0.00047694957563654516,
      "loss": 0.0019,
      "step": 4617
    },
    {
      "epoch": 2.305541687468797,
      "grad_norm": 0.13695847988128662,
      "learning_rate": 0.000476944583125312,
      "loss": 0.0027,
      "step": 4618
    },
    {
      "epoch": 2.306040938592112,
      "grad_norm": 0.055612608790397644,
      "learning_rate": 0.00047693959061407887,
      "loss": 0.0014,
      "step": 4619
    },
    {
      "epoch": 2.306540189715427,
      "grad_norm": 0.7441591024398804,
      "learning_rate": 0.0004769345981028457,
      "loss": 0.0102,
      "step": 4620
    },
    {
      "epoch": 2.307039440838742,
      "grad_norm": 0.7981099486351013,
      "learning_rate": 0.00047692960559161257,
      "loss": 0.013,
      "step": 4621
    },
    {
      "epoch": 2.307538691962057,
      "grad_norm": 0.14487680792808533,
      "learning_rate": 0.0004769246130803794,
      "loss": 0.0018,
      "step": 4622
    },
    {
      "epoch": 2.308037943085372,
      "grad_norm": 0.9363911151885986,
      "learning_rate": 0.0004769196205691463,
      "loss": 0.0035,
      "step": 4623
    },
    {
      "epoch": 2.3085371942086867,
      "grad_norm": 0.17591552436351776,
      "learning_rate": 0.0004769146280579131,
      "loss": 0.0036,
      "step": 4624
    },
    {
      "epoch": 2.309036445332002,
      "grad_norm": 0.17820006608963013,
      "learning_rate": 0.00047690963554668,
      "loss": 0.0034,
      "step": 4625
    },
    {
      "epoch": 2.309535696455317,
      "grad_norm": 0.5277261137962341,
      "learning_rate": 0.00047690464303544683,
      "loss": 0.0155,
      "step": 4626
    },
    {
      "epoch": 2.310034947578632,
      "grad_norm": 0.4654790163040161,
      "learning_rate": 0.0004768996505242137,
      "loss": 0.0038,
      "step": 4627
    },
    {
      "epoch": 2.310534198701947,
      "grad_norm": 0.1568971425294876,
      "learning_rate": 0.00047689465801298053,
      "loss": 0.0023,
      "step": 4628
    },
    {
      "epoch": 2.311033449825262,
      "grad_norm": 1.2676104307174683,
      "learning_rate": 0.0004768896655017474,
      "loss": 0.0144,
      "step": 4629
    },
    {
      "epoch": 2.311532700948577,
      "grad_norm": 0.5893279314041138,
      "learning_rate": 0.00047688467299051424,
      "loss": 0.0042,
      "step": 4630
    },
    {
      "epoch": 2.312031952071892,
      "grad_norm": 0.3390141725540161,
      "learning_rate": 0.0004768796804792811,
      "loss": 0.008,
      "step": 4631
    },
    {
      "epoch": 2.312531203195207,
      "grad_norm": 0.6006159782409668,
      "learning_rate": 0.00047687468796804794,
      "loss": 0.0093,
      "step": 4632
    },
    {
      "epoch": 2.3130304543185223,
      "grad_norm": 0.38877779245376587,
      "learning_rate": 0.0004768696954568148,
      "loss": 0.0048,
      "step": 4633
    },
    {
      "epoch": 2.3135297054418373,
      "grad_norm": 0.07930122315883636,
      "learning_rate": 0.00047686470294558165,
      "loss": 0.0021,
      "step": 4634
    },
    {
      "epoch": 2.3140289565651524,
      "grad_norm": 0.21719567477703094,
      "learning_rate": 0.0004768597104343485,
      "loss": 0.005,
      "step": 4635
    },
    {
      "epoch": 2.3145282076884675,
      "grad_norm": 0.10199153423309326,
      "learning_rate": 0.00047685471792311535,
      "loss": 0.0025,
      "step": 4636
    },
    {
      "epoch": 2.3150274588117825,
      "grad_norm": 0.9026281237602234,
      "learning_rate": 0.0004768497254118822,
      "loss": 0.0129,
      "step": 4637
    },
    {
      "epoch": 2.3155267099350976,
      "grad_norm": 0.6989959478378296,
      "learning_rate": 0.00047684473290064905,
      "loss": 0.008,
      "step": 4638
    },
    {
      "epoch": 2.316025961058412,
      "grad_norm": 0.04241953045129776,
      "learning_rate": 0.0004768397403894159,
      "loss": 0.0014,
      "step": 4639
    },
    {
      "epoch": 2.3165252121817272,
      "grad_norm": 0.38446006178855896,
      "learning_rate": 0.00047683474787818276,
      "loss": 0.0082,
      "step": 4640
    },
    {
      "epoch": 2.3170244633050423,
      "grad_norm": 0.6542332768440247,
      "learning_rate": 0.00047682975536694956,
      "loss": 0.0133,
      "step": 4641
    },
    {
      "epoch": 2.3175237144283574,
      "grad_norm": 0.7526609301567078,
      "learning_rate": 0.0004768247628557164,
      "loss": 0.0189,
      "step": 4642
    },
    {
      "epoch": 2.3180229655516724,
      "grad_norm": 0.6279863119125366,
      "learning_rate": 0.00047681977034448326,
      "loss": 0.0117,
      "step": 4643
    },
    {
      "epoch": 2.3185222166749875,
      "grad_norm": 0.655569314956665,
      "learning_rate": 0.0004768147778332501,
      "loss": 0.0108,
      "step": 4644
    },
    {
      "epoch": 2.3190214677983025,
      "grad_norm": 0.13599713146686554,
      "learning_rate": 0.00047680978532201697,
      "loss": 0.0022,
      "step": 4645
    },
    {
      "epoch": 2.3195207189216176,
      "grad_norm": 0.00962874200195074,
      "learning_rate": 0.0004768047928107838,
      "loss": 0.0007,
      "step": 4646
    },
    {
      "epoch": 2.3200199700449327,
      "grad_norm": 0.14622251689434052,
      "learning_rate": 0.00047679980029955067,
      "loss": 0.0022,
      "step": 4647
    },
    {
      "epoch": 2.3205192211682477,
      "grad_norm": 0.5631265044212341,
      "learning_rate": 0.0004767948077883175,
      "loss": 0.0233,
      "step": 4648
    },
    {
      "epoch": 2.3210184722915628,
      "grad_norm": 0.3131994605064392,
      "learning_rate": 0.0004767898152770844,
      "loss": 0.0071,
      "step": 4649
    },
    {
      "epoch": 2.321517723414878,
      "grad_norm": 0.2289472073316574,
      "learning_rate": 0.0004767848227658512,
      "loss": 0.004,
      "step": 4650
    },
    {
      "epoch": 2.322016974538193,
      "grad_norm": 0.11728109419345856,
      "learning_rate": 0.0004767798302546181,
      "loss": 0.0038,
      "step": 4651
    },
    {
      "epoch": 2.3225162256615075,
      "grad_norm": 0.38849732279777527,
      "learning_rate": 0.00047677483774338493,
      "loss": 0.0102,
      "step": 4652
    },
    {
      "epoch": 2.323015476784823,
      "grad_norm": 0.12751077115535736,
      "learning_rate": 0.0004767698452321518,
      "loss": 0.0029,
      "step": 4653
    },
    {
      "epoch": 2.3235147279081376,
      "grad_norm": 0.9879277944564819,
      "learning_rate": 0.00047676485272091863,
      "loss": 0.0453,
      "step": 4654
    },
    {
      "epoch": 2.3240139790314527,
      "grad_norm": 1.241676688194275,
      "learning_rate": 0.0004767598602096855,
      "loss": 0.0132,
      "step": 4655
    },
    {
      "epoch": 2.3245132301547677,
      "grad_norm": 0.6397120356559753,
      "learning_rate": 0.00047675486769845234,
      "loss": 0.0109,
      "step": 4656
    },
    {
      "epoch": 2.325012481278083,
      "grad_norm": 0.15591438114643097,
      "learning_rate": 0.0004767498751872192,
      "loss": 0.0025,
      "step": 4657
    },
    {
      "epoch": 2.325511732401398,
      "grad_norm": 0.010848922654986382,
      "learning_rate": 0.00047674488267598604,
      "loss": 0.0009,
      "step": 4658
    },
    {
      "epoch": 2.326010983524713,
      "grad_norm": 0.23822233080863953,
      "learning_rate": 0.0004767398901647529,
      "loss": 0.0049,
      "step": 4659
    },
    {
      "epoch": 2.326510234648028,
      "grad_norm": 0.02639099583029747,
      "learning_rate": 0.00047673489765351975,
      "loss": 0.001,
      "step": 4660
    },
    {
      "epoch": 2.327009485771343,
      "grad_norm": 0.02509080432355404,
      "learning_rate": 0.0004767299051422866,
      "loss": 0.0014,
      "step": 4661
    },
    {
      "epoch": 2.327508736894658,
      "grad_norm": 0.11957743018865585,
      "learning_rate": 0.00047672491263105345,
      "loss": 0.0028,
      "step": 4662
    },
    {
      "epoch": 2.328007988017973,
      "grad_norm": 0.6560494303703308,
      "learning_rate": 0.0004767199201198203,
      "loss": 0.0053,
      "step": 4663
    },
    {
      "epoch": 2.328507239141288,
      "grad_norm": 0.06374814361333847,
      "learning_rate": 0.00047671492760858715,
      "loss": 0.0022,
      "step": 4664
    },
    {
      "epoch": 2.3290064902646033,
      "grad_norm": 0.024812588468194008,
      "learning_rate": 0.000476709935097354,
      "loss": 0.0012,
      "step": 4665
    },
    {
      "epoch": 2.3295057413879183,
      "grad_norm": 0.8988092541694641,
      "learning_rate": 0.00047670494258612086,
      "loss": 0.0392,
      "step": 4666
    },
    {
      "epoch": 2.330004992511233,
      "grad_norm": 0.05549399554729462,
      "learning_rate": 0.0004766999500748877,
      "loss": 0.0017,
      "step": 4667
    },
    {
      "epoch": 2.330504243634548,
      "grad_norm": 0.4290328621864319,
      "learning_rate": 0.00047669495756365456,
      "loss": 0.02,
      "step": 4668
    },
    {
      "epoch": 2.331003494757863,
      "grad_norm": 0.3802540898323059,
      "learning_rate": 0.0004766899650524214,
      "loss": 0.0069,
      "step": 4669
    },
    {
      "epoch": 2.331502745881178,
      "grad_norm": 0.039941176772117615,
      "learning_rate": 0.0004766849725411882,
      "loss": 0.0014,
      "step": 4670
    },
    {
      "epoch": 2.332001997004493,
      "grad_norm": 0.3001405596733093,
      "learning_rate": 0.00047667998002995506,
      "loss": 0.0121,
      "step": 4671
    },
    {
      "epoch": 2.3325012481278082,
      "grad_norm": 0.6500089168548584,
      "learning_rate": 0.0004766749875187219,
      "loss": 0.0079,
      "step": 4672
    },
    {
      "epoch": 2.3330004992511233,
      "grad_norm": 0.1959579437971115,
      "learning_rate": 0.00047666999500748877,
      "loss": 0.005,
      "step": 4673
    },
    {
      "epoch": 2.3334997503744384,
      "grad_norm": 0.07100750505924225,
      "learning_rate": 0.0004766650024962556,
      "loss": 0.0014,
      "step": 4674
    },
    {
      "epoch": 2.3339990014977534,
      "grad_norm": 0.10149116069078445,
      "learning_rate": 0.00047666000998502247,
      "loss": 0.0023,
      "step": 4675
    },
    {
      "epoch": 2.3344982526210685,
      "grad_norm": 0.044484950602054596,
      "learning_rate": 0.0004766550174737893,
      "loss": 0.0012,
      "step": 4676
    },
    {
      "epoch": 2.3349975037443835,
      "grad_norm": 0.02202420122921467,
      "learning_rate": 0.0004766500249625562,
      "loss": 0.0007,
      "step": 4677
    },
    {
      "epoch": 2.3354967548676986,
      "grad_norm": 0.2550528049468994,
      "learning_rate": 0.00047664503245132303,
      "loss": 0.0027,
      "step": 4678
    },
    {
      "epoch": 2.3359960059910136,
      "grad_norm": 0.42417654395103455,
      "learning_rate": 0.0004766400399400899,
      "loss": 0.0105,
      "step": 4679
    },
    {
      "epoch": 2.3364952571143283,
      "grad_norm": 0.35265275835990906,
      "learning_rate": 0.00047663504742885673,
      "loss": 0.0079,
      "step": 4680
    },
    {
      "epoch": 2.3369945082376438,
      "grad_norm": 0.040604762732982635,
      "learning_rate": 0.0004766300549176236,
      "loss": 0.0013,
      "step": 4681
    },
    {
      "epoch": 2.3374937593609584,
      "grad_norm": 2.0398128032684326,
      "learning_rate": 0.00047662506240639044,
      "loss": 0.0269,
      "step": 4682
    },
    {
      "epoch": 2.3379930104842734,
      "grad_norm": 0.13437709212303162,
      "learning_rate": 0.0004766200698951573,
      "loss": 0.002,
      "step": 4683
    },
    {
      "epoch": 2.3384922616075885,
      "grad_norm": 0.6376758217811584,
      "learning_rate": 0.00047661507738392414,
      "loss": 0.0197,
      "step": 4684
    },
    {
      "epoch": 2.3389915127309036,
      "grad_norm": 0.046602774411439896,
      "learning_rate": 0.000476610084872691,
      "loss": 0.001,
      "step": 4685
    },
    {
      "epoch": 2.3394907638542186,
      "grad_norm": 0.2659292221069336,
      "learning_rate": 0.00047660509236145785,
      "loss": 0.0029,
      "step": 4686
    },
    {
      "epoch": 2.3399900149775337,
      "grad_norm": 0.2370723932981491,
      "learning_rate": 0.0004766000998502247,
      "loss": 0.0048,
      "step": 4687
    },
    {
      "epoch": 2.3404892661008487,
      "grad_norm": 0.10147944837808609,
      "learning_rate": 0.00047659510733899155,
      "loss": 0.0014,
      "step": 4688
    },
    {
      "epoch": 2.340988517224164,
      "grad_norm": 0.32540640234947205,
      "learning_rate": 0.0004765901148277584,
      "loss": 0.006,
      "step": 4689
    },
    {
      "epoch": 2.341487768347479,
      "grad_norm": 1.2725354433059692,
      "learning_rate": 0.00047658512231652525,
      "loss": 0.0043,
      "step": 4690
    },
    {
      "epoch": 2.341987019470794,
      "grad_norm": 0.3923811614513397,
      "learning_rate": 0.0004765801298052921,
      "loss": 0.0053,
      "step": 4691
    },
    {
      "epoch": 2.342486270594109,
      "grad_norm": 0.024438437074422836,
      "learning_rate": 0.00047657513729405896,
      "loss": 0.0009,
      "step": 4692
    },
    {
      "epoch": 2.342985521717424,
      "grad_norm": 0.21833936870098114,
      "learning_rate": 0.0004765701447828258,
      "loss": 0.004,
      "step": 4693
    },
    {
      "epoch": 2.343484772840739,
      "grad_norm": 0.483278751373291,
      "learning_rate": 0.00047656515227159266,
      "loss": 0.011,
      "step": 4694
    },
    {
      "epoch": 2.3439840239640537,
      "grad_norm": 0.21175874769687653,
      "learning_rate": 0.0004765601597603595,
      "loss": 0.0028,
      "step": 4695
    },
    {
      "epoch": 2.3444832750873688,
      "grad_norm": 0.04050714150071144,
      "learning_rate": 0.00047655516724912637,
      "loss": 0.0013,
      "step": 4696
    },
    {
      "epoch": 2.344982526210684,
      "grad_norm": 0.6117468476295471,
      "learning_rate": 0.0004765501747378932,
      "loss": 0.006,
      "step": 4697
    },
    {
      "epoch": 2.345481777333999,
      "grad_norm": 0.03829333186149597,
      "learning_rate": 0.00047654518222666007,
      "loss": 0.0011,
      "step": 4698
    },
    {
      "epoch": 2.345981028457314,
      "grad_norm": 0.3836466073989868,
      "learning_rate": 0.0004765401897154269,
      "loss": 0.0036,
      "step": 4699
    },
    {
      "epoch": 2.346480279580629,
      "grad_norm": 0.566221296787262,
      "learning_rate": 0.0004765351972041937,
      "loss": 0.0145,
      "step": 4700
    },
    {
      "epoch": 2.346979530703944,
      "grad_norm": 0.6674289107322693,
      "learning_rate": 0.0004765302046929605,
      "loss": 0.0113,
      "step": 4701
    },
    {
      "epoch": 2.347478781827259,
      "grad_norm": 0.19034403562545776,
      "learning_rate": 0.00047652521218172737,
      "loss": 0.0028,
      "step": 4702
    },
    {
      "epoch": 2.347978032950574,
      "grad_norm": 0.44009947776794434,
      "learning_rate": 0.0004765202196704942,
      "loss": 0.0086,
      "step": 4703
    },
    {
      "epoch": 2.3484772840738892,
      "grad_norm": 1.877665638923645,
      "learning_rate": 0.0004765152271592611,
      "loss": 0.008,
      "step": 4704
    },
    {
      "epoch": 2.3489765351972043,
      "grad_norm": 0.012051151134073734,
      "learning_rate": 0.0004765102346480279,
      "loss": 0.0007,
      "step": 4705
    },
    {
      "epoch": 2.3494757863205193,
      "grad_norm": 0.24379700422286987,
      "learning_rate": 0.0004765052421367948,
      "loss": 0.0035,
      "step": 4706
    },
    {
      "epoch": 2.3499750374438344,
      "grad_norm": 1.8140226602554321,
      "learning_rate": 0.00047650024962556163,
      "loss": 0.0388,
      "step": 4707
    },
    {
      "epoch": 2.3504742885671495,
      "grad_norm": 0.16489049792289734,
      "learning_rate": 0.0004764952571143285,
      "loss": 0.0019,
      "step": 4708
    },
    {
      "epoch": 2.3509735396904645,
      "grad_norm": 0.5345929265022278,
      "learning_rate": 0.00047649026460309533,
      "loss": 0.007,
      "step": 4709
    },
    {
      "epoch": 2.351472790813779,
      "grad_norm": 0.36133459210395813,
      "learning_rate": 0.0004764852720918622,
      "loss": 0.0226,
      "step": 4710
    },
    {
      "epoch": 2.351972041937094,
      "grad_norm": 0.09130381047725677,
      "learning_rate": 0.00047648027958062904,
      "loss": 0.0024,
      "step": 4711
    },
    {
      "epoch": 2.3524712930604093,
      "grad_norm": 0.5745621919631958,
      "learning_rate": 0.0004764752870693959,
      "loss": 0.0045,
      "step": 4712
    },
    {
      "epoch": 2.3529705441837243,
      "grad_norm": 0.5977852940559387,
      "learning_rate": 0.00047647029455816274,
      "loss": 0.0071,
      "step": 4713
    },
    {
      "epoch": 2.3534697953070394,
      "grad_norm": 0.20884373784065247,
      "learning_rate": 0.0004764653020469296,
      "loss": 0.0037,
      "step": 4714
    },
    {
      "epoch": 2.3539690464303544,
      "grad_norm": 0.48900362849235535,
      "learning_rate": 0.00047646030953569645,
      "loss": 0.0185,
      "step": 4715
    },
    {
      "epoch": 2.3544682975536695,
      "grad_norm": 0.12794756889343262,
      "learning_rate": 0.0004764553170244633,
      "loss": 0.0026,
      "step": 4716
    },
    {
      "epoch": 2.3549675486769845,
      "grad_norm": 0.28684067726135254,
      "learning_rate": 0.00047645032451323015,
      "loss": 0.0054,
      "step": 4717
    },
    {
      "epoch": 2.3554667998002996,
      "grad_norm": 0.20553480088710785,
      "learning_rate": 0.000476445332001997,
      "loss": 0.0038,
      "step": 4718
    },
    {
      "epoch": 2.3559660509236147,
      "grad_norm": 0.28189927339553833,
      "learning_rate": 0.00047644033949076386,
      "loss": 0.0026,
      "step": 4719
    },
    {
      "epoch": 2.3564653020469297,
      "grad_norm": 0.079177625477314,
      "learning_rate": 0.0004764353469795307,
      "loss": 0.0016,
      "step": 4720
    },
    {
      "epoch": 2.356964553170245,
      "grad_norm": 0.8290961980819702,
      "learning_rate": 0.00047643035446829756,
      "loss": 0.0173,
      "step": 4721
    },
    {
      "epoch": 2.35746380429356,
      "grad_norm": 0.11791832000017166,
      "learning_rate": 0.0004764253619570644,
      "loss": 0.002,
      "step": 4722
    },
    {
      "epoch": 2.3579630554168745,
      "grad_norm": 0.8176766037940979,
      "learning_rate": 0.00047642036944583126,
      "loss": 0.0277,
      "step": 4723
    },
    {
      "epoch": 2.35846230654019,
      "grad_norm": 0.029993118718266487,
      "learning_rate": 0.0004764153769345981,
      "loss": 0.0011,
      "step": 4724
    },
    {
      "epoch": 2.3589615576635046,
      "grad_norm": 0.33855345845222473,
      "learning_rate": 0.00047641038442336497,
      "loss": 0.0034,
      "step": 4725
    },
    {
      "epoch": 2.3594608087868196,
      "grad_norm": 0.3676973283290863,
      "learning_rate": 0.0004764053919121318,
      "loss": 0.0074,
      "step": 4726
    },
    {
      "epoch": 2.3599600599101347,
      "grad_norm": 0.36831480264663696,
      "learning_rate": 0.00047640039940089867,
      "loss": 0.0064,
      "step": 4727
    },
    {
      "epoch": 2.3604593110334497,
      "grad_norm": 0.250311940908432,
      "learning_rate": 0.0004763954068896655,
      "loss": 0.0062,
      "step": 4728
    },
    {
      "epoch": 2.360958562156765,
      "grad_norm": 1.2073396444320679,
      "learning_rate": 0.0004763904143784323,
      "loss": 0.024,
      "step": 4729
    },
    {
      "epoch": 2.36145781328008,
      "grad_norm": 0.26990601420402527,
      "learning_rate": 0.0004763854218671992,
      "loss": 0.0048,
      "step": 4730
    },
    {
      "epoch": 2.361957064403395,
      "grad_norm": 0.313791960477829,
      "learning_rate": 0.000476380429355966,
      "loss": 0.0018,
      "step": 4731
    },
    {
      "epoch": 2.36245631552671,
      "grad_norm": 0.25584185123443604,
      "learning_rate": 0.0004763754368447329,
      "loss": 0.0036,
      "step": 4732
    },
    {
      "epoch": 2.362955566650025,
      "grad_norm": 0.5282586812973022,
      "learning_rate": 0.00047637044433349973,
      "loss": 0.0147,
      "step": 4733
    },
    {
      "epoch": 2.36345481777334,
      "grad_norm": 0.4617844521999359,
      "learning_rate": 0.0004763654518222666,
      "loss": 0.0099,
      "step": 4734
    },
    {
      "epoch": 2.363954068896655,
      "grad_norm": 0.4115346074104309,
      "learning_rate": 0.00047636045931103343,
      "loss": 0.0057,
      "step": 4735
    },
    {
      "epoch": 2.36445332001997,
      "grad_norm": 0.2662895619869232,
      "learning_rate": 0.0004763554667998003,
      "loss": 0.0018,
      "step": 4736
    },
    {
      "epoch": 2.3649525711432853,
      "grad_norm": 0.19459496438503265,
      "learning_rate": 0.00047635047428856714,
      "loss": 0.0039,
      "step": 4737
    },
    {
      "epoch": 2.3654518222666,
      "grad_norm": 0.30083778500556946,
      "learning_rate": 0.000476345481777334,
      "loss": 0.005,
      "step": 4738
    },
    {
      "epoch": 2.365951073389915,
      "grad_norm": 0.1866927593946457,
      "learning_rate": 0.00047634048926610084,
      "loss": 0.0014,
      "step": 4739
    },
    {
      "epoch": 2.36645032451323,
      "grad_norm": 0.08693967014551163,
      "learning_rate": 0.0004763354967548677,
      "loss": 0.0016,
      "step": 4740
    },
    {
      "epoch": 2.366949575636545,
      "grad_norm": 0.2566964328289032,
      "learning_rate": 0.00047633050424363455,
      "loss": 0.0036,
      "step": 4741
    },
    {
      "epoch": 2.36744882675986,
      "grad_norm": 0.8938944935798645,
      "learning_rate": 0.0004763255117324014,
      "loss": 0.0023,
      "step": 4742
    },
    {
      "epoch": 2.367948077883175,
      "grad_norm": 0.15065285563468933,
      "learning_rate": 0.00047632051922116825,
      "loss": 0.0027,
      "step": 4743
    },
    {
      "epoch": 2.3684473290064902,
      "grad_norm": 0.04448191076517105,
      "learning_rate": 0.0004763155267099351,
      "loss": 0.0012,
      "step": 4744
    },
    {
      "epoch": 2.3689465801298053,
      "grad_norm": 0.01030239462852478,
      "learning_rate": 0.00047631053419870195,
      "loss": 0.0006,
      "step": 4745
    },
    {
      "epoch": 2.3694458312531204,
      "grad_norm": 0.680260181427002,
      "learning_rate": 0.0004763055416874688,
      "loss": 0.0103,
      "step": 4746
    },
    {
      "epoch": 2.3699450823764354,
      "grad_norm": 0.23524847626686096,
      "learning_rate": 0.00047630054917623566,
      "loss": 0.0179,
      "step": 4747
    },
    {
      "epoch": 2.3704443334997505,
      "grad_norm": 0.1297941952943802,
      "learning_rate": 0.0004762955566650025,
      "loss": 0.0024,
      "step": 4748
    },
    {
      "epoch": 2.3709435846230655,
      "grad_norm": 0.09653795510530472,
      "learning_rate": 0.00047629056415376936,
      "loss": 0.002,
      "step": 4749
    },
    {
      "epoch": 2.3714428357463806,
      "grad_norm": 0.08236240595579147,
      "learning_rate": 0.0004762855716425362,
      "loss": 0.0012,
      "step": 4750
    },
    {
      "epoch": 2.371942086869695,
      "grad_norm": 0.26777833700180054,
      "learning_rate": 0.00047628057913130307,
      "loss": 0.0028,
      "step": 4751
    },
    {
      "epoch": 2.3724413379930107,
      "grad_norm": 0.016621746122837067,
      "learning_rate": 0.0004762755866200699,
      "loss": 0.0009,
      "step": 4752
    },
    {
      "epoch": 2.3729405891163253,
      "grad_norm": 0.4843667447566986,
      "learning_rate": 0.00047627059410883677,
      "loss": 0.0053,
      "step": 4753
    },
    {
      "epoch": 2.3734398402396404,
      "grad_norm": 0.06936472654342651,
      "learning_rate": 0.0004762656015976036,
      "loss": 0.0013,
      "step": 4754
    },
    {
      "epoch": 2.3739390913629554,
      "grad_norm": 0.4099022150039673,
      "learning_rate": 0.0004762606090863705,
      "loss": 0.0055,
      "step": 4755
    },
    {
      "epoch": 2.3744383424862705,
      "grad_norm": 1.1809806823730469,
      "learning_rate": 0.00047625561657513733,
      "loss": 0.0622,
      "step": 4756
    },
    {
      "epoch": 2.3749375936095856,
      "grad_norm": 0.22050867974758148,
      "learning_rate": 0.0004762506240639042,
      "loss": 0.0018,
      "step": 4757
    },
    {
      "epoch": 2.3754368447329006,
      "grad_norm": 0.05769060552120209,
      "learning_rate": 0.000476245631552671,
      "loss": 0.0016,
      "step": 4758
    },
    {
      "epoch": 2.3759360958562157,
      "grad_norm": 0.1885516196489334,
      "learning_rate": 0.00047624063904143783,
      "loss": 0.0032,
      "step": 4759
    },
    {
      "epoch": 2.3764353469795307,
      "grad_norm": 0.2164970189332962,
      "learning_rate": 0.0004762356465302047,
      "loss": 0.0022,
      "step": 4760
    },
    {
      "epoch": 2.376934598102846,
      "grad_norm": 0.8005385994911194,
      "learning_rate": 0.00047623065401897153,
      "loss": 0.0119,
      "step": 4761
    },
    {
      "epoch": 2.377433849226161,
      "grad_norm": 0.04164827987551689,
      "learning_rate": 0.0004762256615077384,
      "loss": 0.0012,
      "step": 4762
    },
    {
      "epoch": 2.377933100349476,
      "grad_norm": 0.24324338138103485,
      "learning_rate": 0.00047622066899650524,
      "loss": 0.0045,
      "step": 4763
    },
    {
      "epoch": 2.378432351472791,
      "grad_norm": 0.16115480661392212,
      "learning_rate": 0.0004762156764852721,
      "loss": 0.0016,
      "step": 4764
    },
    {
      "epoch": 2.378931602596106,
      "grad_norm": 0.197727233171463,
      "learning_rate": 0.00047621068397403894,
      "loss": 0.0033,
      "step": 4765
    },
    {
      "epoch": 2.3794308537194206,
      "grad_norm": 0.008008928038179874,
      "learning_rate": 0.0004762056914628058,
      "loss": 0.0006,
      "step": 4766
    },
    {
      "epoch": 2.3799301048427357,
      "grad_norm": 0.15311256051063538,
      "learning_rate": 0.00047620069895157265,
      "loss": 0.0032,
      "step": 4767
    },
    {
      "epoch": 2.3804293559660508,
      "grad_norm": 0.060101937502622604,
      "learning_rate": 0.0004761957064403395,
      "loss": 0.0017,
      "step": 4768
    },
    {
      "epoch": 2.380928607089366,
      "grad_norm": 0.021429583430290222,
      "learning_rate": 0.00047619071392910635,
      "loss": 0.0008,
      "step": 4769
    },
    {
      "epoch": 2.381427858212681,
      "grad_norm": 0.14620615541934967,
      "learning_rate": 0.0004761857214178732,
      "loss": 0.0019,
      "step": 4770
    },
    {
      "epoch": 2.381927109335996,
      "grad_norm": 0.967170774936676,
      "learning_rate": 0.00047618072890664005,
      "loss": 0.0098,
      "step": 4771
    },
    {
      "epoch": 2.382426360459311,
      "grad_norm": 0.16759386658668518,
      "learning_rate": 0.0004761757363954069,
      "loss": 0.0018,
      "step": 4772
    },
    {
      "epoch": 2.382925611582626,
      "grad_norm": 0.018904123455286026,
      "learning_rate": 0.00047617074388417376,
      "loss": 0.0008,
      "step": 4773
    },
    {
      "epoch": 2.383424862705941,
      "grad_norm": 0.12908875942230225,
      "learning_rate": 0.0004761657513729406,
      "loss": 0.0015,
      "step": 4774
    },
    {
      "epoch": 2.383924113829256,
      "grad_norm": 0.4276977479457855,
      "learning_rate": 0.00047616075886170746,
      "loss": 0.0049,
      "step": 4775
    },
    {
      "epoch": 2.3844233649525712,
      "grad_norm": 0.0753820389509201,
      "learning_rate": 0.0004761557663504743,
      "loss": 0.0013,
      "step": 4776
    },
    {
      "epoch": 2.3849226160758863,
      "grad_norm": 0.05117684230208397,
      "learning_rate": 0.00047615077383924117,
      "loss": 0.0013,
      "step": 4777
    },
    {
      "epoch": 2.3854218671992014,
      "grad_norm": 0.018769895657896996,
      "learning_rate": 0.000476145781328008,
      "loss": 0.0008,
      "step": 4778
    },
    {
      "epoch": 2.3859211183225164,
      "grad_norm": 0.020276915282011032,
      "learning_rate": 0.00047614078881677487,
      "loss": 0.0006,
      "step": 4779
    },
    {
      "epoch": 2.3864203694458315,
      "grad_norm": 0.4382351040840149,
      "learning_rate": 0.0004761357963055417,
      "loss": 0.0019,
      "step": 4780
    },
    {
      "epoch": 2.386919620569146,
      "grad_norm": 0.015425998717546463,
      "learning_rate": 0.0004761308037943086,
      "loss": 0.0006,
      "step": 4781
    },
    {
      "epoch": 2.387418871692461,
      "grad_norm": 0.43739816546440125,
      "learning_rate": 0.00047612581128307543,
      "loss": 0.0028,
      "step": 4782
    },
    {
      "epoch": 2.387918122815776,
      "grad_norm": 0.05746741220355034,
      "learning_rate": 0.0004761208187718423,
      "loss": 0.0015,
      "step": 4783
    },
    {
      "epoch": 2.3884173739390913,
      "grad_norm": 0.09633027017116547,
      "learning_rate": 0.00047611582626060913,
      "loss": 0.0013,
      "step": 4784
    },
    {
      "epoch": 2.3889166250624063,
      "grad_norm": 0.16492921113967896,
      "learning_rate": 0.000476110833749376,
      "loss": 0.0023,
      "step": 4785
    },
    {
      "epoch": 2.3894158761857214,
      "grad_norm": 0.023812884464859962,
      "learning_rate": 0.00047610584123814284,
      "loss": 0.0007,
      "step": 4786
    },
    {
      "epoch": 2.3899151273090364,
      "grad_norm": 0.2946128845214844,
      "learning_rate": 0.00047610084872690963,
      "loss": 0.0042,
      "step": 4787
    },
    {
      "epoch": 2.3904143784323515,
      "grad_norm": 0.14614592492580414,
      "learning_rate": 0.0004760958562156765,
      "loss": 0.0015,
      "step": 4788
    },
    {
      "epoch": 2.3909136295556666,
      "grad_norm": 0.31186583638191223,
      "learning_rate": 0.00047609086370444334,
      "loss": 0.0112,
      "step": 4789
    },
    {
      "epoch": 2.3914128806789816,
      "grad_norm": 0.20216935873031616,
      "learning_rate": 0.0004760858711932102,
      "loss": 0.0028,
      "step": 4790
    },
    {
      "epoch": 2.3919121318022967,
      "grad_norm": 0.3692326247692108,
      "learning_rate": 0.00047608087868197704,
      "loss": 0.0059,
      "step": 4791
    },
    {
      "epoch": 2.3924113829256117,
      "grad_norm": 0.3723331689834595,
      "learning_rate": 0.0004760758861707439,
      "loss": 0.0037,
      "step": 4792
    },
    {
      "epoch": 2.392910634048927,
      "grad_norm": 0.35793301463127136,
      "learning_rate": 0.00047607089365951075,
      "loss": 0.0033,
      "step": 4793
    },
    {
      "epoch": 2.3934098851722414,
      "grad_norm": 0.008169306442141533,
      "learning_rate": 0.0004760659011482776,
      "loss": 0.0005,
      "step": 4794
    },
    {
      "epoch": 2.393909136295557,
      "grad_norm": 0.4157765507698059,
      "learning_rate": 0.00047606090863704445,
      "loss": 0.0085,
      "step": 4795
    },
    {
      "epoch": 2.3944083874188715,
      "grad_norm": 0.016962172463536263,
      "learning_rate": 0.0004760559161258113,
      "loss": 0.0007,
      "step": 4796
    },
    {
      "epoch": 2.3949076385421866,
      "grad_norm": 0.5384810566902161,
      "learning_rate": 0.00047605092361457815,
      "loss": 0.0057,
      "step": 4797
    },
    {
      "epoch": 2.3954068896655016,
      "grad_norm": 0.04366356134414673,
      "learning_rate": 0.000476045931103345,
      "loss": 0.0008,
      "step": 4798
    },
    {
      "epoch": 2.3959061407888167,
      "grad_norm": 0.2112981379032135,
      "learning_rate": 0.00047604093859211186,
      "loss": 0.0032,
      "step": 4799
    },
    {
      "epoch": 2.3964053919121318,
      "grad_norm": 0.10404031723737717,
      "learning_rate": 0.0004760359460808787,
      "loss": 0.0013,
      "step": 4800
    },
    {
      "epoch": 2.396904643035447,
      "grad_norm": 1.154585838317871,
      "learning_rate": 0.00047603095356964556,
      "loss": 0.0194,
      "step": 4801
    },
    {
      "epoch": 2.397403894158762,
      "grad_norm": 0.05442585423588753,
      "learning_rate": 0.0004760259610584124,
      "loss": 0.0012,
      "step": 4802
    },
    {
      "epoch": 2.397903145282077,
      "grad_norm": 0.030379408970475197,
      "learning_rate": 0.00047602096854717927,
      "loss": 0.0007,
      "step": 4803
    },
    {
      "epoch": 2.398402396405392,
      "grad_norm": 0.35747960209846497,
      "learning_rate": 0.0004760159760359461,
      "loss": 0.0047,
      "step": 4804
    },
    {
      "epoch": 2.398901647528707,
      "grad_norm": 0.03261930122971535,
      "learning_rate": 0.00047601098352471297,
      "loss": 0.0008,
      "step": 4805
    },
    {
      "epoch": 2.399400898652022,
      "grad_norm": 0.03222334757447243,
      "learning_rate": 0.0004760059910134798,
      "loss": 0.001,
      "step": 4806
    },
    {
      "epoch": 2.399900149775337,
      "grad_norm": 0.20932574570178986,
      "learning_rate": 0.0004760009985022467,
      "loss": 0.004,
      "step": 4807
    },
    {
      "epoch": 2.4003994008986522,
      "grad_norm": 0.4250860810279846,
      "learning_rate": 0.0004759960059910135,
      "loss": 0.0074,
      "step": 4808
    },
    {
      "epoch": 2.400898652021967,
      "grad_norm": 0.4344140589237213,
      "learning_rate": 0.0004759910134797804,
      "loss": 0.0062,
      "step": 4809
    },
    {
      "epoch": 2.401397903145282,
      "grad_norm": 0.009048882871866226,
      "learning_rate": 0.00047598602096854723,
      "loss": 0.0005,
      "step": 4810
    },
    {
      "epoch": 2.401897154268597,
      "grad_norm": 0.461568683385849,
      "learning_rate": 0.0004759810284573141,
      "loss": 0.0469,
      "step": 4811
    },
    {
      "epoch": 2.402396405391912,
      "grad_norm": 1.2655935287475586,
      "learning_rate": 0.00047597603594608093,
      "loss": 0.0232,
      "step": 4812
    },
    {
      "epoch": 2.402895656515227,
      "grad_norm": 0.017077013850212097,
      "learning_rate": 0.0004759710434348478,
      "loss": 0.0008,
      "step": 4813
    },
    {
      "epoch": 2.403394907638542,
      "grad_norm": 0.12579600512981415,
      "learning_rate": 0.00047596605092361464,
      "loss": 0.0023,
      "step": 4814
    },
    {
      "epoch": 2.403894158761857,
      "grad_norm": 0.0070875221863389015,
      "learning_rate": 0.0004759610584123815,
      "loss": 0.0005,
      "step": 4815
    },
    {
      "epoch": 2.4043934098851723,
      "grad_norm": 0.019977977499365807,
      "learning_rate": 0.00047595606590114823,
      "loss": 0.0008,
      "step": 4816
    },
    {
      "epoch": 2.4048926610084873,
      "grad_norm": 0.03882059082388878,
      "learning_rate": 0.0004759510733899151,
      "loss": 0.0011,
      "step": 4817
    },
    {
      "epoch": 2.4053919121318024,
      "grad_norm": 0.41204479336738586,
      "learning_rate": 0.00047594608087868194,
      "loss": 0.0168,
      "step": 4818
    },
    {
      "epoch": 2.4058911632551174,
      "grad_norm": 0.02003878355026245,
      "learning_rate": 0.0004759410883674488,
      "loss": 0.0011,
      "step": 4819
    },
    {
      "epoch": 2.4063904143784325,
      "grad_norm": 0.04537549614906311,
      "learning_rate": 0.00047593609585621564,
      "loss": 0.001,
      "step": 4820
    },
    {
      "epoch": 2.4068896655017475,
      "grad_norm": 0.06982095539569855,
      "learning_rate": 0.0004759311033449825,
      "loss": 0.0013,
      "step": 4821
    },
    {
      "epoch": 2.407388916625062,
      "grad_norm": 0.22851547598838806,
      "learning_rate": 0.00047592611083374935,
      "loss": 0.0035,
      "step": 4822
    },
    {
      "epoch": 2.4078881677483777,
      "grad_norm": 2.9311916828155518,
      "learning_rate": 0.0004759211183225162,
      "loss": 0.0167,
      "step": 4823
    },
    {
      "epoch": 2.4083874188716923,
      "grad_norm": 0.06388591229915619,
      "learning_rate": 0.00047591612581128305,
      "loss": 0.0018,
      "step": 4824
    },
    {
      "epoch": 2.4088866699950073,
      "grad_norm": 0.04718133062124252,
      "learning_rate": 0.0004759111333000499,
      "loss": 0.0013,
      "step": 4825
    },
    {
      "epoch": 2.4093859211183224,
      "grad_norm": 0.04839049652218819,
      "learning_rate": 0.00047590614078881676,
      "loss": 0.0009,
      "step": 4826
    },
    {
      "epoch": 2.4098851722416375,
      "grad_norm": 0.41707003116607666,
      "learning_rate": 0.0004759011482775836,
      "loss": 0.0063,
      "step": 4827
    },
    {
      "epoch": 2.4103844233649525,
      "grad_norm": 0.007929890416562557,
      "learning_rate": 0.00047589615576635046,
      "loss": 0.0006,
      "step": 4828
    },
    {
      "epoch": 2.4108836744882676,
      "grad_norm": 0.35345107316970825,
      "learning_rate": 0.0004758911632551173,
      "loss": 0.0077,
      "step": 4829
    },
    {
      "epoch": 2.4113829256115826,
      "grad_norm": 0.9564809799194336,
      "learning_rate": 0.00047588617074388416,
      "loss": 0.018,
      "step": 4830
    },
    {
      "epoch": 2.4118821767348977,
      "grad_norm": 0.05744690075516701,
      "learning_rate": 0.000475881178232651,
      "loss": 0.0014,
      "step": 4831
    },
    {
      "epoch": 2.4123814278582127,
      "grad_norm": 0.012432191520929337,
      "learning_rate": 0.00047587618572141787,
      "loss": 0.0008,
      "step": 4832
    },
    {
      "epoch": 2.412880678981528,
      "grad_norm": 1.4892668724060059,
      "learning_rate": 0.0004758711932101847,
      "loss": 0.0132,
      "step": 4833
    },
    {
      "epoch": 2.413379930104843,
      "grad_norm": 0.3606247901916504,
      "learning_rate": 0.00047586620069895157,
      "loss": 0.0039,
      "step": 4834
    },
    {
      "epoch": 2.413879181228158,
      "grad_norm": 0.011309954337775707,
      "learning_rate": 0.0004758612081877184,
      "loss": 0.0007,
      "step": 4835
    },
    {
      "epoch": 2.414378432351473,
      "grad_norm": 0.01943853311240673,
      "learning_rate": 0.0004758562156764853,
      "loss": 0.0009,
      "step": 4836
    },
    {
      "epoch": 2.4148776834747876,
      "grad_norm": 0.8402853608131409,
      "learning_rate": 0.00047585122316525213,
      "loss": 0.0015,
      "step": 4837
    },
    {
      "epoch": 2.4153769345981027,
      "grad_norm": 0.022340746596455574,
      "learning_rate": 0.000475846230654019,
      "loss": 0.0009,
      "step": 4838
    },
    {
      "epoch": 2.4158761857214177,
      "grad_norm": 0.014260717667639256,
      "learning_rate": 0.00047584123814278583,
      "loss": 0.0008,
      "step": 4839
    },
    {
      "epoch": 2.4163754368447328,
      "grad_norm": 0.334651917219162,
      "learning_rate": 0.0004758362456315527,
      "loss": 0.0194,
      "step": 4840
    },
    {
      "epoch": 2.416874687968048,
      "grad_norm": 0.2658434510231018,
      "learning_rate": 0.00047583125312031954,
      "loss": 0.0068,
      "step": 4841
    },
    {
      "epoch": 2.417373939091363,
      "grad_norm": 0.16524967551231384,
      "learning_rate": 0.0004758262606090864,
      "loss": 0.0027,
      "step": 4842
    },
    {
      "epoch": 2.417873190214678,
      "grad_norm": 0.13266687095165253,
      "learning_rate": 0.00047582126809785324,
      "loss": 0.0029,
      "step": 4843
    },
    {
      "epoch": 2.418372441337993,
      "grad_norm": 0.04294844716787338,
      "learning_rate": 0.0004758162755866201,
      "loss": 0.0012,
      "step": 4844
    },
    {
      "epoch": 2.418871692461308,
      "grad_norm": 0.20646841824054718,
      "learning_rate": 0.0004758112830753869,
      "loss": 0.0036,
      "step": 4845
    },
    {
      "epoch": 2.419370943584623,
      "grad_norm": 0.10800601541996002,
      "learning_rate": 0.00047580629056415374,
      "loss": 0.0021,
      "step": 4846
    },
    {
      "epoch": 2.419870194707938,
      "grad_norm": 0.35370880365371704,
      "learning_rate": 0.0004758012980529206,
      "loss": 0.0063,
      "step": 4847
    },
    {
      "epoch": 2.4203694458312532,
      "grad_norm": 0.4528515338897705,
      "learning_rate": 0.00047579630554168745,
      "loss": 0.0093,
      "step": 4848
    },
    {
      "epoch": 2.4208686969545683,
      "grad_norm": 0.5061532258987427,
      "learning_rate": 0.0004757913130304543,
      "loss": 0.0139,
      "step": 4849
    },
    {
      "epoch": 2.4213679480778834,
      "grad_norm": 0.04123421385884285,
      "learning_rate": 0.00047578632051922115,
      "loss": 0.0014,
      "step": 4850
    },
    {
      "epoch": 2.4218671992011984,
      "grad_norm": 0.02787618897855282,
      "learning_rate": 0.000475781328007988,
      "loss": 0.0011,
      "step": 4851
    },
    {
      "epoch": 2.422366450324513,
      "grad_norm": 0.10101879388093948,
      "learning_rate": 0.00047577633549675486,
      "loss": 0.0014,
      "step": 4852
    },
    {
      "epoch": 2.422865701447828,
      "grad_norm": 0.13352826237678528,
      "learning_rate": 0.0004757713429855217,
      "loss": 0.0025,
      "step": 4853
    },
    {
      "epoch": 2.423364952571143,
      "grad_norm": 0.39165231585502625,
      "learning_rate": 0.00047576635047428856,
      "loss": 0.0048,
      "step": 4854
    },
    {
      "epoch": 2.423864203694458,
      "grad_norm": 0.09380310773849487,
      "learning_rate": 0.0004757613579630554,
      "loss": 0.0018,
      "step": 4855
    },
    {
      "epoch": 2.4243634548177733,
      "grad_norm": 0.11802557110786438,
      "learning_rate": 0.00047575636545182226,
      "loss": 0.0015,
      "step": 4856
    },
    {
      "epoch": 2.4248627059410883,
      "grad_norm": 0.14945894479751587,
      "learning_rate": 0.0004757513729405891,
      "loss": 0.0031,
      "step": 4857
    },
    {
      "epoch": 2.4253619570644034,
      "grad_norm": 0.21292410790920258,
      "learning_rate": 0.00047574638042935597,
      "loss": 0.0028,
      "step": 4858
    },
    {
      "epoch": 2.4258612081877184,
      "grad_norm": 0.7747000455856323,
      "learning_rate": 0.0004757413879181228,
      "loss": 0.0214,
      "step": 4859
    },
    {
      "epoch": 2.4263604593110335,
      "grad_norm": 0.056417014449834824,
      "learning_rate": 0.00047573639540688967,
      "loss": 0.0009,
      "step": 4860
    },
    {
      "epoch": 2.4268597104343486,
      "grad_norm": 0.6912304759025574,
      "learning_rate": 0.0004757314028956565,
      "loss": 0.0034,
      "step": 4861
    },
    {
      "epoch": 2.4273589615576636,
      "grad_norm": 0.124320387840271,
      "learning_rate": 0.0004757264103844234,
      "loss": 0.0024,
      "step": 4862
    },
    {
      "epoch": 2.4278582126809787,
      "grad_norm": 0.6951799392700195,
      "learning_rate": 0.00047572141787319023,
      "loss": 0.0198,
      "step": 4863
    },
    {
      "epoch": 2.4283574638042937,
      "grad_norm": 0.6904380321502686,
      "learning_rate": 0.0004757164253619571,
      "loss": 0.0112,
      "step": 4864
    },
    {
      "epoch": 2.4288567149276084,
      "grad_norm": 0.13311058282852173,
      "learning_rate": 0.00047571143285072393,
      "loss": 0.0091,
      "step": 4865
    },
    {
      "epoch": 2.429355966050924,
      "grad_norm": 0.03724926337599754,
      "learning_rate": 0.0004757064403394908,
      "loss": 0.0011,
      "step": 4866
    },
    {
      "epoch": 2.4298552171742385,
      "grad_norm": 0.05458216369152069,
      "learning_rate": 0.00047570144782825764,
      "loss": 0.0013,
      "step": 4867
    },
    {
      "epoch": 2.4303544682975535,
      "grad_norm": 0.13751280307769775,
      "learning_rate": 0.0004756964553170245,
      "loss": 0.0034,
      "step": 4868
    },
    {
      "epoch": 2.4308537194208686,
      "grad_norm": 1.2244387865066528,
      "learning_rate": 0.00047569146280579134,
      "loss": 0.018,
      "step": 4869
    },
    {
      "epoch": 2.4313529705441836,
      "grad_norm": 0.13548916578292847,
      "learning_rate": 0.0004756864702945582,
      "loss": 0.0014,
      "step": 4870
    },
    {
      "epoch": 2.4318522216674987,
      "grad_norm": 0.19597114622592926,
      "learning_rate": 0.00047568147778332504,
      "loss": 0.0035,
      "step": 4871
    },
    {
      "epoch": 2.4323514727908138,
      "grad_norm": 0.18186885118484497,
      "learning_rate": 0.0004756764852720919,
      "loss": 0.0036,
      "step": 4872
    },
    {
      "epoch": 2.432850723914129,
      "grad_norm": 0.761468768119812,
      "learning_rate": 0.00047567149276085875,
      "loss": 0.0068,
      "step": 4873
    },
    {
      "epoch": 2.433349975037444,
      "grad_norm": 0.21775931119918823,
      "learning_rate": 0.00047566650024962555,
      "loss": 0.0029,
      "step": 4874
    },
    {
      "epoch": 2.433849226160759,
      "grad_norm": 0.17090146243572235,
      "learning_rate": 0.0004756615077383924,
      "loss": 0.0017,
      "step": 4875
    },
    {
      "epoch": 2.434348477284074,
      "grad_norm": 0.7251971960067749,
      "learning_rate": 0.00047565651522715925,
      "loss": 0.003,
      "step": 4876
    },
    {
      "epoch": 2.434847728407389,
      "grad_norm": 0.5255690217018127,
      "learning_rate": 0.0004756515227159261,
      "loss": 0.0054,
      "step": 4877
    },
    {
      "epoch": 2.435346979530704,
      "grad_norm": 0.13318786025047302,
      "learning_rate": 0.00047564653020469295,
      "loss": 0.0018,
      "step": 4878
    },
    {
      "epoch": 2.435846230654019,
      "grad_norm": 0.18836233019828796,
      "learning_rate": 0.0004756415376934598,
      "loss": 0.0038,
      "step": 4879
    },
    {
      "epoch": 2.436345481777334,
      "grad_norm": 0.4921409785747528,
      "learning_rate": 0.00047563654518222666,
      "loss": 0.003,
      "step": 4880
    },
    {
      "epoch": 2.436844732900649,
      "grad_norm": 0.04174666851758957,
      "learning_rate": 0.0004756315526709935,
      "loss": 0.0008,
      "step": 4881
    },
    {
      "epoch": 2.437343984023964,
      "grad_norm": 1.406969428062439,
      "learning_rate": 0.00047562656015976036,
      "loss": 0.0166,
      "step": 4882
    },
    {
      "epoch": 2.437843235147279,
      "grad_norm": 1.0142004489898682,
      "learning_rate": 0.0004756215676485272,
      "loss": 0.0092,
      "step": 4883
    },
    {
      "epoch": 2.438342486270594,
      "grad_norm": 0.3524397909641266,
      "learning_rate": 0.00047561657513729407,
      "loss": 0.0264,
      "step": 4884
    },
    {
      "epoch": 2.438841737393909,
      "grad_norm": 0.08573495596647263,
      "learning_rate": 0.0004756115826260609,
      "loss": 0.0017,
      "step": 4885
    },
    {
      "epoch": 2.439340988517224,
      "grad_norm": 0.2359778881072998,
      "learning_rate": 0.00047560659011482777,
      "loss": 0.0027,
      "step": 4886
    },
    {
      "epoch": 2.439840239640539,
      "grad_norm": 0.047073714435100555,
      "learning_rate": 0.0004756015976035946,
      "loss": 0.001,
      "step": 4887
    },
    {
      "epoch": 2.4403394907638543,
      "grad_norm": 0.12614908814430237,
      "learning_rate": 0.0004755966050923615,
      "loss": 0.0025,
      "step": 4888
    },
    {
      "epoch": 2.4408387418871693,
      "grad_norm": 0.09986478090286255,
      "learning_rate": 0.00047559161258112833,
      "loss": 0.0017,
      "step": 4889
    },
    {
      "epoch": 2.4413379930104844,
      "grad_norm": 0.37902989983558655,
      "learning_rate": 0.0004755866200698952,
      "loss": 0.0061,
      "step": 4890
    },
    {
      "epoch": 2.4418372441337994,
      "grad_norm": 0.3439054787158966,
      "learning_rate": 0.00047558162755866203,
      "loss": 0.0186,
      "step": 4891
    },
    {
      "epoch": 2.4423364952571145,
      "grad_norm": 0.14701026678085327,
      "learning_rate": 0.0004755766350474289,
      "loss": 0.0023,
      "step": 4892
    },
    {
      "epoch": 2.442835746380429,
      "grad_norm": 0.5932660102844238,
      "learning_rate": 0.00047557164253619574,
      "loss": 0.0067,
      "step": 4893
    },
    {
      "epoch": 2.4433349975037446,
      "grad_norm": 0.30702635645866394,
      "learning_rate": 0.0004755666500249626,
      "loss": 0.0046,
      "step": 4894
    },
    {
      "epoch": 2.4438342486270592,
      "grad_norm": 0.17080579698085785,
      "learning_rate": 0.00047556165751372944,
      "loss": 0.003,
      "step": 4895
    },
    {
      "epoch": 2.4443334997503743,
      "grad_norm": 0.2288007289171219,
      "learning_rate": 0.0004755566650024963,
      "loss": 0.0062,
      "step": 4896
    },
    {
      "epoch": 2.4448327508736893,
      "grad_norm": 0.7263511419296265,
      "learning_rate": 0.00047555167249126314,
      "loss": 0.0119,
      "step": 4897
    },
    {
      "epoch": 2.4453320019970044,
      "grad_norm": 0.8936514854431152,
      "learning_rate": 0.00047554667998003,
      "loss": 0.012,
      "step": 4898
    },
    {
      "epoch": 2.4458312531203195,
      "grad_norm": 0.024005120620131493,
      "learning_rate": 0.00047554168746879685,
      "loss": 0.0009,
      "step": 4899
    },
    {
      "epoch": 2.4463305042436345,
      "grad_norm": 0.01791391149163246,
      "learning_rate": 0.0004755366949575637,
      "loss": 0.0008,
      "step": 4900
    },
    {
      "epoch": 2.4468297553669496,
      "grad_norm": 0.8814741969108582,
      "learning_rate": 0.00047553170244633055,
      "loss": 0.0119,
      "step": 4901
    },
    {
      "epoch": 2.4473290064902646,
      "grad_norm": 0.05389866605401039,
      "learning_rate": 0.0004755267099350974,
      "loss": 0.0015,
      "step": 4902
    },
    {
      "epoch": 2.4478282576135797,
      "grad_norm": 0.028470957651734352,
      "learning_rate": 0.0004755217174238642,
      "loss": 0.0011,
      "step": 4903
    },
    {
      "epoch": 2.4483275087368948,
      "grad_norm": 0.4012485444545746,
      "learning_rate": 0.00047551672491263105,
      "loss": 0.0028,
      "step": 4904
    },
    {
      "epoch": 2.44882675986021,
      "grad_norm": 0.558673083782196,
      "learning_rate": 0.0004755117324013979,
      "loss": 0.0083,
      "step": 4905
    },
    {
      "epoch": 2.449326010983525,
      "grad_norm": 0.21462352573871613,
      "learning_rate": 0.00047550673989016476,
      "loss": 0.0043,
      "step": 4906
    },
    {
      "epoch": 2.44982526210684,
      "grad_norm": 0.2590784430503845,
      "learning_rate": 0.0004755017473789316,
      "loss": 0.003,
      "step": 4907
    },
    {
      "epoch": 2.4503245132301545,
      "grad_norm": 0.011419165879487991,
      "learning_rate": 0.00047549675486769846,
      "loss": 0.0008,
      "step": 4908
    },
    {
      "epoch": 2.4508237643534696,
      "grad_norm": 0.3935137391090393,
      "learning_rate": 0.0004754917623564653,
      "loss": 0.0101,
      "step": 4909
    },
    {
      "epoch": 2.4513230154767847,
      "grad_norm": 0.3170873820781708,
      "learning_rate": 0.00047548676984523217,
      "loss": 0.0035,
      "step": 4910
    },
    {
      "epoch": 2.4518222666000997,
      "grad_norm": 1.1109777688980103,
      "learning_rate": 0.000475481777333999,
      "loss": 0.0237,
      "step": 4911
    },
    {
      "epoch": 2.452321517723415,
      "grad_norm": 0.3452083468437195,
      "learning_rate": 0.00047547678482276587,
      "loss": 0.0055,
      "step": 4912
    },
    {
      "epoch": 2.45282076884673,
      "grad_norm": 0.2559130787849426,
      "learning_rate": 0.0004754717923115327,
      "loss": 0.0032,
      "step": 4913
    },
    {
      "epoch": 2.453320019970045,
      "grad_norm": 0.17342819273471832,
      "learning_rate": 0.0004754667998002996,
      "loss": 0.0019,
      "step": 4914
    },
    {
      "epoch": 2.45381927109336,
      "grad_norm": 0.3075212240219116,
      "learning_rate": 0.0004754618072890664,
      "loss": 0.0047,
      "step": 4915
    },
    {
      "epoch": 2.454318522216675,
      "grad_norm": 0.017152288928627968,
      "learning_rate": 0.0004754568147778333,
      "loss": 0.0009,
      "step": 4916
    },
    {
      "epoch": 2.45481777333999,
      "grad_norm": 0.1396620124578476,
      "learning_rate": 0.00047545182226660013,
      "loss": 0.0024,
      "step": 4917
    },
    {
      "epoch": 2.455317024463305,
      "grad_norm": 0.31459519267082214,
      "learning_rate": 0.000475446829755367,
      "loss": 0.0085,
      "step": 4918
    },
    {
      "epoch": 2.45581627558662,
      "grad_norm": 0.2860832214355469,
      "learning_rate": 0.00047544183724413384,
      "loss": 0.0028,
      "step": 4919
    },
    {
      "epoch": 2.4563155267099352,
      "grad_norm": 0.07341764122247696,
      "learning_rate": 0.0004754368447329007,
      "loss": 0.0012,
      "step": 4920
    },
    {
      "epoch": 2.4568147778332503,
      "grad_norm": 0.24423955380916595,
      "learning_rate": 0.00047543185222166754,
      "loss": 0.0022,
      "step": 4921
    },
    {
      "epoch": 2.4573140289565654,
      "grad_norm": 0.767065703868866,
      "learning_rate": 0.0004754268597104344,
      "loss": 0.0377,
      "step": 4922
    },
    {
      "epoch": 2.45781328007988,
      "grad_norm": 0.05713838338851929,
      "learning_rate": 0.00047542186719920124,
      "loss": 0.0013,
      "step": 4923
    },
    {
      "epoch": 2.458312531203195,
      "grad_norm": 0.09263351559638977,
      "learning_rate": 0.0004754168746879681,
      "loss": 0.0012,
      "step": 4924
    },
    {
      "epoch": 2.45881178232651,
      "grad_norm": 0.40401792526245117,
      "learning_rate": 0.00047541188217673495,
      "loss": 0.0124,
      "step": 4925
    },
    {
      "epoch": 2.459311033449825,
      "grad_norm": 0.1295691877603531,
      "learning_rate": 0.0004754068896655018,
      "loss": 0.0024,
      "step": 4926
    },
    {
      "epoch": 2.45981028457314,
      "grad_norm": 0.044953279197216034,
      "learning_rate": 0.00047540189715426865,
      "loss": 0.001,
      "step": 4927
    },
    {
      "epoch": 2.4603095356964553,
      "grad_norm": 0.7636155486106873,
      "learning_rate": 0.0004753969046430355,
      "loss": 0.007,
      "step": 4928
    },
    {
      "epoch": 2.4608087868197703,
      "grad_norm": 0.09925597906112671,
      "learning_rate": 0.00047539191213180236,
      "loss": 0.0022,
      "step": 4929
    },
    {
      "epoch": 2.4613080379430854,
      "grad_norm": 0.04925192520022392,
      "learning_rate": 0.00047538691962056915,
      "loss": 0.0013,
      "step": 4930
    },
    {
      "epoch": 2.4618072890664005,
      "grad_norm": 0.1370561122894287,
      "learning_rate": 0.000475381927109336,
      "loss": 0.0019,
      "step": 4931
    },
    {
      "epoch": 2.4623065401897155,
      "grad_norm": 0.14516688883304596,
      "learning_rate": 0.0004753769345981028,
      "loss": 0.0019,
      "step": 4932
    },
    {
      "epoch": 2.4628057913130306,
      "grad_norm": 0.27371346950531006,
      "learning_rate": 0.00047537194208686966,
      "loss": 0.0048,
      "step": 4933
    },
    {
      "epoch": 2.4633050424363456,
      "grad_norm": 0.28638315200805664,
      "learning_rate": 0.0004753669495756365,
      "loss": 0.0049,
      "step": 4934
    },
    {
      "epoch": 2.4638042935596607,
      "grad_norm": 0.03142077848315239,
      "learning_rate": 0.00047536195706440336,
      "loss": 0.0012,
      "step": 4935
    },
    {
      "epoch": 2.4643035446829753,
      "grad_norm": 0.37816306948661804,
      "learning_rate": 0.0004753569645531702,
      "loss": 0.0131,
      "step": 4936
    },
    {
      "epoch": 2.464802795806291,
      "grad_norm": 0.3822307884693146,
      "learning_rate": 0.00047535197204193706,
      "loss": 0.0064,
      "step": 4937
    },
    {
      "epoch": 2.4653020469296054,
      "grad_norm": 0.412913978099823,
      "learning_rate": 0.0004753469795307039,
      "loss": 0.0207,
      "step": 4938
    },
    {
      "epoch": 2.4658012980529205,
      "grad_norm": 0.20465126633644104,
      "learning_rate": 0.00047534198701947077,
      "loss": 0.0027,
      "step": 4939
    },
    {
      "epoch": 2.4663005491762355,
      "grad_norm": 0.15430693328380585,
      "learning_rate": 0.0004753369945082376,
      "loss": 0.002,
      "step": 4940
    },
    {
      "epoch": 2.4667998002995506,
      "grad_norm": 0.5743799209594727,
      "learning_rate": 0.00047533200199700447,
      "loss": 0.0157,
      "step": 4941
    },
    {
      "epoch": 2.4672990514228657,
      "grad_norm": 0.45572882890701294,
      "learning_rate": 0.0004753270094857713,
      "loss": 0.0075,
      "step": 4942
    },
    {
      "epoch": 2.4677983025461807,
      "grad_norm": 0.14978842437267303,
      "learning_rate": 0.0004753220169745382,
      "loss": 0.0027,
      "step": 4943
    },
    {
      "epoch": 2.4682975536694958,
      "grad_norm": 0.13624444603919983,
      "learning_rate": 0.00047531702446330503,
      "loss": 0.0022,
      "step": 4944
    },
    {
      "epoch": 2.468796804792811,
      "grad_norm": 0.04839325696229935,
      "learning_rate": 0.0004753120319520719,
      "loss": 0.0012,
      "step": 4945
    },
    {
      "epoch": 2.469296055916126,
      "grad_norm": 0.022506559267640114,
      "learning_rate": 0.00047530703944083873,
      "loss": 0.0007,
      "step": 4946
    },
    {
      "epoch": 2.469795307039441,
      "grad_norm": 0.258466511964798,
      "learning_rate": 0.0004753020469296056,
      "loss": 0.0029,
      "step": 4947
    },
    {
      "epoch": 2.470294558162756,
      "grad_norm": 0.19697867333889008,
      "learning_rate": 0.00047529705441837244,
      "loss": 0.0037,
      "step": 4948
    },
    {
      "epoch": 2.470793809286071,
      "grad_norm": 0.4330991804599762,
      "learning_rate": 0.0004752920619071393,
      "loss": 0.0083,
      "step": 4949
    },
    {
      "epoch": 2.471293060409386,
      "grad_norm": 0.06874372065067291,
      "learning_rate": 0.00047528706939590614,
      "loss": 0.0014,
      "step": 4950
    },
    {
      "epoch": 2.4717923115327007,
      "grad_norm": 0.2971805930137634,
      "learning_rate": 0.000475282076884673,
      "loss": 0.0155,
      "step": 4951
    },
    {
      "epoch": 2.472291562656016,
      "grad_norm": 0.03164837136864662,
      "learning_rate": 0.00047527708437343984,
      "loss": 0.0008,
      "step": 4952
    },
    {
      "epoch": 2.472790813779331,
      "grad_norm": 0.06657911837100983,
      "learning_rate": 0.0004752720918622067,
      "loss": 0.0016,
      "step": 4953
    },
    {
      "epoch": 2.473290064902646,
      "grad_norm": 0.06574311852455139,
      "learning_rate": 0.00047526709935097355,
      "loss": 0.0015,
      "step": 4954
    },
    {
      "epoch": 2.473789316025961,
      "grad_norm": 0.008704892359673977,
      "learning_rate": 0.0004752621068397404,
      "loss": 0.0004,
      "step": 4955
    },
    {
      "epoch": 2.474288567149276,
      "grad_norm": 0.01052617747336626,
      "learning_rate": 0.00047525711432850725,
      "loss": 0.0005,
      "step": 4956
    },
    {
      "epoch": 2.474787818272591,
      "grad_norm": 0.2015426605939865,
      "learning_rate": 0.0004752521218172741,
      "loss": 0.003,
      "step": 4957
    },
    {
      "epoch": 2.475287069395906,
      "grad_norm": 0.14426074922084808,
      "learning_rate": 0.00047524712930604096,
      "loss": 0.0013,
      "step": 4958
    },
    {
      "epoch": 2.475786320519221,
      "grad_norm": 0.02361447922885418,
      "learning_rate": 0.0004752421367948078,
      "loss": 0.0006,
      "step": 4959
    },
    {
      "epoch": 2.4762855716425363,
      "grad_norm": 0.46934112906455994,
      "learning_rate": 0.00047523714428357466,
      "loss": 0.0053,
      "step": 4960
    },
    {
      "epoch": 2.4767848227658513,
      "grad_norm": 0.01258875336498022,
      "learning_rate": 0.00047523215177234146,
      "loss": 0.0006,
      "step": 4961
    },
    {
      "epoch": 2.4772840738891664,
      "grad_norm": 0.009074604138731956,
      "learning_rate": 0.0004752271592611083,
      "loss": 0.0005,
      "step": 4962
    },
    {
      "epoch": 2.4777833250124814,
      "grad_norm": 0.1674422323703766,
      "learning_rate": 0.00047522216674987516,
      "loss": 0.0012,
      "step": 4963
    },
    {
      "epoch": 2.478282576135796,
      "grad_norm": 0.017297936603426933,
      "learning_rate": 0.000475217174238642,
      "loss": 0.0006,
      "step": 4964
    },
    {
      "epoch": 2.4787818272591116,
      "grad_norm": 0.013164808042347431,
      "learning_rate": 0.00047521218172740887,
      "loss": 0.0006,
      "step": 4965
    },
    {
      "epoch": 2.479281078382426,
      "grad_norm": 0.012703869491815567,
      "learning_rate": 0.0004752071892161757,
      "loss": 0.0006,
      "step": 4966
    },
    {
      "epoch": 2.4797803295057412,
      "grad_norm": 0.019371002912521362,
      "learning_rate": 0.00047520219670494257,
      "loss": 0.0006,
      "step": 4967
    },
    {
      "epoch": 2.4802795806290563,
      "grad_norm": 0.006825340446084738,
      "learning_rate": 0.0004751972041937094,
      "loss": 0.0004,
      "step": 4968
    },
    {
      "epoch": 2.4807788317523713,
      "grad_norm": 0.01059012208133936,
      "learning_rate": 0.0004751922116824763,
      "loss": 0.0006,
      "step": 4969
    },
    {
      "epoch": 2.4812780828756864,
      "grad_norm": 0.005549249704927206,
      "learning_rate": 0.00047518721917124313,
      "loss": 0.0004,
      "step": 4970
    },
    {
      "epoch": 2.4817773339990015,
      "grad_norm": 0.01645035110414028,
      "learning_rate": 0.00047518222666001,
      "loss": 0.0006,
      "step": 4971
    },
    {
      "epoch": 2.4822765851223165,
      "grad_norm": 0.04103155434131622,
      "learning_rate": 0.00047517723414877683,
      "loss": 0.001,
      "step": 4972
    },
    {
      "epoch": 2.4827758362456316,
      "grad_norm": 0.0045794108882546425,
      "learning_rate": 0.0004751722416375437,
      "loss": 0.0004,
      "step": 4973
    },
    {
      "epoch": 2.4832750873689466,
      "grad_norm": 0.3776090443134308,
      "learning_rate": 0.00047516724912631054,
      "loss": 0.0052,
      "step": 4974
    },
    {
      "epoch": 2.4837743384922617,
      "grad_norm": 0.05791866034269333,
      "learning_rate": 0.0004751622566150774,
      "loss": 0.0011,
      "step": 4975
    },
    {
      "epoch": 2.4842735896155768,
      "grad_norm": 0.31803154945373535,
      "learning_rate": 0.00047515726410384424,
      "loss": 0.0066,
      "step": 4976
    },
    {
      "epoch": 2.484772840738892,
      "grad_norm": 0.019549494609236717,
      "learning_rate": 0.0004751522715926111,
      "loss": 0.0006,
      "step": 4977
    },
    {
      "epoch": 2.485272091862207,
      "grad_norm": 0.08003504574298859,
      "learning_rate": 0.00047514727908137794,
      "loss": 0.0013,
      "step": 4978
    },
    {
      "epoch": 2.4857713429855215,
      "grad_norm": 0.14136236906051636,
      "learning_rate": 0.0004751422865701448,
      "loss": 0.0015,
      "step": 4979
    },
    {
      "epoch": 2.4862705941088366,
      "grad_norm": 0.255810409784317,
      "learning_rate": 0.00047513729405891165,
      "loss": 0.003,
      "step": 4980
    },
    {
      "epoch": 2.4867698452321516,
      "grad_norm": 0.016097161918878555,
      "learning_rate": 0.0004751323015476785,
      "loss": 0.0005,
      "step": 4981
    },
    {
      "epoch": 2.4872690963554667,
      "grad_norm": 0.03199298307299614,
      "learning_rate": 0.00047512730903644535,
      "loss": 0.0008,
      "step": 4982
    },
    {
      "epoch": 2.4877683474787817,
      "grad_norm": 0.08476436883211136,
      "learning_rate": 0.0004751223165252122,
      "loss": 0.0013,
      "step": 4983
    },
    {
      "epoch": 2.488267598602097,
      "grad_norm": 0.4181603789329529,
      "learning_rate": 0.00047511732401397906,
      "loss": 0.0175,
      "step": 4984
    },
    {
      "epoch": 2.488766849725412,
      "grad_norm": 0.0848315954208374,
      "learning_rate": 0.0004751123315027459,
      "loss": 0.001,
      "step": 4985
    },
    {
      "epoch": 2.489266100848727,
      "grad_norm": 0.009537841193377972,
      "learning_rate": 0.00047510733899151276,
      "loss": 0.0004,
      "step": 4986
    },
    {
      "epoch": 2.489765351972042,
      "grad_norm": 0.08579523116350174,
      "learning_rate": 0.0004751023464802796,
      "loss": 0.0008,
      "step": 4987
    },
    {
      "epoch": 2.490264603095357,
      "grad_norm": 0.22458428144454956,
      "learning_rate": 0.00047509735396904647,
      "loss": 0.0025,
      "step": 4988
    },
    {
      "epoch": 2.490763854218672,
      "grad_norm": 0.46233150362968445,
      "learning_rate": 0.0004750923614578133,
      "loss": 0.004,
      "step": 4989
    },
    {
      "epoch": 2.491263105341987,
      "grad_norm": 0.3824234902858734,
      "learning_rate": 0.00047508736894658017,
      "loss": 0.0262,
      "step": 4990
    },
    {
      "epoch": 2.491762356465302,
      "grad_norm": 0.0577969029545784,
      "learning_rate": 0.00047508237643534697,
      "loss": 0.0008,
      "step": 4991
    },
    {
      "epoch": 2.4922616075886173,
      "grad_norm": 0.035564061254262924,
      "learning_rate": 0.0004750773839241138,
      "loss": 0.0005,
      "step": 4992
    },
    {
      "epoch": 2.4927608587119323,
      "grad_norm": 0.01734890788793564,
      "learning_rate": 0.00047507239141288067,
      "loss": 0.0007,
      "step": 4993
    },
    {
      "epoch": 2.493260109835247,
      "grad_norm": 0.03874138742685318,
      "learning_rate": 0.0004750673989016475,
      "loss": 0.0009,
      "step": 4994
    },
    {
      "epoch": 2.493759360958562,
      "grad_norm": 0.03919551521539688,
      "learning_rate": 0.0004750624063904144,
      "loss": 0.0007,
      "step": 4995
    },
    {
      "epoch": 2.494258612081877,
      "grad_norm": 0.011768979951739311,
      "learning_rate": 0.00047505741387918123,
      "loss": 0.0006,
      "step": 4996
    },
    {
      "epoch": 2.494757863205192,
      "grad_norm": 0.019425714388489723,
      "learning_rate": 0.0004750524213679481,
      "loss": 0.0007,
      "step": 4997
    },
    {
      "epoch": 2.495257114328507,
      "grad_norm": 0.30881935358047485,
      "learning_rate": 0.00047504742885671493,
      "loss": 0.033,
      "step": 4998
    },
    {
      "epoch": 2.4957563654518222,
      "grad_norm": 0.17074650526046753,
      "learning_rate": 0.0004750424363454818,
      "loss": 0.0011,
      "step": 4999
    },
    {
      "epoch": 2.4962556165751373,
      "grad_norm": 0.23987750709056854,
      "learning_rate": 0.00047503744383424864,
      "loss": 0.0049,
      "step": 5000
    },
    {
      "epoch": 2.4967548676984523,
      "grad_norm": 0.0029848902486264706,
      "learning_rate": 0.0004750324513230155,
      "loss": 0.0003,
      "step": 5001
    },
    {
      "epoch": 2.4972541188217674,
      "grad_norm": 0.08752673864364624,
      "learning_rate": 0.00047502745881178234,
      "loss": 0.0015,
      "step": 5002
    },
    {
      "epoch": 2.4977533699450825,
      "grad_norm": 0.034040458500385284,
      "learning_rate": 0.0004750224663005492,
      "loss": 0.0009,
      "step": 5003
    },
    {
      "epoch": 2.4982526210683975,
      "grad_norm": 0.009893061593174934,
      "learning_rate": 0.00047501747378931604,
      "loss": 0.0004,
      "step": 5004
    },
    {
      "epoch": 2.4987518721917126,
      "grad_norm": 0.01602851040661335,
      "learning_rate": 0.0004750124812780829,
      "loss": 0.0004,
      "step": 5005
    },
    {
      "epoch": 2.4992511233150276,
      "grad_norm": 0.014154759235680103,
      "learning_rate": 0.00047500748876684975,
      "loss": 0.0005,
      "step": 5006
    },
    {
      "epoch": 2.4997503744383422,
      "grad_norm": 0.016555538401007652,
      "learning_rate": 0.0004750024962556166,
      "loss": 0.0005,
      "step": 5007
    },
    {
      "epoch": 2.5002496255616578,
      "grad_norm": 0.006085139233618975,
      "learning_rate": 0.00047499750374438345,
      "loss": 0.0003,
      "step": 5008
    },
    {
      "epoch": 2.5007488766849724,
      "grad_norm": 0.1603206843137741,
      "learning_rate": 0.0004749925112331503,
      "loss": 0.0045,
      "step": 5009
    },
    {
      "epoch": 2.5012481278082874,
      "grad_norm": 0.0283406600356102,
      "learning_rate": 0.00047498751872191716,
      "loss": 0.0006,
      "step": 5010
    },
    {
      "epoch": 2.5017473789316025,
      "grad_norm": 0.08054051548242569,
      "learning_rate": 0.000474982526210684,
      "loss": 0.0011,
      "step": 5011
    },
    {
      "epoch": 2.5022466300549175,
      "grad_norm": 0.01265979465097189,
      "learning_rate": 0.00047497753369945086,
      "loss": 0.0005,
      "step": 5012
    },
    {
      "epoch": 2.5027458811782326,
      "grad_norm": 0.03107050061225891,
      "learning_rate": 0.0004749725411882177,
      "loss": 0.0005,
      "step": 5013
    },
    {
      "epoch": 2.5032451323015477,
      "grad_norm": 0.6072568297386169,
      "learning_rate": 0.00047496754867698456,
      "loss": 0.0159,
      "step": 5014
    },
    {
      "epoch": 2.5037443834248627,
      "grad_norm": 0.3439640402793884,
      "learning_rate": 0.0004749625561657514,
      "loss": 0.0022,
      "step": 5015
    },
    {
      "epoch": 2.5042436345481778,
      "grad_norm": 0.31160542368888855,
      "learning_rate": 0.00047495756365451827,
      "loss": 0.015,
      "step": 5016
    },
    {
      "epoch": 2.504742885671493,
      "grad_norm": 0.13599497079849243,
      "learning_rate": 0.0004749525711432851,
      "loss": 0.0012,
      "step": 5017
    },
    {
      "epoch": 2.505242136794808,
      "grad_norm": 0.6188063621520996,
      "learning_rate": 0.00047494757863205197,
      "loss": 0.0046,
      "step": 5018
    },
    {
      "epoch": 2.505741387918123,
      "grad_norm": 0.008497779257595539,
      "learning_rate": 0.0004749425861208188,
      "loss": 0.0004,
      "step": 5019
    },
    {
      "epoch": 2.5062406390414376,
      "grad_norm": 0.04712620750069618,
      "learning_rate": 0.0004749375936095856,
      "loss": 0.0008,
      "step": 5020
    },
    {
      "epoch": 2.506739890164753,
      "grad_norm": 0.09694437682628632,
      "learning_rate": 0.0004749326010983525,
      "loss": 0.0015,
      "step": 5021
    },
    {
      "epoch": 2.5072391412880677,
      "grad_norm": 0.034060291945934296,
      "learning_rate": 0.0004749276085871193,
      "loss": 0.0006,
      "step": 5022
    },
    {
      "epoch": 2.507738392411383,
      "grad_norm": 0.008848728612065315,
      "learning_rate": 0.0004749226160758862,
      "loss": 0.0004,
      "step": 5023
    },
    {
      "epoch": 2.508237643534698,
      "grad_norm": 0.31167206168174744,
      "learning_rate": 0.00047491762356465303,
      "loss": 0.0026,
      "step": 5024
    },
    {
      "epoch": 2.508736894658013,
      "grad_norm": 0.013563313521444798,
      "learning_rate": 0.0004749126310534199,
      "loss": 0.0005,
      "step": 5025
    },
    {
      "epoch": 2.509236145781328,
      "grad_norm": 0.3036462068557739,
      "learning_rate": 0.00047490763854218674,
      "loss": 0.0086,
      "step": 5026
    },
    {
      "epoch": 2.509735396904643,
      "grad_norm": 0.4124195873737335,
      "learning_rate": 0.0004749026460309536,
      "loss": 0.0235,
      "step": 5027
    },
    {
      "epoch": 2.510234648027958,
      "grad_norm": 0.8240177631378174,
      "learning_rate": 0.00047489765351972044,
      "loss": 0.0335,
      "step": 5028
    },
    {
      "epoch": 2.510733899151273,
      "grad_norm": 0.011108724400401115,
      "learning_rate": 0.0004748926610084873,
      "loss": 0.0004,
      "step": 5029
    },
    {
      "epoch": 2.511233150274588,
      "grad_norm": 0.4742071330547333,
      "learning_rate": 0.00047488766849725414,
      "loss": 0.0207,
      "step": 5030
    },
    {
      "epoch": 2.511732401397903,
      "grad_norm": 0.024875912815332413,
      "learning_rate": 0.000474882675986021,
      "loss": 0.0007,
      "step": 5031
    },
    {
      "epoch": 2.5122316525212183,
      "grad_norm": 0.0372963547706604,
      "learning_rate": 0.00047487768347478785,
      "loss": 0.0007,
      "step": 5032
    },
    {
      "epoch": 2.5127309036445333,
      "grad_norm": 0.624610424041748,
      "learning_rate": 0.0004748726909635547,
      "loss": 0.0033,
      "step": 5033
    },
    {
      "epoch": 2.5132301547678484,
      "grad_norm": 0.02928178943693638,
      "learning_rate": 0.00047486769845232155,
      "loss": 0.0006,
      "step": 5034
    },
    {
      "epoch": 2.513729405891163,
      "grad_norm": 0.08415760844945908,
      "learning_rate": 0.0004748627059410884,
      "loss": 0.0013,
      "step": 5035
    },
    {
      "epoch": 2.5142286570144785,
      "grad_norm": 0.22679173946380615,
      "learning_rate": 0.00047485771342985526,
      "loss": 0.0051,
      "step": 5036
    },
    {
      "epoch": 2.514727908137793,
      "grad_norm": 0.20904229581356049,
      "learning_rate": 0.0004748527209186221,
      "loss": 0.0048,
      "step": 5037
    },
    {
      "epoch": 2.515227159261108,
      "grad_norm": 0.032095544040203094,
      "learning_rate": 0.00047484772840738896,
      "loss": 0.0009,
      "step": 5038
    },
    {
      "epoch": 2.5157264103844232,
      "grad_norm": 0.0335482656955719,
      "learning_rate": 0.0004748427358961558,
      "loss": 0.001,
      "step": 5039
    },
    {
      "epoch": 2.5162256615077383,
      "grad_norm": 0.04243107885122299,
      "learning_rate": 0.00047483774338492266,
      "loss": 0.0012,
      "step": 5040
    },
    {
      "epoch": 2.5167249126310534,
      "grad_norm": 0.2906879484653473,
      "learning_rate": 0.0004748327508736895,
      "loss": 0.0036,
      "step": 5041
    },
    {
      "epoch": 2.5172241637543684,
      "grad_norm": 0.2668294608592987,
      "learning_rate": 0.00047482775836245637,
      "loss": 0.0037,
      "step": 5042
    },
    {
      "epoch": 2.5177234148776835,
      "grad_norm": 0.11518845707178116,
      "learning_rate": 0.00047482276585122317,
      "loss": 0.0019,
      "step": 5043
    },
    {
      "epoch": 2.5182226660009985,
      "grad_norm": 0.31322920322418213,
      "learning_rate": 0.00047481777333999,
      "loss": 0.0037,
      "step": 5044
    },
    {
      "epoch": 2.5187219171243136,
      "grad_norm": 0.02915787324309349,
      "learning_rate": 0.00047481278082875687,
      "loss": 0.0008,
      "step": 5045
    },
    {
      "epoch": 2.5192211682476287,
      "grad_norm": 0.027370670810341835,
      "learning_rate": 0.0004748077883175237,
      "loss": 0.0007,
      "step": 5046
    },
    {
      "epoch": 2.5197204193709437,
      "grad_norm": 0.3198665380477905,
      "learning_rate": 0.0004748027958062906,
      "loss": 0.0008,
      "step": 5047
    },
    {
      "epoch": 2.5202196704942588,
      "grad_norm": 0.06754238158464432,
      "learning_rate": 0.0004747978032950574,
      "loss": 0.0011,
      "step": 5048
    },
    {
      "epoch": 2.520718921617574,
      "grad_norm": 0.17023536562919617,
      "learning_rate": 0.0004747928107838242,
      "loss": 0.0013,
      "step": 5049
    },
    {
      "epoch": 2.5212181727408884,
      "grad_norm": 0.012951472774147987,
      "learning_rate": 0.0004747878182725911,
      "loss": 0.0005,
      "step": 5050
    },
    {
      "epoch": 2.521717423864204,
      "grad_norm": 0.3198189437389374,
      "learning_rate": 0.00047478282576135793,
      "loss": 0.0023,
      "step": 5051
    },
    {
      "epoch": 2.5222166749875186,
      "grad_norm": 0.004358384758234024,
      "learning_rate": 0.0004747778332501248,
      "loss": 0.0004,
      "step": 5052
    },
    {
      "epoch": 2.5227159261108336,
      "grad_norm": 0.05757869780063629,
      "learning_rate": 0.00047477284073889163,
      "loss": 0.0009,
      "step": 5053
    },
    {
      "epoch": 2.5232151772341487,
      "grad_norm": 0.025324542075395584,
      "learning_rate": 0.0004747678482276585,
      "loss": 0.0006,
      "step": 5054
    },
    {
      "epoch": 2.5237144283574637,
      "grad_norm": 0.014058973640203476,
      "learning_rate": 0.00047476285571642534,
      "loss": 0.0004,
      "step": 5055
    },
    {
      "epoch": 2.524213679480779,
      "grad_norm": 0.3262835144996643,
      "learning_rate": 0.0004747578632051922,
      "loss": 0.0094,
      "step": 5056
    },
    {
      "epoch": 2.524712930604094,
      "grad_norm": 0.07638406753540039,
      "learning_rate": 0.00047475287069395904,
      "loss": 0.0011,
      "step": 5057
    },
    {
      "epoch": 2.525212181727409,
      "grad_norm": 0.02682419866323471,
      "learning_rate": 0.0004747478781827259,
      "loss": 0.0006,
      "step": 5058
    },
    {
      "epoch": 2.525711432850724,
      "grad_norm": 0.023286618292331696,
      "learning_rate": 0.00047474288567149274,
      "loss": 0.0005,
      "step": 5059
    },
    {
      "epoch": 2.526210683974039,
      "grad_norm": 0.0062453229911625385,
      "learning_rate": 0.0004747378931602596,
      "loss": 0.0004,
      "step": 5060
    },
    {
      "epoch": 2.526709935097354,
      "grad_norm": 0.0043640658259391785,
      "learning_rate": 0.00047473290064902645,
      "loss": 0.0004,
      "step": 5061
    },
    {
      "epoch": 2.527209186220669,
      "grad_norm": 0.12150577455759048,
      "learning_rate": 0.0004747279081377933,
      "loss": 0.0008,
      "step": 5062
    },
    {
      "epoch": 2.5277084373439838,
      "grad_norm": 0.009709097445011139,
      "learning_rate": 0.00047472291562656015,
      "loss": 0.0006,
      "step": 5063
    },
    {
      "epoch": 2.5282076884672993,
      "grad_norm": 0.019969280809164047,
      "learning_rate": 0.000474717923115327,
      "loss": 0.0007,
      "step": 5064
    },
    {
      "epoch": 2.528706939590614,
      "grad_norm": 0.5180334448814392,
      "learning_rate": 0.00047471293060409386,
      "loss": 0.0027,
      "step": 5065
    },
    {
      "epoch": 2.5292061907139294,
      "grad_norm": 0.05917094275355339,
      "learning_rate": 0.0004747079380928607,
      "loss": 0.0008,
      "step": 5066
    },
    {
      "epoch": 2.529705441837244,
      "grad_norm": 0.040920060127973557,
      "learning_rate": 0.00047470294558162756,
      "loss": 0.0008,
      "step": 5067
    },
    {
      "epoch": 2.530204692960559,
      "grad_norm": 0.011111226864159107,
      "learning_rate": 0.0004746979530703944,
      "loss": 0.0005,
      "step": 5068
    },
    {
      "epoch": 2.530703944083874,
      "grad_norm": 0.40512117743492126,
      "learning_rate": 0.00047469296055916127,
      "loss": 0.0102,
      "step": 5069
    },
    {
      "epoch": 2.531203195207189,
      "grad_norm": 0.11216837167739868,
      "learning_rate": 0.0004746879680479281,
      "loss": 0.0017,
      "step": 5070
    },
    {
      "epoch": 2.5317024463305042,
      "grad_norm": 0.02168854884803295,
      "learning_rate": 0.00047468297553669497,
      "loss": 0.0006,
      "step": 5071
    },
    {
      "epoch": 2.5322016974538193,
      "grad_norm": 0.03834700584411621,
      "learning_rate": 0.0004746779830254618,
      "loss": 0.0005,
      "step": 5072
    },
    {
      "epoch": 2.5327009485771343,
      "grad_norm": 0.013270321302115917,
      "learning_rate": 0.0004746729905142287,
      "loss": 0.0004,
      "step": 5073
    },
    {
      "epoch": 2.5332001997004494,
      "grad_norm": 0.04594946652650833,
      "learning_rate": 0.0004746679980029955,
      "loss": 0.0009,
      "step": 5074
    },
    {
      "epoch": 2.5336994508237645,
      "grad_norm": 0.03869068995118141,
      "learning_rate": 0.0004746630054917624,
      "loss": 0.0006,
      "step": 5075
    },
    {
      "epoch": 2.5341987019470795,
      "grad_norm": 0.0200029443949461,
      "learning_rate": 0.00047465801298052923,
      "loss": 0.0007,
      "step": 5076
    },
    {
      "epoch": 2.5346979530703946,
      "grad_norm": 0.050009120255708694,
      "learning_rate": 0.0004746530204692961,
      "loss": 0.0012,
      "step": 5077
    },
    {
      "epoch": 2.535197204193709,
      "grad_norm": 0.017566321417689323,
      "learning_rate": 0.0004746480279580629,
      "loss": 0.0005,
      "step": 5078
    },
    {
      "epoch": 2.5356964553170247,
      "grad_norm": 2.7751996517181396,
      "learning_rate": 0.00047464303544682973,
      "loss": 0.0138,
      "step": 5079
    },
    {
      "epoch": 2.5361957064403393,
      "grad_norm": 0.038624003529548645,
      "learning_rate": 0.0004746380429355966,
      "loss": 0.0008,
      "step": 5080
    },
    {
      "epoch": 2.5366949575636544,
      "grad_norm": 0.016205621883273125,
      "learning_rate": 0.00047463305042436344,
      "loss": 0.0004,
      "step": 5081
    },
    {
      "epoch": 2.5371942086869694,
      "grad_norm": 0.01987878419458866,
      "learning_rate": 0.0004746280579131303,
      "loss": 0.0006,
      "step": 5082
    },
    {
      "epoch": 2.5376934598102845,
      "grad_norm": 0.0025361129082739353,
      "learning_rate": 0.00047462306540189714,
      "loss": 0.0003,
      "step": 5083
    },
    {
      "epoch": 2.5381927109335995,
      "grad_norm": 0.012231312692165375,
      "learning_rate": 0.000474618072890664,
      "loss": 0.0004,
      "step": 5084
    },
    {
      "epoch": 2.5386919620569146,
      "grad_norm": 0.22851943969726562,
      "learning_rate": 0.00047461308037943084,
      "loss": 0.0029,
      "step": 5085
    },
    {
      "epoch": 2.5391912131802297,
      "grad_norm": 0.0054726810194551945,
      "learning_rate": 0.0004746080878681977,
      "loss": 0.0003,
      "step": 5086
    },
    {
      "epoch": 2.5396904643035447,
      "grad_norm": 0.011978721246123314,
      "learning_rate": 0.00047460309535696455,
      "loss": 0.0004,
      "step": 5087
    },
    {
      "epoch": 2.54018971542686,
      "grad_norm": 0.0198769923299551,
      "learning_rate": 0.0004745981028457314,
      "loss": 0.0005,
      "step": 5088
    },
    {
      "epoch": 2.540688966550175,
      "grad_norm": 0.375601589679718,
      "learning_rate": 0.00047459311033449825,
      "loss": 0.0025,
      "step": 5089
    },
    {
      "epoch": 2.54118821767349,
      "grad_norm": 0.018527762964367867,
      "learning_rate": 0.0004745881178232651,
      "loss": 0.0004,
      "step": 5090
    },
    {
      "epoch": 2.5416874687968045,
      "grad_norm": 0.059341300278902054,
      "learning_rate": 0.00047458312531203196,
      "loss": 0.0005,
      "step": 5091
    },
    {
      "epoch": 2.54218671992012,
      "grad_norm": 0.07241582125425339,
      "learning_rate": 0.0004745781328007988,
      "loss": 0.001,
      "step": 5092
    },
    {
      "epoch": 2.5426859710434346,
      "grad_norm": 0.8314339518547058,
      "learning_rate": 0.00047457314028956566,
      "loss": 0.001,
      "step": 5093
    },
    {
      "epoch": 2.54318522216675,
      "grad_norm": 1.1696895360946655,
      "learning_rate": 0.0004745681477783325,
      "loss": 0.0193,
      "step": 5094
    },
    {
      "epoch": 2.5436844732900648,
      "grad_norm": 0.6295567750930786,
      "learning_rate": 0.00047456315526709937,
      "loss": 0.0097,
      "step": 5095
    },
    {
      "epoch": 2.54418372441338,
      "grad_norm": 1.7529462575912476,
      "learning_rate": 0.0004745581627558662,
      "loss": 0.0032,
      "step": 5096
    },
    {
      "epoch": 2.544682975536695,
      "grad_norm": 0.031165853142738342,
      "learning_rate": 0.00047455317024463307,
      "loss": 0.0004,
      "step": 5097
    },
    {
      "epoch": 2.54518222666001,
      "grad_norm": 0.01162799634039402,
      "learning_rate": 0.0004745481777333999,
      "loss": 0.0004,
      "step": 5098
    },
    {
      "epoch": 2.545681477783325,
      "grad_norm": 0.1761874258518219,
      "learning_rate": 0.0004745431852221668,
      "loss": 0.0016,
      "step": 5099
    },
    {
      "epoch": 2.54618072890664,
      "grad_norm": 2.57012939453125,
      "learning_rate": 0.0004745381927109336,
      "loss": 0.0205,
      "step": 5100
    },
    {
      "epoch": 2.546679980029955,
      "grad_norm": 0.1422455757856369,
      "learning_rate": 0.0004745332001997005,
      "loss": 0.0014,
      "step": 5101
    },
    {
      "epoch": 2.54717923115327,
      "grad_norm": 0.8753781914710999,
      "learning_rate": 0.00047452820768846733,
      "loss": 0.0234,
      "step": 5102
    },
    {
      "epoch": 2.547678482276585,
      "grad_norm": 0.17156094312667847,
      "learning_rate": 0.0004745232151772342,
      "loss": 0.0016,
      "step": 5103
    },
    {
      "epoch": 2.5481777333999003,
      "grad_norm": 3.736004114151001,
      "learning_rate": 0.00047451822266600103,
      "loss": 0.0602,
      "step": 5104
    },
    {
      "epoch": 2.5486769845232153,
      "grad_norm": 0.573190450668335,
      "learning_rate": 0.0004745132301547679,
      "loss": 0.0257,
      "step": 5105
    },
    {
      "epoch": 2.54917623564653,
      "grad_norm": 0.4755263924598694,
      "learning_rate": 0.00047450823764353474,
      "loss": 0.0035,
      "step": 5106
    },
    {
      "epoch": 2.5496754867698455,
      "grad_norm": 0.08428598940372467,
      "learning_rate": 0.00047450324513230154,
      "loss": 0.0016,
      "step": 5107
    },
    {
      "epoch": 2.55017473789316,
      "grad_norm": 0.15876245498657227,
      "learning_rate": 0.0004744982526210684,
      "loss": 0.0017,
      "step": 5108
    },
    {
      "epoch": 2.550673989016475,
      "grad_norm": 0.030707478523254395,
      "learning_rate": 0.00047449326010983524,
      "loss": 0.0008,
      "step": 5109
    },
    {
      "epoch": 2.55117324013979,
      "grad_norm": 1.935616135597229,
      "learning_rate": 0.0004744882675986021,
      "loss": 0.0061,
      "step": 5110
    },
    {
      "epoch": 2.5516724912631052,
      "grad_norm": 0.26045864820480347,
      "learning_rate": 0.00047448327508736894,
      "loss": 0.0028,
      "step": 5111
    },
    {
      "epoch": 2.5521717423864203,
      "grad_norm": 0.6135554909706116,
      "learning_rate": 0.0004744782825761358,
      "loss": 0.0397,
      "step": 5112
    },
    {
      "epoch": 2.5526709935097354,
      "grad_norm": 0.34858405590057373,
      "learning_rate": 0.00047447329006490265,
      "loss": 0.0035,
      "step": 5113
    },
    {
      "epoch": 2.5531702446330504,
      "grad_norm": 0.5228046178817749,
      "learning_rate": 0.0004744682975536695,
      "loss": 0.0058,
      "step": 5114
    },
    {
      "epoch": 2.5536694957563655,
      "grad_norm": 0.07319166511297226,
      "learning_rate": 0.00047446330504243635,
      "loss": 0.0016,
      "step": 5115
    },
    {
      "epoch": 2.5541687468796805,
      "grad_norm": 0.7400792241096497,
      "learning_rate": 0.0004744583125312032,
      "loss": 0.0127,
      "step": 5116
    },
    {
      "epoch": 2.5546679980029956,
      "grad_norm": 0.05383361503481865,
      "learning_rate": 0.00047445332001997006,
      "loss": 0.0012,
      "step": 5117
    },
    {
      "epoch": 2.5551672491263107,
      "grad_norm": 0.07502041757106781,
      "learning_rate": 0.0004744483275087369,
      "loss": 0.0013,
      "step": 5118
    },
    {
      "epoch": 2.5556665002496257,
      "grad_norm": 0.044361662119627,
      "learning_rate": 0.00047444333499750376,
      "loss": 0.001,
      "step": 5119
    },
    {
      "epoch": 2.5561657513729408,
      "grad_norm": 0.029553595930337906,
      "learning_rate": 0.0004744383424862706,
      "loss": 0.0009,
      "step": 5120
    },
    {
      "epoch": 2.5566650024962554,
      "grad_norm": 0.11317387223243713,
      "learning_rate": 0.00047443334997503746,
      "loss": 0.002,
      "step": 5121
    },
    {
      "epoch": 2.557164253619571,
      "grad_norm": 0.07940322905778885,
      "learning_rate": 0.0004744283574638043,
      "loss": 0.0015,
      "step": 5122
    },
    {
      "epoch": 2.5576635047428855,
      "grad_norm": 0.8886115550994873,
      "learning_rate": 0.00047442336495257117,
      "loss": 0.0143,
      "step": 5123
    },
    {
      "epoch": 2.5581627558662006,
      "grad_norm": 1.5748677253723145,
      "learning_rate": 0.000474418372441338,
      "loss": 0.0117,
      "step": 5124
    },
    {
      "epoch": 2.5586620069895156,
      "grad_norm": 0.6998187899589539,
      "learning_rate": 0.0004744133799301049,
      "loss": 0.0795,
      "step": 5125
    },
    {
      "epoch": 2.5591612581128307,
      "grad_norm": 0.5441035032272339,
      "learning_rate": 0.0004744083874188717,
      "loss": 0.003,
      "step": 5126
    },
    {
      "epoch": 2.5596605092361457,
      "grad_norm": 0.35381269454956055,
      "learning_rate": 0.0004744033949076386,
      "loss": 0.0055,
      "step": 5127
    },
    {
      "epoch": 2.560159760359461,
      "grad_norm": 0.026353321969509125,
      "learning_rate": 0.00047439840239640543,
      "loss": 0.0009,
      "step": 5128
    },
    {
      "epoch": 2.560659011482776,
      "grad_norm": 0.07772648334503174,
      "learning_rate": 0.0004743934098851723,
      "loss": 0.0013,
      "step": 5129
    },
    {
      "epoch": 2.561158262606091,
      "grad_norm": 0.41008469462394714,
      "learning_rate": 0.00047438841737393913,
      "loss": 0.0059,
      "step": 5130
    },
    {
      "epoch": 2.561657513729406,
      "grad_norm": 0.15746207535266876,
      "learning_rate": 0.000474383424862706,
      "loss": 0.0028,
      "step": 5131
    },
    {
      "epoch": 2.562156764852721,
      "grad_norm": 0.5428381562232971,
      "learning_rate": 0.00047437843235147284,
      "loss": 0.0137,
      "step": 5132
    },
    {
      "epoch": 2.562656015976036,
      "grad_norm": 0.42903193831443787,
      "learning_rate": 0.0004743734398402397,
      "loss": 0.0052,
      "step": 5133
    },
    {
      "epoch": 2.5631552670993507,
      "grad_norm": 0.016667287796735764,
      "learning_rate": 0.00047436844732900654,
      "loss": 0.0007,
      "step": 5134
    },
    {
      "epoch": 2.563654518222666,
      "grad_norm": 0.19059500098228455,
      "learning_rate": 0.0004743634548177734,
      "loss": 0.002,
      "step": 5135
    },
    {
      "epoch": 2.564153769345981,
      "grad_norm": 0.08595367521047592,
      "learning_rate": 0.0004743584623065402,
      "loss": 0.0016,
      "step": 5136
    },
    {
      "epoch": 2.5646530204692963,
      "grad_norm": 0.24192556738853455,
      "learning_rate": 0.00047435346979530704,
      "loss": 0.0042,
      "step": 5137
    },
    {
      "epoch": 2.565152271592611,
      "grad_norm": 0.18069475889205933,
      "learning_rate": 0.0004743484772840739,
      "loss": 0.0021,
      "step": 5138
    },
    {
      "epoch": 2.565651522715926,
      "grad_norm": 0.4236820936203003,
      "learning_rate": 0.00047434348477284075,
      "loss": 0.0067,
      "step": 5139
    },
    {
      "epoch": 2.566150773839241,
      "grad_norm": 0.6636418104171753,
      "learning_rate": 0.0004743384922616076,
      "loss": 0.0172,
      "step": 5140
    },
    {
      "epoch": 2.566650024962556,
      "grad_norm": 0.11052606999874115,
      "learning_rate": 0.00047433349975037445,
      "loss": 0.0017,
      "step": 5141
    },
    {
      "epoch": 2.567149276085871,
      "grad_norm": 0.02843889594078064,
      "learning_rate": 0.0004743285072391413,
      "loss": 0.0007,
      "step": 5142
    },
    {
      "epoch": 2.5676485272091862,
      "grad_norm": 0.05500602722167969,
      "learning_rate": 0.00047432351472790816,
      "loss": 0.0013,
      "step": 5143
    },
    {
      "epoch": 2.5681477783325013,
      "grad_norm": 0.0339684784412384,
      "learning_rate": 0.000474318522216675,
      "loss": 0.0009,
      "step": 5144
    },
    {
      "epoch": 2.5686470294558164,
      "grad_norm": 0.05304201692342758,
      "learning_rate": 0.00047431352970544186,
      "loss": 0.0013,
      "step": 5145
    },
    {
      "epoch": 2.5691462805791314,
      "grad_norm": 0.14632390439510345,
      "learning_rate": 0.0004743085371942087,
      "loss": 0.0024,
      "step": 5146
    },
    {
      "epoch": 2.5696455317024465,
      "grad_norm": 0.024271933361887932,
      "learning_rate": 0.00047430354468297556,
      "loss": 0.0007,
      "step": 5147
    },
    {
      "epoch": 2.5701447828257615,
      "grad_norm": 0.3993241488933563,
      "learning_rate": 0.0004742985521717424,
      "loss": 0.0049,
      "step": 5148
    },
    {
      "epoch": 2.570644033949076,
      "grad_norm": 0.14879904687404633,
      "learning_rate": 0.00047429355966050927,
      "loss": 0.0015,
      "step": 5149
    },
    {
      "epoch": 2.5711432850723916,
      "grad_norm": 0.11584517359733582,
      "learning_rate": 0.0004742885671492761,
      "loss": 0.0018,
      "step": 5150
    },
    {
      "epoch": 2.5716425361957063,
      "grad_norm": 0.03344280272722244,
      "learning_rate": 0.00047428357463804297,
      "loss": 0.0009,
      "step": 5151
    },
    {
      "epoch": 2.5721417873190213,
      "grad_norm": 1.3750895261764526,
      "learning_rate": 0.0004742785821268098,
      "loss": 0.0133,
      "step": 5152
    },
    {
      "epoch": 2.5726410384423364,
      "grad_norm": 0.07746481895446777,
      "learning_rate": 0.0004742735896155767,
      "loss": 0.0011,
      "step": 5153
    },
    {
      "epoch": 2.5731402895656514,
      "grad_norm": 0.2887670695781708,
      "learning_rate": 0.00047426859710434353,
      "loss": 0.0044,
      "step": 5154
    },
    {
      "epoch": 2.5736395406889665,
      "grad_norm": 0.025797978043556213,
      "learning_rate": 0.0004742636045931104,
      "loss": 0.0008,
      "step": 5155
    },
    {
      "epoch": 2.5741387918122816,
      "grad_norm": 0.36015036702156067,
      "learning_rate": 0.0004742586120818772,
      "loss": 0.0019,
      "step": 5156
    },
    {
      "epoch": 2.5746380429355966,
      "grad_norm": 0.022560233250260353,
      "learning_rate": 0.00047425361957064403,
      "loss": 0.0009,
      "step": 5157
    },
    {
      "epoch": 2.5751372940589117,
      "grad_norm": 0.22985690832138062,
      "learning_rate": 0.0004742486270594109,
      "loss": 0.0031,
      "step": 5158
    },
    {
      "epoch": 2.5756365451822267,
      "grad_norm": 0.703082799911499,
      "learning_rate": 0.00047424363454817773,
      "loss": 0.007,
      "step": 5159
    },
    {
      "epoch": 2.576135796305542,
      "grad_norm": 0.7831361293792725,
      "learning_rate": 0.0004742386420369446,
      "loss": 0.0037,
      "step": 5160
    },
    {
      "epoch": 2.576635047428857,
      "grad_norm": 0.025900747627019882,
      "learning_rate": 0.00047423364952571144,
      "loss": 0.001,
      "step": 5161
    },
    {
      "epoch": 2.5771342985521715,
      "grad_norm": 0.3210184872150421,
      "learning_rate": 0.0004742286570144783,
      "loss": 0.0052,
      "step": 5162
    },
    {
      "epoch": 2.577633549675487,
      "grad_norm": 0.6545701026916504,
      "learning_rate": 0.00047422366450324514,
      "loss": 0.0166,
      "step": 5163
    },
    {
      "epoch": 2.5781328007988016,
      "grad_norm": 0.15351924300193787,
      "learning_rate": 0.000474218671992012,
      "loss": 0.003,
      "step": 5164
    },
    {
      "epoch": 2.578632051922117,
      "grad_norm": 0.02467966079711914,
      "learning_rate": 0.0004742136794807788,
      "loss": 0.0013,
      "step": 5165
    },
    {
      "epoch": 2.5791313030454317,
      "grad_norm": 0.7007651925086975,
      "learning_rate": 0.00047420868696954565,
      "loss": 0.0062,
      "step": 5166
    },
    {
      "epoch": 2.5796305541687468,
      "grad_norm": 0.2928209602832794,
      "learning_rate": 0.0004742036944583125,
      "loss": 0.0031,
      "step": 5167
    },
    {
      "epoch": 2.580129805292062,
      "grad_norm": 0.0836675837635994,
      "learning_rate": 0.00047419870194707935,
      "loss": 0.0025,
      "step": 5168
    },
    {
      "epoch": 2.580629056415377,
      "grad_norm": 0.45681214332580566,
      "learning_rate": 0.0004741937094358462,
      "loss": 0.0188,
      "step": 5169
    },
    {
      "epoch": 2.581128307538692,
      "grad_norm": 0.4041821360588074,
      "learning_rate": 0.00047418871692461305,
      "loss": 0.0062,
      "step": 5170
    },
    {
      "epoch": 2.581627558662007,
      "grad_norm": 0.1959913969039917,
      "learning_rate": 0.0004741837244133799,
      "loss": 0.0029,
      "step": 5171
    },
    {
      "epoch": 2.582126809785322,
      "grad_norm": 0.20492100715637207,
      "learning_rate": 0.00047417873190214676,
      "loss": 0.0041,
      "step": 5172
    },
    {
      "epoch": 2.582626060908637,
      "grad_norm": 0.6555081605911255,
      "learning_rate": 0.0004741737393909136,
      "loss": 0.0252,
      "step": 5173
    },
    {
      "epoch": 2.583125312031952,
      "grad_norm": 0.44846996665000916,
      "learning_rate": 0.00047416874687968046,
      "loss": 0.0032,
      "step": 5174
    },
    {
      "epoch": 2.5836245631552672,
      "grad_norm": 0.044234104454517365,
      "learning_rate": 0.0004741637543684473,
      "loss": 0.0014,
      "step": 5175
    },
    {
      "epoch": 2.5841238142785823,
      "grad_norm": 0.10708480328321457,
      "learning_rate": 0.00047415876185721417,
      "loss": 0.0017,
      "step": 5176
    },
    {
      "epoch": 2.584623065401897,
      "grad_norm": 0.4756474792957306,
      "learning_rate": 0.000474153769345981,
      "loss": 0.0057,
      "step": 5177
    },
    {
      "epoch": 2.5851223165252124,
      "grad_norm": 0.03520902991294861,
      "learning_rate": 0.00047414877683474787,
      "loss": 0.0012,
      "step": 5178
    },
    {
      "epoch": 2.585621567648527,
      "grad_norm": 0.17369581758975983,
      "learning_rate": 0.0004741437843235147,
      "loss": 0.0019,
      "step": 5179
    },
    {
      "epoch": 2.586120818771842,
      "grad_norm": 0.20138375461101532,
      "learning_rate": 0.0004741387918122816,
      "loss": 0.0034,
      "step": 5180
    },
    {
      "epoch": 2.586620069895157,
      "grad_norm": 0.20991191267967224,
      "learning_rate": 0.0004741337993010484,
      "loss": 0.006,
      "step": 5181
    },
    {
      "epoch": 2.587119321018472,
      "grad_norm": 0.07939516007900238,
      "learning_rate": 0.0004741288067898153,
      "loss": 0.0009,
      "step": 5182
    },
    {
      "epoch": 2.5876185721417873,
      "grad_norm": 1.1711455583572388,
      "learning_rate": 0.00047412381427858213,
      "loss": 0.0158,
      "step": 5183
    },
    {
      "epoch": 2.5881178232651023,
      "grad_norm": 0.4581000804901123,
      "learning_rate": 0.000474118821767349,
      "loss": 0.0052,
      "step": 5184
    },
    {
      "epoch": 2.5886170743884174,
      "grad_norm": 0.29259759187698364,
      "learning_rate": 0.00047411382925611583,
      "loss": 0.0026,
      "step": 5185
    },
    {
      "epoch": 2.5891163255117324,
      "grad_norm": 2.3772945404052734,
      "learning_rate": 0.0004741088367448827,
      "loss": 0.0055,
      "step": 5186
    },
    {
      "epoch": 2.5896155766350475,
      "grad_norm": 0.032797060906887054,
      "learning_rate": 0.00047410384423364954,
      "loss": 0.0008,
      "step": 5187
    },
    {
      "epoch": 2.5901148277583625,
      "grad_norm": 0.12264407426118851,
      "learning_rate": 0.0004740988517224164,
      "loss": 0.0012,
      "step": 5188
    },
    {
      "epoch": 2.5906140788816776,
      "grad_norm": 0.6503785252571106,
      "learning_rate": 0.00047409385921118324,
      "loss": 0.0159,
      "step": 5189
    },
    {
      "epoch": 2.5911133300049927,
      "grad_norm": 0.03690105304121971,
      "learning_rate": 0.0004740888666999501,
      "loss": 0.0009,
      "step": 5190
    },
    {
      "epoch": 2.5916125811283077,
      "grad_norm": 0.019162630662322044,
      "learning_rate": 0.00047408387418871695,
      "loss": 0.0008,
      "step": 5191
    },
    {
      "epoch": 2.5921118322516223,
      "grad_norm": 0.5037464499473572,
      "learning_rate": 0.0004740788816774838,
      "loss": 0.0068,
      "step": 5192
    },
    {
      "epoch": 2.592611083374938,
      "grad_norm": 0.49540549516677856,
      "learning_rate": 0.00047407388916625065,
      "loss": 0.0179,
      "step": 5193
    },
    {
      "epoch": 2.5931103344982525,
      "grad_norm": 0.7473593950271606,
      "learning_rate": 0.00047406889665501745,
      "loss": 0.029,
      "step": 5194
    },
    {
      "epoch": 2.5936095856215675,
      "grad_norm": 0.6834574341773987,
      "learning_rate": 0.0004740639041437843,
      "loss": 0.0035,
      "step": 5195
    },
    {
      "epoch": 2.5941088367448826,
      "grad_norm": 0.28686779737472534,
      "learning_rate": 0.00047405891163255115,
      "loss": 0.0041,
      "step": 5196
    },
    {
      "epoch": 2.5946080878681976,
      "grad_norm": 1.3066577911376953,
      "learning_rate": 0.000474053919121318,
      "loss": 0.0249,
      "step": 5197
    },
    {
      "epoch": 2.5951073389915127,
      "grad_norm": 0.1257316768169403,
      "learning_rate": 0.00047404892661008486,
      "loss": 0.0026,
      "step": 5198
    },
    {
      "epoch": 2.5956065901148277,
      "grad_norm": 0.0821545198559761,
      "learning_rate": 0.0004740439340988517,
      "loss": 0.0018,
      "step": 5199
    },
    {
      "epoch": 2.596105841238143,
      "grad_norm": 0.8970964550971985,
      "learning_rate": 0.00047403894158761856,
      "loss": 0.0072,
      "step": 5200
    },
    {
      "epoch": 2.596605092361458,
      "grad_norm": 0.35721033811569214,
      "learning_rate": 0.0004740339490763854,
      "loss": 0.0072,
      "step": 5201
    },
    {
      "epoch": 2.597104343484773,
      "grad_norm": 0.4044061005115509,
      "learning_rate": 0.00047402895656515227,
      "loss": 0.011,
      "step": 5202
    },
    {
      "epoch": 2.597603594608088,
      "grad_norm": 0.4858359098434448,
      "learning_rate": 0.0004740239640539191,
      "loss": 0.0077,
      "step": 5203
    },
    {
      "epoch": 2.598102845731403,
      "grad_norm": 0.23422467708587646,
      "learning_rate": 0.00047401897154268597,
      "loss": 0.0038,
      "step": 5204
    },
    {
      "epoch": 2.5986020968547177,
      "grad_norm": 0.27425652742385864,
      "learning_rate": 0.0004740139790314528,
      "loss": 0.0032,
      "step": 5205
    },
    {
      "epoch": 2.599101347978033,
      "grad_norm": 0.3295878767967224,
      "learning_rate": 0.0004740089865202197,
      "loss": 0.005,
      "step": 5206
    },
    {
      "epoch": 2.5996005991013478,
      "grad_norm": 0.5542248487472534,
      "learning_rate": 0.0004740039940089865,
      "loss": 0.0084,
      "step": 5207
    },
    {
      "epoch": 2.6000998502246633,
      "grad_norm": 0.37378403544425964,
      "learning_rate": 0.0004739990014977534,
      "loss": 0.007,
      "step": 5208
    },
    {
      "epoch": 2.600599101347978,
      "grad_norm": 1.0710303783416748,
      "learning_rate": 0.00047399400898652023,
      "loss": 0.0118,
      "step": 5209
    },
    {
      "epoch": 2.601098352471293,
      "grad_norm": 0.07732430100440979,
      "learning_rate": 0.0004739890164752871,
      "loss": 0.0018,
      "step": 5210
    },
    {
      "epoch": 2.601597603594608,
      "grad_norm": 0.09367043524980545,
      "learning_rate": 0.00047398402396405393,
      "loss": 0.0025,
      "step": 5211
    },
    {
      "epoch": 2.602096854717923,
      "grad_norm": 0.148185133934021,
      "learning_rate": 0.0004739790314528208,
      "loss": 0.0032,
      "step": 5212
    },
    {
      "epoch": 2.602596105841238,
      "grad_norm": 0.7664543390274048,
      "learning_rate": 0.00047397403894158764,
      "loss": 0.0074,
      "step": 5213
    },
    {
      "epoch": 2.603095356964553,
      "grad_norm": 0.1308397650718689,
      "learning_rate": 0.0004739690464303545,
      "loss": 0.0027,
      "step": 5214
    },
    {
      "epoch": 2.6035946080878682,
      "grad_norm": 0.3762720227241516,
      "learning_rate": 0.00047396405391912134,
      "loss": 0.0052,
      "step": 5215
    },
    {
      "epoch": 2.6040938592111833,
      "grad_norm": 0.16692903637886047,
      "learning_rate": 0.0004739590614078882,
      "loss": 0.0037,
      "step": 5216
    },
    {
      "epoch": 2.6045931103344984,
      "grad_norm": 0.03974730521440506,
      "learning_rate": 0.00047395406889665505,
      "loss": 0.0014,
      "step": 5217
    },
    {
      "epoch": 2.6050923614578134,
      "grad_norm": 0.18097983300685883,
      "learning_rate": 0.0004739490763854219,
      "loss": 0.0033,
      "step": 5218
    },
    {
      "epoch": 2.6055916125811285,
      "grad_norm": 1.6279553174972534,
      "learning_rate": 0.00047394408387418875,
      "loss": 0.0318,
      "step": 5219
    },
    {
      "epoch": 2.606090863704443,
      "grad_norm": 0.05683658644556999,
      "learning_rate": 0.0004739390913629556,
      "loss": 0.002,
      "step": 5220
    },
    {
      "epoch": 2.6065901148277586,
      "grad_norm": 0.11907480657100677,
      "learning_rate": 0.00047393409885172245,
      "loss": 0.0023,
      "step": 5221
    },
    {
      "epoch": 2.607089365951073,
      "grad_norm": 0.27230748534202576,
      "learning_rate": 0.0004739291063404893,
      "loss": 0.0073,
      "step": 5222
    },
    {
      "epoch": 2.6075886170743883,
      "grad_norm": 0.05305939167737961,
      "learning_rate": 0.0004739241138292561,
      "loss": 0.0013,
      "step": 5223
    },
    {
      "epoch": 2.6080878681977033,
      "grad_norm": 0.24398906528949738,
      "learning_rate": 0.00047391912131802296,
      "loss": 0.0041,
      "step": 5224
    },
    {
      "epoch": 2.6085871193210184,
      "grad_norm": 1.0556960105895996,
      "learning_rate": 0.0004739141288067898,
      "loss": 0.0167,
      "step": 5225
    },
    {
      "epoch": 2.6090863704443334,
      "grad_norm": 0.550711452960968,
      "learning_rate": 0.00047390913629555666,
      "loss": 0.021,
      "step": 5226
    },
    {
      "epoch": 2.6095856215676485,
      "grad_norm": 0.1282958984375,
      "learning_rate": 0.0004739041437843235,
      "loss": 0.0018,
      "step": 5227
    },
    {
      "epoch": 2.6100848726909636,
      "grad_norm": 0.04081554710865021,
      "learning_rate": 0.00047389915127309036,
      "loss": 0.0012,
      "step": 5228
    },
    {
      "epoch": 2.6105841238142786,
      "grad_norm": 0.24263198673725128,
      "learning_rate": 0.0004738941587618572,
      "loss": 0.0102,
      "step": 5229
    },
    {
      "epoch": 2.6110833749375937,
      "grad_norm": 0.10771960020065308,
      "learning_rate": 0.00047388916625062407,
      "loss": 0.0016,
      "step": 5230
    },
    {
      "epoch": 2.6115826260609087,
      "grad_norm": 0.1261511594057083,
      "learning_rate": 0.0004738841737393909,
      "loss": 0.002,
      "step": 5231
    },
    {
      "epoch": 2.612081877184224,
      "grad_norm": 0.29577064514160156,
      "learning_rate": 0.0004738791812281578,
      "loss": 0.0042,
      "step": 5232
    },
    {
      "epoch": 2.612581128307539,
      "grad_norm": 0.4089691638946533,
      "learning_rate": 0.0004738741887169246,
      "loss": 0.0119,
      "step": 5233
    },
    {
      "epoch": 2.613080379430854,
      "grad_norm": 0.16766540706157684,
      "learning_rate": 0.0004738691962056915,
      "loss": 0.0036,
      "step": 5234
    },
    {
      "epoch": 2.6135796305541685,
      "grad_norm": 0.41906464099884033,
      "learning_rate": 0.00047386420369445833,
      "loss": 0.0132,
      "step": 5235
    },
    {
      "epoch": 2.614078881677484,
      "grad_norm": 0.2620093524456024,
      "learning_rate": 0.0004738592111832252,
      "loss": 0.0044,
      "step": 5236
    },
    {
      "epoch": 2.6145781328007986,
      "grad_norm": 0.14872649312019348,
      "learning_rate": 0.00047385421867199203,
      "loss": 0.0028,
      "step": 5237
    },
    {
      "epoch": 2.6150773839241137,
      "grad_norm": 0.5598771572113037,
      "learning_rate": 0.0004738492261607589,
      "loss": 0.0057,
      "step": 5238
    },
    {
      "epoch": 2.6155766350474288,
      "grad_norm": 0.3952343165874481,
      "learning_rate": 0.00047384423364952574,
      "loss": 0.0052,
      "step": 5239
    },
    {
      "epoch": 2.616075886170744,
      "grad_norm": 0.9362009763717651,
      "learning_rate": 0.0004738392411382926,
      "loss": 0.0121,
      "step": 5240
    },
    {
      "epoch": 2.616575137294059,
      "grad_norm": 0.271535724401474,
      "learning_rate": 0.00047383424862705944,
      "loss": 0.0041,
      "step": 5241
    },
    {
      "epoch": 2.617074388417374,
      "grad_norm": 0.2603663206100464,
      "learning_rate": 0.0004738292561158263,
      "loss": 0.0016,
      "step": 5242
    },
    {
      "epoch": 2.617573639540689,
      "grad_norm": 0.2726254463195801,
      "learning_rate": 0.00047382426360459315,
      "loss": 0.0032,
      "step": 5243
    },
    {
      "epoch": 2.618072890664004,
      "grad_norm": 0.24379436671733856,
      "learning_rate": 0.00047381927109336,
      "loss": 0.0052,
      "step": 5244
    },
    {
      "epoch": 2.618572141787319,
      "grad_norm": 0.45019274950027466,
      "learning_rate": 0.00047381427858212685,
      "loss": 0.0114,
      "step": 5245
    },
    {
      "epoch": 2.619071392910634,
      "grad_norm": 0.06285407394170761,
      "learning_rate": 0.0004738092860708937,
      "loss": 0.0012,
      "step": 5246
    },
    {
      "epoch": 2.6195706440339492,
      "grad_norm": 3.250213384628296,
      "learning_rate": 0.00047380429355966055,
      "loss": 0.013,
      "step": 5247
    },
    {
      "epoch": 2.620069895157264,
      "grad_norm": 0.44397807121276855,
      "learning_rate": 0.0004737993010484274,
      "loss": 0.0068,
      "step": 5248
    },
    {
      "epoch": 2.6205691462805794,
      "grad_norm": 0.35478609800338745,
      "learning_rate": 0.00047379430853719426,
      "loss": 0.005,
      "step": 5249
    },
    {
      "epoch": 2.621068397403894,
      "grad_norm": 0.17749199271202087,
      "learning_rate": 0.0004737893160259611,
      "loss": 0.0026,
      "step": 5250
    },
    {
      "epoch": 2.621567648527209,
      "grad_norm": 0.24415527284145355,
      "learning_rate": 0.00047378432351472796,
      "loss": 0.0046,
      "step": 5251
    },
    {
      "epoch": 2.622066899650524,
      "grad_norm": 0.8815836906433105,
      "learning_rate": 0.00047377933100349476,
      "loss": 0.013,
      "step": 5252
    },
    {
      "epoch": 2.622566150773839,
      "grad_norm": 0.028310809284448624,
      "learning_rate": 0.0004737743384922616,
      "loss": 0.001,
      "step": 5253
    },
    {
      "epoch": 2.623065401897154,
      "grad_norm": 0.09189339727163315,
      "learning_rate": 0.00047376934598102846,
      "loss": 0.0013,
      "step": 5254
    },
    {
      "epoch": 2.6235646530204693,
      "grad_norm": 0.7783353328704834,
      "learning_rate": 0.0004737643534697953,
      "loss": 0.0233,
      "step": 5255
    },
    {
      "epoch": 2.6240639041437843,
      "grad_norm": 0.08103221654891968,
      "learning_rate": 0.00047375936095856217,
      "loss": 0.0018,
      "step": 5256
    },
    {
      "epoch": 2.6245631552670994,
      "grad_norm": 0.4137299656867981,
      "learning_rate": 0.000473754368447329,
      "loss": 0.0191,
      "step": 5257
    },
    {
      "epoch": 2.6250624063904144,
      "grad_norm": 0.08567453175783157,
      "learning_rate": 0.00047374937593609587,
      "loss": 0.0015,
      "step": 5258
    },
    {
      "epoch": 2.6255616575137295,
      "grad_norm": 0.17177273333072662,
      "learning_rate": 0.0004737443834248627,
      "loss": 0.0026,
      "step": 5259
    },
    {
      "epoch": 2.6260609086370446,
      "grad_norm": 0.5126968026161194,
      "learning_rate": 0.0004737393909136296,
      "loss": 0.0074,
      "step": 5260
    },
    {
      "epoch": 2.6265601597603596,
      "grad_norm": 0.09692128747701645,
      "learning_rate": 0.00047373439840239643,
      "loss": 0.0016,
      "step": 5261
    },
    {
      "epoch": 2.6270594108836747,
      "grad_norm": 0.11956297606229782,
      "learning_rate": 0.0004737294058911633,
      "loss": 0.0018,
      "step": 5262
    },
    {
      "epoch": 2.6275586620069893,
      "grad_norm": 0.35201066732406616,
      "learning_rate": 0.00047372441337993013,
      "loss": 0.0039,
      "step": 5263
    },
    {
      "epoch": 2.628057913130305,
      "grad_norm": 0.06791634857654572,
      "learning_rate": 0.000473719420868697,
      "loss": 0.0014,
      "step": 5264
    },
    {
      "epoch": 2.6285571642536194,
      "grad_norm": 0.15222032368183136,
      "learning_rate": 0.00047371442835746384,
      "loss": 0.0037,
      "step": 5265
    },
    {
      "epoch": 2.6290564153769345,
      "grad_norm": 0.377170205116272,
      "learning_rate": 0.0004737094358462307,
      "loss": 0.0039,
      "step": 5266
    },
    {
      "epoch": 2.6295556665002495,
      "grad_norm": 0.36292564868927,
      "learning_rate": 0.00047370444333499754,
      "loss": 0.0062,
      "step": 5267
    },
    {
      "epoch": 2.6300549176235646,
      "grad_norm": 0.0208151675760746,
      "learning_rate": 0.0004736994508237644,
      "loss": 0.0008,
      "step": 5268
    },
    {
      "epoch": 2.6305541687468796,
      "grad_norm": 0.024109870195388794,
      "learning_rate": 0.0004736944583125312,
      "loss": 0.001,
      "step": 5269
    },
    {
      "epoch": 2.6310534198701947,
      "grad_norm": 0.2513985335826874,
      "learning_rate": 0.00047368946580129804,
      "loss": 0.0064,
      "step": 5270
    },
    {
      "epoch": 2.6315526709935098,
      "grad_norm": 0.26795825362205505,
      "learning_rate": 0.0004736844732900649,
      "loss": 0.0031,
      "step": 5271
    },
    {
      "epoch": 2.632051922116825,
      "grad_norm": 0.0536988191306591,
      "learning_rate": 0.00047367948077883175,
      "loss": 0.0012,
      "step": 5272
    },
    {
      "epoch": 2.63255117324014,
      "grad_norm": 0.03024117648601532,
      "learning_rate": 0.0004736744882675986,
      "loss": 0.0009,
      "step": 5273
    },
    {
      "epoch": 2.633050424363455,
      "grad_norm": 0.8978301882743835,
      "learning_rate": 0.00047366949575636545,
      "loss": 0.0553,
      "step": 5274
    },
    {
      "epoch": 2.63354967548677,
      "grad_norm": 0.4871300160884857,
      "learning_rate": 0.0004736645032451323,
      "loss": 0.0059,
      "step": 5275
    },
    {
      "epoch": 2.6340489266100846,
      "grad_norm": 0.01422484964132309,
      "learning_rate": 0.00047365951073389916,
      "loss": 0.0007,
      "step": 5276
    },
    {
      "epoch": 2.6345481777334,
      "grad_norm": 0.22076761722564697,
      "learning_rate": 0.000473654518222666,
      "loss": 0.0108,
      "step": 5277
    },
    {
      "epoch": 2.6350474288567147,
      "grad_norm": 0.053337130695581436,
      "learning_rate": 0.00047364952571143286,
      "loss": 0.0012,
      "step": 5278
    },
    {
      "epoch": 2.6355466799800302,
      "grad_norm": 0.5025407075881958,
      "learning_rate": 0.0004736445332001997,
      "loss": 0.019,
      "step": 5279
    },
    {
      "epoch": 2.636045931103345,
      "grad_norm": 0.3089081943035126,
      "learning_rate": 0.00047363954068896656,
      "loss": 0.0032,
      "step": 5280
    },
    {
      "epoch": 2.63654518222666,
      "grad_norm": 0.2857474684715271,
      "learning_rate": 0.0004736345481777334,
      "loss": 0.0075,
      "step": 5281
    },
    {
      "epoch": 2.637044433349975,
      "grad_norm": 0.040926896035671234,
      "learning_rate": 0.0004736295556665002,
      "loss": 0.0011,
      "step": 5282
    },
    {
      "epoch": 2.63754368447329,
      "grad_norm": 0.14895138144493103,
      "learning_rate": 0.00047362456315526707,
      "loss": 0.0034,
      "step": 5283
    },
    {
      "epoch": 2.638042935596605,
      "grad_norm": 0.13786734640598297,
      "learning_rate": 0.0004736195706440339,
      "loss": 0.0023,
      "step": 5284
    },
    {
      "epoch": 2.63854218671992,
      "grad_norm": 0.09687026590108871,
      "learning_rate": 0.00047361457813280077,
      "loss": 0.0014,
      "step": 5285
    },
    {
      "epoch": 2.639041437843235,
      "grad_norm": 0.2594188451766968,
      "learning_rate": 0.0004736095856215676,
      "loss": 0.0077,
      "step": 5286
    },
    {
      "epoch": 2.6395406889665503,
      "grad_norm": 0.14573968946933746,
      "learning_rate": 0.0004736045931103345,
      "loss": 0.0027,
      "step": 5287
    },
    {
      "epoch": 2.6400399400898653,
      "grad_norm": 0.009601359255611897,
      "learning_rate": 0.0004735996005991013,
      "loss": 0.0006,
      "step": 5288
    },
    {
      "epoch": 2.6405391912131804,
      "grad_norm": 0.13268813490867615,
      "learning_rate": 0.0004735946080878682,
      "loss": 0.0017,
      "step": 5289
    },
    {
      "epoch": 2.6410384423364954,
      "grad_norm": 0.14490292966365814,
      "learning_rate": 0.00047358961557663503,
      "loss": 0.0024,
      "step": 5290
    },
    {
      "epoch": 2.64153769345981,
      "grad_norm": 0.037576980888843536,
      "learning_rate": 0.0004735846230654019,
      "loss": 0.0008,
      "step": 5291
    },
    {
      "epoch": 2.6420369445831255,
      "grad_norm": 0.28823986649513245,
      "learning_rate": 0.00047357963055416873,
      "loss": 0.0036,
      "step": 5292
    },
    {
      "epoch": 2.64253619570644,
      "grad_norm": 0.1914701908826828,
      "learning_rate": 0.0004735746380429356,
      "loss": 0.0031,
      "step": 5293
    },
    {
      "epoch": 2.643035446829755,
      "grad_norm": 0.015926610678434372,
      "learning_rate": 0.00047356964553170244,
      "loss": 0.0006,
      "step": 5294
    },
    {
      "epoch": 2.6435346979530703,
      "grad_norm": 0.3151698410511017,
      "learning_rate": 0.0004735646530204693,
      "loss": 0.0046,
      "step": 5295
    },
    {
      "epoch": 2.6440339490763853,
      "grad_norm": 0.07074182480573654,
      "learning_rate": 0.00047355966050923614,
      "loss": 0.0011,
      "step": 5296
    },
    {
      "epoch": 2.6445332001997004,
      "grad_norm": 0.15962152183055878,
      "learning_rate": 0.000473554667998003,
      "loss": 0.0015,
      "step": 5297
    },
    {
      "epoch": 2.6450324513230155,
      "grad_norm": 0.4132579267024994,
      "learning_rate": 0.00047354967548676985,
      "loss": 0.0067,
      "step": 5298
    },
    {
      "epoch": 2.6455317024463305,
      "grad_norm": 0.37182265520095825,
      "learning_rate": 0.0004735446829755367,
      "loss": 0.0049,
      "step": 5299
    },
    {
      "epoch": 2.6460309535696456,
      "grad_norm": 0.11308322101831436,
      "learning_rate": 0.00047353969046430355,
      "loss": 0.0014,
      "step": 5300
    },
    {
      "epoch": 2.6465302046929606,
      "grad_norm": 0.1174461841583252,
      "learning_rate": 0.0004735346979530704,
      "loss": 0.0018,
      "step": 5301
    },
    {
      "epoch": 2.6470294558162757,
      "grad_norm": 0.012732462957501411,
      "learning_rate": 0.00047352970544183726,
      "loss": 0.0005,
      "step": 5302
    },
    {
      "epoch": 2.6475287069395907,
      "grad_norm": 0.10482392460107803,
      "learning_rate": 0.0004735247129306041,
      "loss": 0.0015,
      "step": 5303
    },
    {
      "epoch": 2.648027958062906,
      "grad_norm": 0.8677244782447815,
      "learning_rate": 0.00047351972041937096,
      "loss": 0.0027,
      "step": 5304
    },
    {
      "epoch": 2.648527209186221,
      "grad_norm": 0.07369306683540344,
      "learning_rate": 0.0004735147279081378,
      "loss": 0.0006,
      "step": 5305
    },
    {
      "epoch": 2.6490264603095355,
      "grad_norm": 0.03171398863196373,
      "learning_rate": 0.00047350973539690466,
      "loss": 0.0008,
      "step": 5306
    },
    {
      "epoch": 2.649525711432851,
      "grad_norm": 0.2138976752758026,
      "learning_rate": 0.0004735047428856715,
      "loss": 0.0089,
      "step": 5307
    },
    {
      "epoch": 2.6500249625561656,
      "grad_norm": 0.5078824162483215,
      "learning_rate": 0.00047349975037443837,
      "loss": 0.0048,
      "step": 5308
    },
    {
      "epoch": 2.6505242136794807,
      "grad_norm": 0.1314522922039032,
      "learning_rate": 0.0004734947578632052,
      "loss": 0.0015,
      "step": 5309
    },
    {
      "epoch": 2.6510234648027957,
      "grad_norm": 0.315382182598114,
      "learning_rate": 0.00047348976535197207,
      "loss": 0.0031,
      "step": 5310
    },
    {
      "epoch": 2.6515227159261108,
      "grad_norm": 0.10445519536733627,
      "learning_rate": 0.00047348477284073887,
      "loss": 0.0012,
      "step": 5311
    },
    {
      "epoch": 2.652021967049426,
      "grad_norm": 0.0601019449532032,
      "learning_rate": 0.0004734797803295057,
      "loss": 0.0009,
      "step": 5312
    },
    {
      "epoch": 2.652521218172741,
      "grad_norm": 0.47113776206970215,
      "learning_rate": 0.0004734747878182726,
      "loss": 0.0037,
      "step": 5313
    },
    {
      "epoch": 2.653020469296056,
      "grad_norm": 0.039878860116004944,
      "learning_rate": 0.0004734697953070394,
      "loss": 0.0008,
      "step": 5314
    },
    {
      "epoch": 2.653519720419371,
      "grad_norm": 0.5261668562889099,
      "learning_rate": 0.0004734648027958063,
      "loss": 0.0059,
      "step": 5315
    },
    {
      "epoch": 2.654018971542686,
      "grad_norm": 0.053010717034339905,
      "learning_rate": 0.00047345981028457313,
      "loss": 0.0008,
      "step": 5316
    },
    {
      "epoch": 2.654518222666001,
      "grad_norm": 0.12137774378061295,
      "learning_rate": 0.00047345481777334,
      "loss": 0.0017,
      "step": 5317
    },
    {
      "epoch": 2.655017473789316,
      "grad_norm": 0.20539988577365875,
      "learning_rate": 0.00047344982526210683,
      "loss": 0.0031,
      "step": 5318
    },
    {
      "epoch": 2.655516724912631,
      "grad_norm": 0.6721805334091187,
      "learning_rate": 0.0004734448327508737,
      "loss": 0.015,
      "step": 5319
    },
    {
      "epoch": 2.6560159760359463,
      "grad_norm": 0.6832074522972107,
      "learning_rate": 0.00047343984023964054,
      "loss": 0.0019,
      "step": 5320
    },
    {
      "epoch": 2.656515227159261,
      "grad_norm": 0.8367159366607666,
      "learning_rate": 0.0004734348477284074,
      "loss": 0.014,
      "step": 5321
    },
    {
      "epoch": 2.657014478282576,
      "grad_norm": 0.06114073097705841,
      "learning_rate": 0.00047342985521717424,
      "loss": 0.0009,
      "step": 5322
    },
    {
      "epoch": 2.657513729405891,
      "grad_norm": 0.23231413960456848,
      "learning_rate": 0.0004734248627059411,
      "loss": 0.0038,
      "step": 5323
    },
    {
      "epoch": 2.658012980529206,
      "grad_norm": 0.22636640071868896,
      "learning_rate": 0.00047341987019470795,
      "loss": 0.0039,
      "step": 5324
    },
    {
      "epoch": 2.658512231652521,
      "grad_norm": 0.04495909437537193,
      "learning_rate": 0.0004734148776834748,
      "loss": 0.001,
      "step": 5325
    },
    {
      "epoch": 2.659011482775836,
      "grad_norm": 0.560653805732727,
      "learning_rate": 0.00047340988517224165,
      "loss": 0.0209,
      "step": 5326
    },
    {
      "epoch": 2.6595107338991513,
      "grad_norm": 0.4214307367801666,
      "learning_rate": 0.0004734048926610085,
      "loss": 0.0136,
      "step": 5327
    },
    {
      "epoch": 2.6600099850224663,
      "grad_norm": 0.10382415354251862,
      "learning_rate": 0.00047339990014977535,
      "loss": 0.0014,
      "step": 5328
    },
    {
      "epoch": 2.6605092361457814,
      "grad_norm": 0.8967181444168091,
      "learning_rate": 0.0004733949076385422,
      "loss": 0.1413,
      "step": 5329
    },
    {
      "epoch": 2.6610084872690964,
      "grad_norm": 0.7153639793395996,
      "learning_rate": 0.00047338991512730906,
      "loss": 0.0055,
      "step": 5330
    },
    {
      "epoch": 2.6615077383924115,
      "grad_norm": 1.4240574836730957,
      "learning_rate": 0.0004733849226160759,
      "loss": 0.0054,
      "step": 5331
    },
    {
      "epoch": 2.6620069895157266,
      "grad_norm": 0.042078468948602676,
      "learning_rate": 0.00047337993010484276,
      "loss": 0.0016,
      "step": 5332
    },
    {
      "epoch": 2.6625062406390416,
      "grad_norm": 0.021465277299284935,
      "learning_rate": 0.0004733749375936096,
      "loss": 0.0009,
      "step": 5333
    },
    {
      "epoch": 2.6630054917623562,
      "grad_norm": 0.056878965348005295,
      "learning_rate": 0.00047336994508237647,
      "loss": 0.0012,
      "step": 5334
    },
    {
      "epoch": 2.6635047428856717,
      "grad_norm": 0.15193210542201996,
      "learning_rate": 0.0004733649525711433,
      "loss": 0.0028,
      "step": 5335
    },
    {
      "epoch": 2.6640039940089864,
      "grad_norm": 0.6023770570755005,
      "learning_rate": 0.00047335996005991017,
      "loss": 0.0232,
      "step": 5336
    },
    {
      "epoch": 2.6645032451323014,
      "grad_norm": 0.36405789852142334,
      "learning_rate": 0.000473354967548677,
      "loss": 0.0067,
      "step": 5337
    },
    {
      "epoch": 2.6650024962556165,
      "grad_norm": 0.1352347880601883,
      "learning_rate": 0.0004733499750374439,
      "loss": 0.003,
      "step": 5338
    },
    {
      "epoch": 2.6655017473789315,
      "grad_norm": 0.24829602241516113,
      "learning_rate": 0.00047334498252621073,
      "loss": 0.0048,
      "step": 5339
    },
    {
      "epoch": 2.6660009985022466,
      "grad_norm": 0.10898204147815704,
      "learning_rate": 0.0004733399900149775,
      "loss": 0.0025,
      "step": 5340
    },
    {
      "epoch": 2.6665002496255616,
      "grad_norm": 0.2564423382282257,
      "learning_rate": 0.0004733349975037444,
      "loss": 0.0055,
      "step": 5341
    },
    {
      "epoch": 2.6669995007488767,
      "grad_norm": 0.11350947618484497,
      "learning_rate": 0.00047333000499251123,
      "loss": 0.0034,
      "step": 5342
    },
    {
      "epoch": 2.6674987518721918,
      "grad_norm": 0.24081574380397797,
      "learning_rate": 0.0004733250124812781,
      "loss": 0.0071,
      "step": 5343
    },
    {
      "epoch": 2.667998002995507,
      "grad_norm": 0.12809878587722778,
      "learning_rate": 0.00047332001997004493,
      "loss": 0.0021,
      "step": 5344
    },
    {
      "epoch": 2.668497254118822,
      "grad_norm": 0.4863182008266449,
      "learning_rate": 0.0004733150274588118,
      "loss": 0.0231,
      "step": 5345
    },
    {
      "epoch": 2.668996505242137,
      "grad_norm": 0.16078917682170868,
      "learning_rate": 0.00047331003494757864,
      "loss": 0.0036,
      "step": 5346
    },
    {
      "epoch": 2.6694957563654516,
      "grad_norm": 0.11833971738815308,
      "learning_rate": 0.0004733050424363455,
      "loss": 0.0024,
      "step": 5347
    },
    {
      "epoch": 2.669995007488767,
      "grad_norm": 0.0758470967411995,
      "learning_rate": 0.00047330004992511234,
      "loss": 0.0016,
      "step": 5348
    },
    {
      "epoch": 2.6704942586120817,
      "grad_norm": 0.018635082989931107,
      "learning_rate": 0.0004732950574138792,
      "loss": 0.0009,
      "step": 5349
    },
    {
      "epoch": 2.670993509735397,
      "grad_norm": 0.11186929047107697,
      "learning_rate": 0.00047329006490264605,
      "loss": 0.0018,
      "step": 5350
    },
    {
      "epoch": 2.671492760858712,
      "grad_norm": 0.07674059271812439,
      "learning_rate": 0.0004732850723914129,
      "loss": 0.0021,
      "step": 5351
    },
    {
      "epoch": 2.671992011982027,
      "grad_norm": 0.35565707087516785,
      "learning_rate": 0.00047328007988017975,
      "loss": 0.0037,
      "step": 5352
    },
    {
      "epoch": 2.672491263105342,
      "grad_norm": 0.00619569281116128,
      "learning_rate": 0.0004732750873689466,
      "loss": 0.0006,
      "step": 5353
    },
    {
      "epoch": 2.672990514228657,
      "grad_norm": 0.2510656714439392,
      "learning_rate": 0.00047327009485771345,
      "loss": 0.0034,
      "step": 5354
    },
    {
      "epoch": 2.673489765351972,
      "grad_norm": 0.706070065498352,
      "learning_rate": 0.0004732651023464803,
      "loss": 0.0047,
      "step": 5355
    },
    {
      "epoch": 2.673989016475287,
      "grad_norm": 0.23617613315582275,
      "learning_rate": 0.00047326010983524716,
      "loss": 0.0064,
      "step": 5356
    },
    {
      "epoch": 2.674488267598602,
      "grad_norm": 0.11044487357139587,
      "learning_rate": 0.000473255117324014,
      "loss": 0.002,
      "step": 5357
    },
    {
      "epoch": 2.674987518721917,
      "grad_norm": 0.05908426269888878,
      "learning_rate": 0.00047325012481278086,
      "loss": 0.0017,
      "step": 5358
    },
    {
      "epoch": 2.6754867698452323,
      "grad_norm": 0.06164557486772537,
      "learning_rate": 0.0004732451323015477,
      "loss": 0.0015,
      "step": 5359
    },
    {
      "epoch": 2.6759860209685473,
      "grad_norm": 0.2212391495704651,
      "learning_rate": 0.00047324013979031457,
      "loss": 0.0054,
      "step": 5360
    },
    {
      "epoch": 2.6764852720918624,
      "grad_norm": 0.293831467628479,
      "learning_rate": 0.0004732351472790814,
      "loss": 0.0045,
      "step": 5361
    },
    {
      "epoch": 2.676984523215177,
      "grad_norm": 0.2632746994495392,
      "learning_rate": 0.00047323015476784827,
      "loss": 0.0049,
      "step": 5362
    },
    {
      "epoch": 2.6774837743384925,
      "grad_norm": 0.05011811479926109,
      "learning_rate": 0.0004732251622566151,
      "loss": 0.001,
      "step": 5363
    },
    {
      "epoch": 2.677983025461807,
      "grad_norm": 0.04944847896695137,
      "learning_rate": 0.000473220169745382,
      "loss": 0.0014,
      "step": 5364
    },
    {
      "epoch": 2.678482276585122,
      "grad_norm": 0.2721194326877594,
      "learning_rate": 0.0004732151772341488,
      "loss": 0.0037,
      "step": 5365
    },
    {
      "epoch": 2.6789815277084372,
      "grad_norm": 0.18508818745613098,
      "learning_rate": 0.0004732101847229157,
      "loss": 0.0027,
      "step": 5366
    },
    {
      "epoch": 2.6794807788317523,
      "grad_norm": 0.45592615008354187,
      "learning_rate": 0.00047320519221168253,
      "loss": 0.0069,
      "step": 5367
    },
    {
      "epoch": 2.6799800299550673,
      "grad_norm": 0.24087946116924286,
      "learning_rate": 0.0004732001997004494,
      "loss": 0.0085,
      "step": 5368
    },
    {
      "epoch": 2.6804792810783824,
      "grad_norm": 0.014816191047430038,
      "learning_rate": 0.0004731952071892162,
      "loss": 0.0007,
      "step": 5369
    },
    {
      "epoch": 2.6809785322016975,
      "grad_norm": 0.025365130975842476,
      "learning_rate": 0.00047319021467798303,
      "loss": 0.0008,
      "step": 5370
    },
    {
      "epoch": 2.6814777833250125,
      "grad_norm": 0.03819117695093155,
      "learning_rate": 0.0004731852221667499,
      "loss": 0.001,
      "step": 5371
    },
    {
      "epoch": 2.6819770344483276,
      "grad_norm": 0.028813032433390617,
      "learning_rate": 0.00047318022965551674,
      "loss": 0.0008,
      "step": 5372
    },
    {
      "epoch": 2.6824762855716426,
      "grad_norm": 0.019852465018630028,
      "learning_rate": 0.0004731752371442836,
      "loss": 0.0006,
      "step": 5373
    },
    {
      "epoch": 2.6829755366949577,
      "grad_norm": 0.2473919540643692,
      "learning_rate": 0.00047317024463305044,
      "loss": 0.0021,
      "step": 5374
    },
    {
      "epoch": 2.6834747878182728,
      "grad_norm": 0.3272455334663391,
      "learning_rate": 0.0004731652521218173,
      "loss": 0.0054,
      "step": 5375
    },
    {
      "epoch": 2.683974038941588,
      "grad_norm": 0.07169594615697861,
      "learning_rate": 0.00047316025961058415,
      "loss": 0.001,
      "step": 5376
    },
    {
      "epoch": 2.6844732900649024,
      "grad_norm": 0.03121541254222393,
      "learning_rate": 0.000473155267099351,
      "loss": 0.001,
      "step": 5377
    },
    {
      "epoch": 2.684972541188218,
      "grad_norm": 0.08814883977174759,
      "learning_rate": 0.00047315027458811785,
      "loss": 0.0014,
      "step": 5378
    },
    {
      "epoch": 2.6854717923115325,
      "grad_norm": 0.11882758885622025,
      "learning_rate": 0.0004731452820768847,
      "loss": 0.0017,
      "step": 5379
    },
    {
      "epoch": 2.6859710434348476,
      "grad_norm": 0.02253727614879608,
      "learning_rate": 0.00047314028956565155,
      "loss": 0.0009,
      "step": 5380
    },
    {
      "epoch": 2.6864702945581627,
      "grad_norm": 0.20779511332511902,
      "learning_rate": 0.0004731352970544184,
      "loss": 0.0039,
      "step": 5381
    },
    {
      "epoch": 2.6869695456814777,
      "grad_norm": 0.0450175441801548,
      "learning_rate": 0.0004731303045431852,
      "loss": 0.0013,
      "step": 5382
    },
    {
      "epoch": 2.687468796804793,
      "grad_norm": 0.03724401816725731,
      "learning_rate": 0.00047312531203195206,
      "loss": 0.0011,
      "step": 5383
    },
    {
      "epoch": 2.687968047928108,
      "grad_norm": 0.49122145771980286,
      "learning_rate": 0.0004731203195207189,
      "loss": 0.0073,
      "step": 5384
    },
    {
      "epoch": 2.688467299051423,
      "grad_norm": 0.15170659124851227,
      "learning_rate": 0.00047311532700948576,
      "loss": 0.0174,
      "step": 5385
    },
    {
      "epoch": 2.688966550174738,
      "grad_norm": 0.15990309417247772,
      "learning_rate": 0.0004731103344982526,
      "loss": 0.0026,
      "step": 5386
    },
    {
      "epoch": 2.689465801298053,
      "grad_norm": 0.39688876271247864,
      "learning_rate": 0.00047310534198701946,
      "loss": 0.0038,
      "step": 5387
    },
    {
      "epoch": 2.689965052421368,
      "grad_norm": 0.03048555925488472,
      "learning_rate": 0.0004731003494757863,
      "loss": 0.0005,
      "step": 5388
    },
    {
      "epoch": 2.690464303544683,
      "grad_norm": 0.1863551288843155,
      "learning_rate": 0.00047309535696455317,
      "loss": 0.0011,
      "step": 5389
    },
    {
      "epoch": 2.6909635546679977,
      "grad_norm": 0.4525885581970215,
      "learning_rate": 0.00047309036445332,
      "loss": 0.0066,
      "step": 5390
    },
    {
      "epoch": 2.6914628057913133,
      "grad_norm": 0.06324736028909683,
      "learning_rate": 0.00047308537194208687,
      "loss": 0.0008,
      "step": 5391
    },
    {
      "epoch": 2.691962056914628,
      "grad_norm": 0.010427680797874928,
      "learning_rate": 0.0004730803794308537,
      "loss": 0.0005,
      "step": 5392
    },
    {
      "epoch": 2.692461308037943,
      "grad_norm": 0.02589513175189495,
      "learning_rate": 0.0004730753869196206,
      "loss": 0.0009,
      "step": 5393
    },
    {
      "epoch": 2.692960559161258,
      "grad_norm": 0.030582288280129433,
      "learning_rate": 0.00047307039440838743,
      "loss": 0.001,
      "step": 5394
    },
    {
      "epoch": 2.693459810284573,
      "grad_norm": 0.3328228294849396,
      "learning_rate": 0.0004730654018971543,
      "loss": 0.0043,
      "step": 5395
    },
    {
      "epoch": 2.693959061407888,
      "grad_norm": 0.025116702541708946,
      "learning_rate": 0.00047306040938592113,
      "loss": 0.0008,
      "step": 5396
    },
    {
      "epoch": 2.694458312531203,
      "grad_norm": 0.02712772786617279,
      "learning_rate": 0.000473055416874688,
      "loss": 0.0008,
      "step": 5397
    },
    {
      "epoch": 2.694957563654518,
      "grad_norm": 0.1675771325826645,
      "learning_rate": 0.0004730504243634548,
      "loss": 0.002,
      "step": 5398
    },
    {
      "epoch": 2.6954568147778333,
      "grad_norm": 0.03776855021715164,
      "learning_rate": 0.00047304543185222163,
      "loss": 0.0008,
      "step": 5399
    },
    {
      "epoch": 2.6959560659011483,
      "grad_norm": 0.07650831341743469,
      "learning_rate": 0.0004730404393409885,
      "loss": 0.0014,
      "step": 5400
    },
    {
      "epoch": 2.6964553170244634,
      "grad_norm": 0.09071941673755646,
      "learning_rate": 0.00047303544682975534,
      "loss": 0.0012,
      "step": 5401
    },
    {
      "epoch": 2.6969545681477785,
      "grad_norm": 0.5188087821006775,
      "learning_rate": 0.0004730304543185222,
      "loss": 0.0143,
      "step": 5402
    },
    {
      "epoch": 2.6974538192710935,
      "grad_norm": 0.5250565409660339,
      "learning_rate": 0.00047302546180728904,
      "loss": 0.0084,
      "step": 5403
    },
    {
      "epoch": 2.6979530703944086,
      "grad_norm": 0.02192259021103382,
      "learning_rate": 0.0004730204692960559,
      "loss": 0.0007,
      "step": 5404
    },
    {
      "epoch": 2.698452321517723,
      "grad_norm": 0.05914798378944397,
      "learning_rate": 0.00047301547678482275,
      "loss": 0.0014,
      "step": 5405
    },
    {
      "epoch": 2.6989515726410387,
      "grad_norm": 0.02608649805188179,
      "learning_rate": 0.0004730104842735896,
      "loss": 0.0006,
      "step": 5406
    },
    {
      "epoch": 2.6994508237643533,
      "grad_norm": 0.18708480894565582,
      "learning_rate": 0.00047300549176235645,
      "loss": 0.0023,
      "step": 5407
    },
    {
      "epoch": 2.6999500748876684,
      "grad_norm": 0.5856794118881226,
      "learning_rate": 0.0004730004992511233,
      "loss": 0.0123,
      "step": 5408
    },
    {
      "epoch": 2.7004493260109834,
      "grad_norm": 0.12854240834712982,
      "learning_rate": 0.00047299550673989016,
      "loss": 0.0011,
      "step": 5409
    },
    {
      "epoch": 2.7009485771342985,
      "grad_norm": 0.021045546978712082,
      "learning_rate": 0.000472990514228657,
      "loss": 0.0008,
      "step": 5410
    },
    {
      "epoch": 2.7014478282576135,
      "grad_norm": 0.07963094860315323,
      "learning_rate": 0.00047298552171742386,
      "loss": 0.0008,
      "step": 5411
    },
    {
      "epoch": 2.7019470793809286,
      "grad_norm": 0.014082500711083412,
      "learning_rate": 0.0004729805292061907,
      "loss": 0.0005,
      "step": 5412
    },
    {
      "epoch": 2.7024463305042437,
      "grad_norm": 0.035188887268304825,
      "learning_rate": 0.00047297553669495756,
      "loss": 0.0008,
      "step": 5413
    },
    {
      "epoch": 2.7029455816275587,
      "grad_norm": 0.24930019676685333,
      "learning_rate": 0.0004729705441837244,
      "loss": 0.0023,
      "step": 5414
    },
    {
      "epoch": 2.7034448327508738,
      "grad_norm": 0.0225918460637331,
      "learning_rate": 0.00047296555167249127,
      "loss": 0.0005,
      "step": 5415
    },
    {
      "epoch": 2.703944083874189,
      "grad_norm": 0.08211253583431244,
      "learning_rate": 0.0004729605591612581,
      "loss": 0.0017,
      "step": 5416
    },
    {
      "epoch": 2.704443334997504,
      "grad_norm": 0.028417525812983513,
      "learning_rate": 0.00047295556665002497,
      "loss": 0.0008,
      "step": 5417
    },
    {
      "epoch": 2.7049425861208185,
      "grad_norm": 0.030120622366666794,
      "learning_rate": 0.0004729505741387918,
      "loss": 0.0006,
      "step": 5418
    },
    {
      "epoch": 2.705441837244134,
      "grad_norm": 0.044007930904626846,
      "learning_rate": 0.0004729455816275587,
      "loss": 0.0007,
      "step": 5419
    },
    {
      "epoch": 2.7059410883674486,
      "grad_norm": 0.5320612788200378,
      "learning_rate": 0.00047294058911632553,
      "loss": 0.0117,
      "step": 5420
    },
    {
      "epoch": 2.706440339490764,
      "grad_norm": 0.23629961907863617,
      "learning_rate": 0.0004729355966050924,
      "loss": 0.0027,
      "step": 5421
    },
    {
      "epoch": 2.7069395906140787,
      "grad_norm": 0.004286929499357939,
      "learning_rate": 0.00047293060409385923,
      "loss": 0.0003,
      "step": 5422
    },
    {
      "epoch": 2.707438841737394,
      "grad_norm": 0.04168933257460594,
      "learning_rate": 0.0004729256115826261,
      "loss": 0.0007,
      "step": 5423
    },
    {
      "epoch": 2.707938092860709,
      "grad_norm": 0.3911387324333191,
      "learning_rate": 0.00047292061907139294,
      "loss": 0.0051,
      "step": 5424
    },
    {
      "epoch": 2.708437343984024,
      "grad_norm": 0.2603537440299988,
      "learning_rate": 0.0004729156265601598,
      "loss": 0.0024,
      "step": 5425
    },
    {
      "epoch": 2.708936595107339,
      "grad_norm": 0.22231422364711761,
      "learning_rate": 0.00047291063404892664,
      "loss": 0.0154,
      "step": 5426
    },
    {
      "epoch": 2.709435846230654,
      "grad_norm": 0.01908683590590954,
      "learning_rate": 0.00047290564153769344,
      "loss": 0.0004,
      "step": 5427
    },
    {
      "epoch": 2.709935097353969,
      "grad_norm": 0.21487067639827728,
      "learning_rate": 0.0004729006490264603,
      "loss": 0.0043,
      "step": 5428
    },
    {
      "epoch": 2.710434348477284,
      "grad_norm": 0.023066548630595207,
      "learning_rate": 0.00047289565651522714,
      "loss": 0.0005,
      "step": 5429
    },
    {
      "epoch": 2.710933599600599,
      "grad_norm": 0.007299803663045168,
      "learning_rate": 0.000472890664003994,
      "loss": 0.0004,
      "step": 5430
    },
    {
      "epoch": 2.7114328507239143,
      "grad_norm": 0.012824882753193378,
      "learning_rate": 0.00047288567149276085,
      "loss": 0.0007,
      "step": 5431
    },
    {
      "epoch": 2.7119321018472293,
      "grad_norm": 0.007105374243110418,
      "learning_rate": 0.0004728806789815277,
      "loss": 0.0005,
      "step": 5432
    },
    {
      "epoch": 2.712431352970544,
      "grad_norm": 0.07245020568370819,
      "learning_rate": 0.00047287568647029455,
      "loss": 0.0016,
      "step": 5433
    },
    {
      "epoch": 2.7129306040938594,
      "grad_norm": 0.006060225889086723,
      "learning_rate": 0.0004728706939590614,
      "loss": 0.0004,
      "step": 5434
    },
    {
      "epoch": 2.713429855217174,
      "grad_norm": 0.10024730116128922,
      "learning_rate": 0.00047286570144782825,
      "loss": 0.0016,
      "step": 5435
    },
    {
      "epoch": 2.713929106340489,
      "grad_norm": 0.017513995990157127,
      "learning_rate": 0.0004728607089365951,
      "loss": 0.0006,
      "step": 5436
    },
    {
      "epoch": 2.714428357463804,
      "grad_norm": 0.6083012223243713,
      "learning_rate": 0.00047285571642536196,
      "loss": 0.0199,
      "step": 5437
    },
    {
      "epoch": 2.7149276085871192,
      "grad_norm": 0.04048500210046768,
      "learning_rate": 0.0004728507239141288,
      "loss": 0.0008,
      "step": 5438
    },
    {
      "epoch": 2.7154268597104343,
      "grad_norm": 0.010386277921497822,
      "learning_rate": 0.00047284573140289566,
      "loss": 0.0006,
      "step": 5439
    },
    {
      "epoch": 2.7159261108337494,
      "grad_norm": 0.009824056178331375,
      "learning_rate": 0.0004728407388916625,
      "loss": 0.0006,
      "step": 5440
    },
    {
      "epoch": 2.7164253619570644,
      "grad_norm": 0.006112548988312483,
      "learning_rate": 0.00047283574638042937,
      "loss": 0.0005,
      "step": 5441
    },
    {
      "epoch": 2.7169246130803795,
      "grad_norm": 0.010562779381871223,
      "learning_rate": 0.0004728307538691962,
      "loss": 0.0006,
      "step": 5442
    },
    {
      "epoch": 2.7174238642036945,
      "grad_norm": 0.08955693989992142,
      "learning_rate": 0.00047282576135796307,
      "loss": 0.0013,
      "step": 5443
    },
    {
      "epoch": 2.7179231153270096,
      "grad_norm": 0.11777406930923462,
      "learning_rate": 0.0004728207688467299,
      "loss": 0.0015,
      "step": 5444
    },
    {
      "epoch": 2.7184223664503246,
      "grad_norm": 0.20591485500335693,
      "learning_rate": 0.0004728157763354968,
      "loss": 0.0021,
      "step": 5445
    },
    {
      "epoch": 2.7189216175736397,
      "grad_norm": 0.37357184290885925,
      "learning_rate": 0.00047281078382426363,
      "loss": 0.006,
      "step": 5446
    },
    {
      "epoch": 2.7194208686969548,
      "grad_norm": 0.01644713245332241,
      "learning_rate": 0.0004728057913130305,
      "loss": 0.0007,
      "step": 5447
    },
    {
      "epoch": 2.7199201198202694,
      "grad_norm": 0.18389767408370972,
      "learning_rate": 0.00047280079880179733,
      "loss": 0.002,
      "step": 5448
    },
    {
      "epoch": 2.720419370943585,
      "grad_norm": 0.029840221628546715,
      "learning_rate": 0.0004727958062905642,
      "loss": 0.0006,
      "step": 5449
    },
    {
      "epoch": 2.7209186220668995,
      "grad_norm": 0.1213688850402832,
      "learning_rate": 0.00047279081377933104,
      "loss": 0.0013,
      "step": 5450
    },
    {
      "epoch": 2.7214178731902146,
      "grad_norm": 0.4144822657108307,
      "learning_rate": 0.0004727858212680979,
      "loss": 0.0139,
      "step": 5451
    },
    {
      "epoch": 2.7219171243135296,
      "grad_norm": 0.13841001689434052,
      "learning_rate": 0.00047278082875686474,
      "loss": 0.0022,
      "step": 5452
    },
    {
      "epoch": 2.7224163754368447,
      "grad_norm": 0.31441015005111694,
      "learning_rate": 0.0004727758362456316,
      "loss": 0.0239,
      "step": 5453
    },
    {
      "epoch": 2.7229156265601597,
      "grad_norm": 0.48954036831855774,
      "learning_rate": 0.00047277084373439844,
      "loss": 0.0109,
      "step": 5454
    },
    {
      "epoch": 2.723414877683475,
      "grad_norm": 0.03949642926454544,
      "learning_rate": 0.0004727658512231653,
      "loss": 0.001,
      "step": 5455
    },
    {
      "epoch": 2.72391412880679,
      "grad_norm": 0.11192741990089417,
      "learning_rate": 0.0004727608587119321,
      "loss": 0.0017,
      "step": 5456
    },
    {
      "epoch": 2.724413379930105,
      "grad_norm": 0.038291774690151215,
      "learning_rate": 0.00047275586620069895,
      "loss": 0.0005,
      "step": 5457
    },
    {
      "epoch": 2.72491263105342,
      "grad_norm": 0.10681794583797455,
      "learning_rate": 0.0004727508736894658,
      "loss": 0.0007,
      "step": 5458
    },
    {
      "epoch": 2.725411882176735,
      "grad_norm": 0.047748252749443054,
      "learning_rate": 0.00047274588117823265,
      "loss": 0.0007,
      "step": 5459
    },
    {
      "epoch": 2.72591113330005,
      "grad_norm": 0.179946631193161,
      "learning_rate": 0.0004727408886669995,
      "loss": 0.002,
      "step": 5460
    },
    {
      "epoch": 2.7264103844233647,
      "grad_norm": 0.47722503542900085,
      "learning_rate": 0.00047273589615576635,
      "loss": 0.0035,
      "step": 5461
    },
    {
      "epoch": 2.72690963554668,
      "grad_norm": 0.07751147449016571,
      "learning_rate": 0.0004727309036445332,
      "loss": 0.001,
      "step": 5462
    },
    {
      "epoch": 2.727408886669995,
      "grad_norm": 0.0036236592568457127,
      "learning_rate": 0.00047272591113330006,
      "loss": 0.0003,
      "step": 5463
    },
    {
      "epoch": 2.72790813779331,
      "grad_norm": 0.0307892095297575,
      "learning_rate": 0.0004727209186220669,
      "loss": 0.0005,
      "step": 5464
    },
    {
      "epoch": 2.728407388916625,
      "grad_norm": 0.08465414494276047,
      "learning_rate": 0.00047271592611083376,
      "loss": 0.0013,
      "step": 5465
    },
    {
      "epoch": 2.72890664003994,
      "grad_norm": 0.02473568357527256,
      "learning_rate": 0.0004727109335996006,
      "loss": 0.0004,
      "step": 5466
    },
    {
      "epoch": 2.729405891163255,
      "grad_norm": 0.032989319413900375,
      "learning_rate": 0.00047270594108836747,
      "loss": 0.0006,
      "step": 5467
    },
    {
      "epoch": 2.72990514228657,
      "grad_norm": 0.056898411363363266,
      "learning_rate": 0.0004727009485771343,
      "loss": 0.0008,
      "step": 5468
    },
    {
      "epoch": 2.730404393409885,
      "grad_norm": 0.01996525749564171,
      "learning_rate": 0.00047269595606590117,
      "loss": 0.0005,
      "step": 5469
    },
    {
      "epoch": 2.7309036445332002,
      "grad_norm": 0.048511479049921036,
      "learning_rate": 0.000472690963554668,
      "loss": 0.0007,
      "step": 5470
    },
    {
      "epoch": 2.7314028956565153,
      "grad_norm": 0.049004677683115005,
      "learning_rate": 0.0004726859710434349,
      "loss": 0.001,
      "step": 5471
    },
    {
      "epoch": 2.7319021467798303,
      "grad_norm": 0.05513085797429085,
      "learning_rate": 0.0004726809785322017,
      "loss": 0.0007,
      "step": 5472
    },
    {
      "epoch": 2.7324013979031454,
      "grad_norm": 0.4807490110397339,
      "learning_rate": 0.0004726759860209686,
      "loss": 0.0039,
      "step": 5473
    },
    {
      "epoch": 2.7329006490264605,
      "grad_norm": 0.4319083094596863,
      "learning_rate": 0.00047267099350973543,
      "loss": 0.0016,
      "step": 5474
    },
    {
      "epoch": 2.7333999001497755,
      "grad_norm": 1.4266326427459717,
      "learning_rate": 0.0004726660009985023,
      "loss": 0.0275,
      "step": 5475
    },
    {
      "epoch": 2.73389915127309,
      "grad_norm": 0.09459344297647476,
      "learning_rate": 0.00047266100848726914,
      "loss": 0.0012,
      "step": 5476
    },
    {
      "epoch": 2.7343984023964056,
      "grad_norm": 0.2210223227739334,
      "learning_rate": 0.000472656015976036,
      "loss": 0.0013,
      "step": 5477
    },
    {
      "epoch": 2.7348976535197203,
      "grad_norm": 0.004581082612276077,
      "learning_rate": 0.00047265102346480284,
      "loss": 0.0003,
      "step": 5478
    },
    {
      "epoch": 2.7353969046430353,
      "grad_norm": 1.3752772808074951,
      "learning_rate": 0.0004726460309535697,
      "loss": 0.0145,
      "step": 5479
    },
    {
      "epoch": 2.7358961557663504,
      "grad_norm": 0.30356350541114807,
      "learning_rate": 0.00047264103844233654,
      "loss": 0.0099,
      "step": 5480
    },
    {
      "epoch": 2.7363954068896654,
      "grad_norm": 0.5889049768447876,
      "learning_rate": 0.0004726360459311034,
      "loss": 0.0083,
      "step": 5481
    },
    {
      "epoch": 2.7368946580129805,
      "grad_norm": 0.2204001396894455,
      "learning_rate": 0.00047263105341987025,
      "loss": 0.0018,
      "step": 5482
    },
    {
      "epoch": 2.7373939091362955,
      "grad_norm": 0.28304529190063477,
      "learning_rate": 0.0004726260609086371,
      "loss": 0.0021,
      "step": 5483
    },
    {
      "epoch": 2.7378931602596106,
      "grad_norm": 0.552166759967804,
      "learning_rate": 0.00047262106839740395,
      "loss": 0.0034,
      "step": 5484
    },
    {
      "epoch": 2.7383924113829257,
      "grad_norm": 1.0755565166473389,
      "learning_rate": 0.00047261607588617075,
      "loss": 0.0332,
      "step": 5485
    },
    {
      "epoch": 2.7388916625062407,
      "grad_norm": 0.28335079550743103,
      "learning_rate": 0.0004726110833749376,
      "loss": 0.0015,
      "step": 5486
    },
    {
      "epoch": 2.739390913629556,
      "grad_norm": 0.4190303683280945,
      "learning_rate": 0.00047260609086370445,
      "loss": 0.0038,
      "step": 5487
    },
    {
      "epoch": 2.739890164752871,
      "grad_norm": 0.43783247470855713,
      "learning_rate": 0.0004726010983524713,
      "loss": 0.0141,
      "step": 5488
    },
    {
      "epoch": 2.7403894158761855,
      "grad_norm": 0.014104830101132393,
      "learning_rate": 0.00047259610584123816,
      "loss": 0.0005,
      "step": 5489
    },
    {
      "epoch": 2.740888666999501,
      "grad_norm": 0.4474290907382965,
      "learning_rate": 0.000472591113330005,
      "loss": 0.0109,
      "step": 5490
    },
    {
      "epoch": 2.7413879181228156,
      "grad_norm": 0.361725777387619,
      "learning_rate": 0.00047258612081877186,
      "loss": 0.002,
      "step": 5491
    },
    {
      "epoch": 2.741887169246131,
      "grad_norm": 0.02575407177209854,
      "learning_rate": 0.0004725811283075387,
      "loss": 0.0007,
      "step": 5492
    },
    {
      "epoch": 2.7423864203694457,
      "grad_norm": 0.003323795273900032,
      "learning_rate": 0.00047257613579630557,
      "loss": 0.0003,
      "step": 5493
    },
    {
      "epoch": 2.7428856714927607,
      "grad_norm": 0.020457694306969643,
      "learning_rate": 0.0004725711432850724,
      "loss": 0.0008,
      "step": 5494
    },
    {
      "epoch": 2.743384922616076,
      "grad_norm": 1.0917147397994995,
      "learning_rate": 0.0004725661507738392,
      "loss": 0.038,
      "step": 5495
    },
    {
      "epoch": 2.743884173739391,
      "grad_norm": 0.2546635568141937,
      "learning_rate": 0.00047256115826260607,
      "loss": 0.0061,
      "step": 5496
    },
    {
      "epoch": 2.744383424862706,
      "grad_norm": 0.5029565095901489,
      "learning_rate": 0.0004725561657513729,
      "loss": 0.0119,
      "step": 5497
    },
    {
      "epoch": 2.744882675986021,
      "grad_norm": 0.3884676992893219,
      "learning_rate": 0.00047255117324013977,
      "loss": 0.0046,
      "step": 5498
    },
    {
      "epoch": 2.745381927109336,
      "grad_norm": 0.11993004381656647,
      "learning_rate": 0.0004725461807289066,
      "loss": 0.0021,
      "step": 5499
    },
    {
      "epoch": 2.745881178232651,
      "grad_norm": 0.6923837661743164,
      "learning_rate": 0.0004725411882176735,
      "loss": 0.0071,
      "step": 5500
    },
    {
      "epoch": 2.746380429355966,
      "grad_norm": 0.028799492865800858,
      "learning_rate": 0.00047253619570644033,
      "loss": 0.0007,
      "step": 5501
    },
    {
      "epoch": 2.746879680479281,
      "grad_norm": 0.5926933288574219,
      "learning_rate": 0.0004725312031952072,
      "loss": 0.02,
      "step": 5502
    },
    {
      "epoch": 2.7473789316025963,
      "grad_norm": 0.1083400696516037,
      "learning_rate": 0.00047252621068397403,
      "loss": 0.0017,
      "step": 5503
    },
    {
      "epoch": 2.747878182725911,
      "grad_norm": 0.31429192423820496,
      "learning_rate": 0.0004725212181727409,
      "loss": 0.0174,
      "step": 5504
    },
    {
      "epoch": 2.7483774338492264,
      "grad_norm": 0.14691992104053497,
      "learning_rate": 0.00047251622566150774,
      "loss": 0.0019,
      "step": 5505
    },
    {
      "epoch": 2.748876684972541,
      "grad_norm": 0.08374606817960739,
      "learning_rate": 0.0004725112331502746,
      "loss": 0.0011,
      "step": 5506
    },
    {
      "epoch": 2.749375936095856,
      "grad_norm": 0.15660107135772705,
      "learning_rate": 0.00047250624063904144,
      "loss": 0.0026,
      "step": 5507
    },
    {
      "epoch": 2.749875187219171,
      "grad_norm": 0.11804503202438354,
      "learning_rate": 0.0004725012481278083,
      "loss": 0.0019,
      "step": 5508
    },
    {
      "epoch": 2.750374438342486,
      "grad_norm": 0.1336664855480194,
      "learning_rate": 0.00047249625561657515,
      "loss": 0.0039,
      "step": 5509
    },
    {
      "epoch": 2.7508736894658012,
      "grad_norm": 2.322418689727783,
      "learning_rate": 0.000472491263105342,
      "loss": 0.029,
      "step": 5510
    },
    {
      "epoch": 2.7513729405891163,
      "grad_norm": 0.6687188744544983,
      "learning_rate": 0.00047248627059410885,
      "loss": 0.006,
      "step": 5511
    },
    {
      "epoch": 2.7518721917124314,
      "grad_norm": 0.4385755658149719,
      "learning_rate": 0.0004724812780828757,
      "loss": 0.0264,
      "step": 5512
    },
    {
      "epoch": 2.7523714428357464,
      "grad_norm": 0.07922317832708359,
      "learning_rate": 0.00047247628557164255,
      "loss": 0.0019,
      "step": 5513
    },
    {
      "epoch": 2.7528706939590615,
      "grad_norm": 0.6258156299591064,
      "learning_rate": 0.00047247129306040935,
      "loss": 0.0132,
      "step": 5514
    },
    {
      "epoch": 2.7533699450823765,
      "grad_norm": 0.09715666621923447,
      "learning_rate": 0.0004724663005491762,
      "loss": 0.001,
      "step": 5515
    },
    {
      "epoch": 2.7538691962056916,
      "grad_norm": 0.16548939049243927,
      "learning_rate": 0.00047246130803794306,
      "loss": 0.0023,
      "step": 5516
    },
    {
      "epoch": 2.7543684473290067,
      "grad_norm": 0.5547792315483093,
      "learning_rate": 0.0004724563155267099,
      "loss": 0.0048,
      "step": 5517
    },
    {
      "epoch": 2.7548676984523217,
      "grad_norm": 0.02540118433535099,
      "learning_rate": 0.00047245132301547676,
      "loss": 0.0007,
      "step": 5518
    },
    {
      "epoch": 2.7553669495756363,
      "grad_norm": 1.073039174079895,
      "learning_rate": 0.0004724463305042436,
      "loss": 0.0104,
      "step": 5519
    },
    {
      "epoch": 2.755866200698952,
      "grad_norm": 0.20943766832351685,
      "learning_rate": 0.00047244133799301046,
      "loss": 0.0031,
      "step": 5520
    },
    {
      "epoch": 2.7563654518222664,
      "grad_norm": 0.8430646657943726,
      "learning_rate": 0.0004724363454817773,
      "loss": 0.0052,
      "step": 5521
    },
    {
      "epoch": 2.7568647029455815,
      "grad_norm": 0.3948570191860199,
      "learning_rate": 0.00047243135297054417,
      "loss": 0.0074,
      "step": 5522
    },
    {
      "epoch": 2.7573639540688966,
      "grad_norm": 0.17274628579616547,
      "learning_rate": 0.000472426360459311,
      "loss": 0.0033,
      "step": 5523
    },
    {
      "epoch": 2.7578632051922116,
      "grad_norm": 0.5528140664100647,
      "learning_rate": 0.00047242136794807787,
      "loss": 0.0123,
      "step": 5524
    },
    {
      "epoch": 2.7583624563155267,
      "grad_norm": 0.35320818424224854,
      "learning_rate": 0.0004724163754368447,
      "loss": 0.0023,
      "step": 5525
    },
    {
      "epoch": 2.7588617074388417,
      "grad_norm": 0.0997210219502449,
      "learning_rate": 0.0004724113829256116,
      "loss": 0.0029,
      "step": 5526
    },
    {
      "epoch": 2.759360958562157,
      "grad_norm": 0.01575964130461216,
      "learning_rate": 0.00047240639041437843,
      "loss": 0.0007,
      "step": 5527
    },
    {
      "epoch": 2.759860209685472,
      "grad_norm": 0.31219911575317383,
      "learning_rate": 0.0004724013979031453,
      "loss": 0.0068,
      "step": 5528
    },
    {
      "epoch": 2.760359460808787,
      "grad_norm": 0.15070916712284088,
      "learning_rate": 0.00047239640539191213,
      "loss": 0.0013,
      "step": 5529
    },
    {
      "epoch": 2.760858711932102,
      "grad_norm": 0.014807592146098614,
      "learning_rate": 0.000472391412880679,
      "loss": 0.0008,
      "step": 5530
    },
    {
      "epoch": 2.761357963055417,
      "grad_norm": 0.40190327167510986,
      "learning_rate": 0.00047238642036944584,
      "loss": 0.0068,
      "step": 5531
    },
    {
      "epoch": 2.7618572141787316,
      "grad_norm": 0.07690117508172989,
      "learning_rate": 0.0004723814278582127,
      "loss": 0.0016,
      "step": 5532
    },
    {
      "epoch": 2.762356465302047,
      "grad_norm": 0.04126689210534096,
      "learning_rate": 0.00047237643534697954,
      "loss": 0.0011,
      "step": 5533
    },
    {
      "epoch": 2.7628557164253618,
      "grad_norm": 0.5409407615661621,
      "learning_rate": 0.0004723714428357464,
      "loss": 0.0087,
      "step": 5534
    },
    {
      "epoch": 2.763354967548677,
      "grad_norm": 0.4620733857154846,
      "learning_rate": 0.00047236645032451324,
      "loss": 0.0127,
      "step": 5535
    },
    {
      "epoch": 2.763854218671992,
      "grad_norm": 0.6748636364936829,
      "learning_rate": 0.0004723614578132801,
      "loss": 0.0152,
      "step": 5536
    },
    {
      "epoch": 2.764353469795307,
      "grad_norm": 0.013850736431777477,
      "learning_rate": 0.00047235646530204695,
      "loss": 0.0006,
      "step": 5537
    },
    {
      "epoch": 2.764852720918622,
      "grad_norm": 3.015878200531006,
      "learning_rate": 0.0004723514727908138,
      "loss": 0.0256,
      "step": 5538
    },
    {
      "epoch": 2.765351972041937,
      "grad_norm": 0.5423363447189331,
      "learning_rate": 0.00047234648027958065,
      "loss": 0.004,
      "step": 5539
    },
    {
      "epoch": 2.765851223165252,
      "grad_norm": 0.14121393859386444,
      "learning_rate": 0.0004723414877683475,
      "loss": 0.003,
      "step": 5540
    },
    {
      "epoch": 2.766350474288567,
      "grad_norm": 0.3048514425754547,
      "learning_rate": 0.00047233649525711436,
      "loss": 0.0044,
      "step": 5541
    },
    {
      "epoch": 2.7668497254118822,
      "grad_norm": 0.06588311493396759,
      "learning_rate": 0.0004723315027458812,
      "loss": 0.0017,
      "step": 5542
    },
    {
      "epoch": 2.7673489765351973,
      "grad_norm": 0.10821733623743057,
      "learning_rate": 0.000472326510234648,
      "loss": 0.0017,
      "step": 5543
    },
    {
      "epoch": 2.7678482276585124,
      "grad_norm": 0.10638657212257385,
      "learning_rate": 0.00047232151772341486,
      "loss": 0.0016,
      "step": 5544
    },
    {
      "epoch": 2.7683474787818274,
      "grad_norm": 0.03723412752151489,
      "learning_rate": 0.0004723165252121817,
      "loss": 0.0011,
      "step": 5545
    },
    {
      "epoch": 2.7688467299051425,
      "grad_norm": 0.05343814566731453,
      "learning_rate": 0.00047231153270094856,
      "loss": 0.0015,
      "step": 5546
    },
    {
      "epoch": 2.769345981028457,
      "grad_norm": 0.06553354859352112,
      "learning_rate": 0.0004723065401897154,
      "loss": 0.0018,
      "step": 5547
    },
    {
      "epoch": 2.7698452321517726,
      "grad_norm": 0.021317537873983383,
      "learning_rate": 0.00047230154767848227,
      "loss": 0.0008,
      "step": 5548
    },
    {
      "epoch": 2.770344483275087,
      "grad_norm": 0.5330188870429993,
      "learning_rate": 0.0004722965551672491,
      "loss": 0.0028,
      "step": 5549
    },
    {
      "epoch": 2.7708437343984023,
      "grad_norm": 0.3555527329444885,
      "learning_rate": 0.00047229156265601597,
      "loss": 0.0122,
      "step": 5550
    },
    {
      "epoch": 2.7713429855217173,
      "grad_norm": 0.9912472367286682,
      "learning_rate": 0.0004722865701447828,
      "loss": 0.0097,
      "step": 5551
    },
    {
      "epoch": 2.7718422366450324,
      "grad_norm": 0.15457803010940552,
      "learning_rate": 0.0004722815776335497,
      "loss": 0.0026,
      "step": 5552
    },
    {
      "epoch": 2.7723414877683474,
      "grad_norm": 0.012378694489598274,
      "learning_rate": 0.00047227658512231653,
      "loss": 0.0006,
      "step": 5553
    },
    {
      "epoch": 2.7728407388916625,
      "grad_norm": 0.04724667966365814,
      "learning_rate": 0.0004722715926110834,
      "loss": 0.001,
      "step": 5554
    },
    {
      "epoch": 2.7733399900149776,
      "grad_norm": 0.35991600155830383,
      "learning_rate": 0.00047226660009985023,
      "loss": 0.0042,
      "step": 5555
    },
    {
      "epoch": 2.7738392411382926,
      "grad_norm": 0.061960406601428986,
      "learning_rate": 0.0004722616075886171,
      "loss": 0.0012,
      "step": 5556
    },
    {
      "epoch": 2.7743384922616077,
      "grad_norm": 0.20026730000972748,
      "learning_rate": 0.00047225661507738394,
      "loss": 0.0036,
      "step": 5557
    },
    {
      "epoch": 2.7748377433849227,
      "grad_norm": 0.1103619709610939,
      "learning_rate": 0.0004722516225661508,
      "loss": 0.0014,
      "step": 5558
    },
    {
      "epoch": 2.775336994508238,
      "grad_norm": 0.1087251678109169,
      "learning_rate": 0.00047224663005491764,
      "loss": 0.0011,
      "step": 5559
    },
    {
      "epoch": 2.7758362456315524,
      "grad_norm": 0.1926857978105545,
      "learning_rate": 0.0004722416375436845,
      "loss": 0.0021,
      "step": 5560
    },
    {
      "epoch": 2.776335496754868,
      "grad_norm": 0.4222017526626587,
      "learning_rate": 0.00047223664503245134,
      "loss": 0.0055,
      "step": 5561
    },
    {
      "epoch": 2.7768347478781825,
      "grad_norm": 0.008117766119539738,
      "learning_rate": 0.0004722316525212182,
      "loss": 0.0005,
      "step": 5562
    },
    {
      "epoch": 2.777333999001498,
      "grad_norm": 0.029619919136166573,
      "learning_rate": 0.00047222666000998505,
      "loss": 0.0007,
      "step": 5563
    },
    {
      "epoch": 2.7778332501248126,
      "grad_norm": 0.018333839252591133,
      "learning_rate": 0.0004722216674987519,
      "loss": 0.0007,
      "step": 5564
    },
    {
      "epoch": 2.7783325012481277,
      "grad_norm": 0.01336299441754818,
      "learning_rate": 0.00047221667498751875,
      "loss": 0.0006,
      "step": 5565
    },
    {
      "epoch": 2.7788317523714428,
      "grad_norm": 0.12219623476266861,
      "learning_rate": 0.0004722116824762856,
      "loss": 0.001,
      "step": 5566
    },
    {
      "epoch": 2.779331003494758,
      "grad_norm": 0.020007362589240074,
      "learning_rate": 0.00047220668996505246,
      "loss": 0.0007,
      "step": 5567
    },
    {
      "epoch": 2.779830254618073,
      "grad_norm": 0.03413013368844986,
      "learning_rate": 0.0004722016974538193,
      "loss": 0.0011,
      "step": 5568
    },
    {
      "epoch": 2.780329505741388,
      "grad_norm": 0.007184482179582119,
      "learning_rate": 0.00047219670494258616,
      "loss": 0.0005,
      "step": 5569
    },
    {
      "epoch": 2.780828756864703,
      "grad_norm": 0.3158295452594757,
      "learning_rate": 0.000472191712431353,
      "loss": 0.0023,
      "step": 5570
    },
    {
      "epoch": 2.781328007988018,
      "grad_norm": 0.029988499358296394,
      "learning_rate": 0.00047218671992011986,
      "loss": 0.0007,
      "step": 5571
    },
    {
      "epoch": 2.781827259111333,
      "grad_norm": 0.040635332465171814,
      "learning_rate": 0.00047218172740888666,
      "loss": 0.0008,
      "step": 5572
    },
    {
      "epoch": 2.782326510234648,
      "grad_norm": 0.1802108883857727,
      "learning_rate": 0.0004721767348976535,
      "loss": 0.0018,
      "step": 5573
    },
    {
      "epoch": 2.7828257613579632,
      "grad_norm": 0.5382537841796875,
      "learning_rate": 0.00047217174238642037,
      "loss": 0.0121,
      "step": 5574
    },
    {
      "epoch": 2.783325012481278,
      "grad_norm": 0.0231785848736763,
      "learning_rate": 0.0004721667498751872,
      "loss": 0.0006,
      "step": 5575
    },
    {
      "epoch": 2.7838242636045933,
      "grad_norm": 0.035461101680994034,
      "learning_rate": 0.00047216175736395407,
      "loss": 0.0005,
      "step": 5576
    },
    {
      "epoch": 2.784323514727908,
      "grad_norm": 0.011402350850403309,
      "learning_rate": 0.0004721567648527209,
      "loss": 0.0005,
      "step": 5577
    },
    {
      "epoch": 2.784822765851223,
      "grad_norm": 0.24208790063858032,
      "learning_rate": 0.0004721517723414878,
      "loss": 0.0028,
      "step": 5578
    },
    {
      "epoch": 2.785322016974538,
      "grad_norm": 0.2526452839374542,
      "learning_rate": 0.00047214677983025463,
      "loss": 0.0022,
      "step": 5579
    },
    {
      "epoch": 2.785821268097853,
      "grad_norm": 0.028835412114858627,
      "learning_rate": 0.0004721417873190215,
      "loss": 0.0007,
      "step": 5580
    },
    {
      "epoch": 2.786320519221168,
      "grad_norm": 0.04635316878557205,
      "learning_rate": 0.00047213679480778833,
      "loss": 0.0007,
      "step": 5581
    },
    {
      "epoch": 2.7868197703444832,
      "grad_norm": 0.4669622778892517,
      "learning_rate": 0.0004721318022965552,
      "loss": 0.0031,
      "step": 5582
    },
    {
      "epoch": 2.7873190214677983,
      "grad_norm": 0.05479113757610321,
      "learning_rate": 0.00047212680978532204,
      "loss": 0.0012,
      "step": 5583
    },
    {
      "epoch": 2.7878182725911134,
      "grad_norm": 0.22345325350761414,
      "learning_rate": 0.0004721218172740889,
      "loss": 0.0016,
      "step": 5584
    },
    {
      "epoch": 2.7883175237144284,
      "grad_norm": 0.005104720126837492,
      "learning_rate": 0.00047211682476285574,
      "loss": 0.0004,
      "step": 5585
    },
    {
      "epoch": 2.7888167748377435,
      "grad_norm": 0.5807745456695557,
      "learning_rate": 0.0004721118322516226,
      "loss": 0.0046,
      "step": 5586
    },
    {
      "epoch": 2.7893160259610585,
      "grad_norm": 0.44592899084091187,
      "learning_rate": 0.00047210683974038944,
      "loss": 0.0102,
      "step": 5587
    },
    {
      "epoch": 2.7898152770843736,
      "grad_norm": 0.012169738300144672,
      "learning_rate": 0.0004721018472291563,
      "loss": 0.0004,
      "step": 5588
    },
    {
      "epoch": 2.7903145282076887,
      "grad_norm": 0.33029115200042725,
      "learning_rate": 0.00047209685471792315,
      "loss": 0.0059,
      "step": 5589
    },
    {
      "epoch": 2.7908137793310033,
      "grad_norm": 0.3239234983921051,
      "learning_rate": 0.00047209186220669,
      "loss": 0.002,
      "step": 5590
    },
    {
      "epoch": 2.7913130304543188,
      "grad_norm": 0.006456041242927313,
      "learning_rate": 0.00047208686969545685,
      "loss": 0.0003,
      "step": 5591
    },
    {
      "epoch": 2.7918122815776334,
      "grad_norm": 0.0213945135474205,
      "learning_rate": 0.0004720818771842237,
      "loss": 0.0006,
      "step": 5592
    },
    {
      "epoch": 2.7923115327009485,
      "grad_norm": 0.6961555480957031,
      "learning_rate": 0.00047207688467299056,
      "loss": 0.0051,
      "step": 5593
    },
    {
      "epoch": 2.7928107838242635,
      "grad_norm": 0.05706492066383362,
      "learning_rate": 0.0004720718921617574,
      "loss": 0.0011,
      "step": 5594
    },
    {
      "epoch": 2.7933100349475786,
      "grad_norm": 0.10352163761854172,
      "learning_rate": 0.00047206689965052426,
      "loss": 0.0011,
      "step": 5595
    },
    {
      "epoch": 2.7938092860708936,
      "grad_norm": 0.09759961068630219,
      "learning_rate": 0.0004720619071392911,
      "loss": 0.0012,
      "step": 5596
    },
    {
      "epoch": 2.7943085371942087,
      "grad_norm": 0.15194712579250336,
      "learning_rate": 0.00047205691462805796,
      "loss": 0.0011,
      "step": 5597
    },
    {
      "epoch": 2.7948077883175237,
      "grad_norm": 0.013896367512643337,
      "learning_rate": 0.0004720519221168248,
      "loss": 0.0005,
      "step": 5598
    },
    {
      "epoch": 2.795307039440839,
      "grad_norm": 0.4274483919143677,
      "learning_rate": 0.00047204692960559167,
      "loss": 0.0027,
      "step": 5599
    },
    {
      "epoch": 2.795806290564154,
      "grad_norm": 0.09039713442325592,
      "learning_rate": 0.0004720419370943585,
      "loss": 0.001,
      "step": 5600
    },
    {
      "epoch": 2.796305541687469,
      "grad_norm": 0.3057146370410919,
      "learning_rate": 0.00047203694458312537,
      "loss": 0.0199,
      "step": 5601
    },
    {
      "epoch": 2.796804792810784,
      "grad_norm": 0.29275068640708923,
      "learning_rate": 0.00047203195207189217,
      "loss": 0.0045,
      "step": 5602
    },
    {
      "epoch": 2.7973040439340986,
      "grad_norm": 0.08159779012203217,
      "learning_rate": 0.000472026959560659,
      "loss": 0.0011,
      "step": 5603
    },
    {
      "epoch": 2.797803295057414,
      "grad_norm": 0.26046741008758545,
      "learning_rate": 0.0004720219670494259,
      "loss": 0.0039,
      "step": 5604
    },
    {
      "epoch": 2.7983025461807287,
      "grad_norm": 0.08880273997783661,
      "learning_rate": 0.0004720169745381927,
      "loss": 0.0013,
      "step": 5605
    },
    {
      "epoch": 2.7988017973040438,
      "grad_norm": 0.043774932622909546,
      "learning_rate": 0.0004720119820269596,
      "loss": 0.0009,
      "step": 5606
    },
    {
      "epoch": 2.799301048427359,
      "grad_norm": 0.016605572775006294,
      "learning_rate": 0.00047200698951572643,
      "loss": 0.0004,
      "step": 5607
    },
    {
      "epoch": 2.799800299550674,
      "grad_norm": 0.01202351227402687,
      "learning_rate": 0.00047200199700449323,
      "loss": 0.0005,
      "step": 5608
    },
    {
      "epoch": 2.800299550673989,
      "grad_norm": 0.34010884165763855,
      "learning_rate": 0.0004719970044932601,
      "loss": 0.006,
      "step": 5609
    },
    {
      "epoch": 2.800798801797304,
      "grad_norm": 0.03606361523270607,
      "learning_rate": 0.00047199201198202693,
      "loss": 0.0009,
      "step": 5610
    },
    {
      "epoch": 2.801298052920619,
      "grad_norm": 0.26225289702415466,
      "learning_rate": 0.0004719870194707938,
      "loss": 0.0034,
      "step": 5611
    },
    {
      "epoch": 2.801797304043934,
      "grad_norm": 0.23405994474887848,
      "learning_rate": 0.00047198202695956064,
      "loss": 0.0081,
      "step": 5612
    },
    {
      "epoch": 2.802296555167249,
      "grad_norm": 0.5490062236785889,
      "learning_rate": 0.0004719770344483275,
      "loss": 0.0105,
      "step": 5613
    },
    {
      "epoch": 2.8027958062905642,
      "grad_norm": 0.016001490876078606,
      "learning_rate": 0.00047197204193709434,
      "loss": 0.0006,
      "step": 5614
    },
    {
      "epoch": 2.8032950574138793,
      "grad_norm": 0.00992613472044468,
      "learning_rate": 0.0004719670494258612,
      "loss": 0.0004,
      "step": 5615
    },
    {
      "epoch": 2.8037943085371944,
      "grad_norm": 0.20023077726364136,
      "learning_rate": 0.00047196205691462805,
      "loss": 0.0014,
      "step": 5616
    },
    {
      "epoch": 2.8042935596605094,
      "grad_norm": 0.049726568162441254,
      "learning_rate": 0.0004719570644033949,
      "loss": 0.0008,
      "step": 5617
    },
    {
      "epoch": 2.804792810783824,
      "grad_norm": 0.17855282127857208,
      "learning_rate": 0.00047195207189216175,
      "loss": 0.0014,
      "step": 5618
    },
    {
      "epoch": 2.8052920619071395,
      "grad_norm": 0.06434664875268936,
      "learning_rate": 0.0004719470793809286,
      "loss": 0.001,
      "step": 5619
    },
    {
      "epoch": 2.805791313030454,
      "grad_norm": 0.6277820467948914,
      "learning_rate": 0.00047194208686969545,
      "loss": 0.0026,
      "step": 5620
    },
    {
      "epoch": 2.806290564153769,
      "grad_norm": 0.3874826729297638,
      "learning_rate": 0.0004719370943584623,
      "loss": 0.0034,
      "step": 5621
    },
    {
      "epoch": 2.8067898152770843,
      "grad_norm": 0.04874958470463753,
      "learning_rate": 0.00047193210184722916,
      "loss": 0.0009,
      "step": 5622
    },
    {
      "epoch": 2.8072890664003993,
      "grad_norm": 0.18663932383060455,
      "learning_rate": 0.000471927109335996,
      "loss": 0.0029,
      "step": 5623
    },
    {
      "epoch": 2.8077883175237144,
      "grad_norm": 0.05370866507291794,
      "learning_rate": 0.00047192211682476286,
      "loss": 0.001,
      "step": 5624
    },
    {
      "epoch": 2.8082875686470294,
      "grad_norm": 0.03169972077012062,
      "learning_rate": 0.0004719171243135297,
      "loss": 0.0007,
      "step": 5625
    },
    {
      "epoch": 2.8087868197703445,
      "grad_norm": 0.027972539886832237,
      "learning_rate": 0.00047191213180229657,
      "loss": 0.0007,
      "step": 5626
    },
    {
      "epoch": 2.8092860708936596,
      "grad_norm": 0.05785702168941498,
      "learning_rate": 0.0004719071392910634,
      "loss": 0.0009,
      "step": 5627
    },
    {
      "epoch": 2.8097853220169746,
      "grad_norm": 0.047742586582899094,
      "learning_rate": 0.00047190214677983027,
      "loss": 0.0006,
      "step": 5628
    },
    {
      "epoch": 2.8102845731402897,
      "grad_norm": 0.12659113109111786,
      "learning_rate": 0.0004718971542685971,
      "loss": 0.0009,
      "step": 5629
    },
    {
      "epoch": 2.8107838242636047,
      "grad_norm": 0.04555504024028778,
      "learning_rate": 0.000471892161757364,
      "loss": 0.0009,
      "step": 5630
    },
    {
      "epoch": 2.8112830753869194,
      "grad_norm": 0.11614038050174713,
      "learning_rate": 0.00047188716924613077,
      "loss": 0.0014,
      "step": 5631
    },
    {
      "epoch": 2.811782326510235,
      "grad_norm": 0.5997309684753418,
      "learning_rate": 0.0004718821767348976,
      "loss": 0.0175,
      "step": 5632
    },
    {
      "epoch": 2.8122815776335495,
      "grad_norm": 0.16666872799396515,
      "learning_rate": 0.0004718771842236645,
      "loss": 0.0016,
      "step": 5633
    },
    {
      "epoch": 2.812780828756865,
      "grad_norm": 0.03829048201441765,
      "learning_rate": 0.00047187219171243133,
      "loss": 0.0008,
      "step": 5634
    },
    {
      "epoch": 2.8132800798801796,
      "grad_norm": 0.061763275414705276,
      "learning_rate": 0.0004718671992011982,
      "loss": 0.0009,
      "step": 5635
    },
    {
      "epoch": 2.8137793310034946,
      "grad_norm": 0.19675910472869873,
      "learning_rate": 0.00047186220668996503,
      "loss": 0.001,
      "step": 5636
    },
    {
      "epoch": 2.8142785821268097,
      "grad_norm": 0.22726088762283325,
      "learning_rate": 0.0004718572141787319,
      "loss": 0.0078,
      "step": 5637
    },
    {
      "epoch": 2.8147778332501248,
      "grad_norm": 0.39380425214767456,
      "learning_rate": 0.00047185222166749874,
      "loss": 0.0104,
      "step": 5638
    },
    {
      "epoch": 2.81527708437344,
      "grad_norm": 0.2507126033306122,
      "learning_rate": 0.0004718472291562656,
      "loss": 0.0021,
      "step": 5639
    },
    {
      "epoch": 2.815776335496755,
      "grad_norm": 0.10744577646255493,
      "learning_rate": 0.00047184223664503244,
      "loss": 0.0018,
      "step": 5640
    },
    {
      "epoch": 2.81627558662007,
      "grad_norm": 0.008153319358825684,
      "learning_rate": 0.0004718372441337993,
      "loss": 0.0005,
      "step": 5641
    },
    {
      "epoch": 2.816774837743385,
      "grad_norm": 0.06219164654612541,
      "learning_rate": 0.00047183225162256614,
      "loss": 0.0008,
      "step": 5642
    },
    {
      "epoch": 2.8172740888667,
      "grad_norm": 0.3446385860443115,
      "learning_rate": 0.000471827259111333,
      "loss": 0.0045,
      "step": 5643
    },
    {
      "epoch": 2.817773339990015,
      "grad_norm": 0.061509281396865845,
      "learning_rate": 0.00047182226660009985,
      "loss": 0.0011,
      "step": 5644
    },
    {
      "epoch": 2.81827259111333,
      "grad_norm": 0.00965946912765503,
      "learning_rate": 0.0004718172740888667,
      "loss": 0.0005,
      "step": 5645
    },
    {
      "epoch": 2.818771842236645,
      "grad_norm": 0.13219331204891205,
      "learning_rate": 0.00047181228157763355,
      "loss": 0.0025,
      "step": 5646
    },
    {
      "epoch": 2.8192710933599603,
      "grad_norm": 0.1496395617723465,
      "learning_rate": 0.0004718072890664004,
      "loss": 0.0018,
      "step": 5647
    },
    {
      "epoch": 2.819770344483275,
      "grad_norm": 0.026615140959620476,
      "learning_rate": 0.00047180229655516726,
      "loss": 0.0009,
      "step": 5648
    },
    {
      "epoch": 2.82026959560659,
      "grad_norm": 0.3524834215641022,
      "learning_rate": 0.0004717973040439341,
      "loss": 0.007,
      "step": 5649
    },
    {
      "epoch": 2.820768846729905,
      "grad_norm": 0.007956208661198616,
      "learning_rate": 0.00047179231153270096,
      "loss": 0.0004,
      "step": 5650
    },
    {
      "epoch": 2.82126809785322,
      "grad_norm": 0.10305716097354889,
      "learning_rate": 0.0004717873190214678,
      "loss": 0.0008,
      "step": 5651
    },
    {
      "epoch": 2.821767348976535,
      "grad_norm": 0.025745084509253502,
      "learning_rate": 0.00047178232651023467,
      "loss": 0.0005,
      "step": 5652
    },
    {
      "epoch": 2.82226660009985,
      "grad_norm": 0.17679597437381744,
      "learning_rate": 0.0004717773339990015,
      "loss": 0.0018,
      "step": 5653
    },
    {
      "epoch": 2.8227658512231653,
      "grad_norm": 0.2839796543121338,
      "learning_rate": 0.00047177234148776837,
      "loss": 0.0036,
      "step": 5654
    },
    {
      "epoch": 2.8232651023464803,
      "grad_norm": 0.012409861199557781,
      "learning_rate": 0.0004717673489765352,
      "loss": 0.0004,
      "step": 5655
    },
    {
      "epoch": 2.8237643534697954,
      "grad_norm": 0.35047101974487305,
      "learning_rate": 0.0004717623564653021,
      "loss": 0.005,
      "step": 5656
    },
    {
      "epoch": 2.8242636045931104,
      "grad_norm": 0.3198223412036896,
      "learning_rate": 0.0004717573639540689,
      "loss": 0.0072,
      "step": 5657
    },
    {
      "epoch": 2.8247628557164255,
      "grad_norm": 0.11551593244075775,
      "learning_rate": 0.0004717523714428358,
      "loss": 0.0013,
      "step": 5658
    },
    {
      "epoch": 2.8252621068397406,
      "grad_norm": 0.026014192029833794,
      "learning_rate": 0.00047174737893160263,
      "loss": 0.0005,
      "step": 5659
    },
    {
      "epoch": 2.8257613579630556,
      "grad_norm": 0.07150574773550034,
      "learning_rate": 0.00047174238642036943,
      "loss": 0.0012,
      "step": 5660
    },
    {
      "epoch": 2.8262606090863702,
      "grad_norm": 0.04415225610136986,
      "learning_rate": 0.0004717373939091363,
      "loss": 0.0005,
      "step": 5661
    },
    {
      "epoch": 2.8267598602096857,
      "grad_norm": 0.12529009580612183,
      "learning_rate": 0.00047173240139790313,
      "loss": 0.0011,
      "step": 5662
    },
    {
      "epoch": 2.8272591113330003,
      "grad_norm": 0.0094378050416708,
      "learning_rate": 0.00047172740888667,
      "loss": 0.0005,
      "step": 5663
    },
    {
      "epoch": 2.8277583624563154,
      "grad_norm": 0.041952505707740784,
      "learning_rate": 0.00047172241637543684,
      "loss": 0.0007,
      "step": 5664
    },
    {
      "epoch": 2.8282576135796305,
      "grad_norm": 0.2748070955276489,
      "learning_rate": 0.0004717174238642037,
      "loss": 0.0006,
      "step": 5665
    },
    {
      "epoch": 2.8287568647029455,
      "grad_norm": 0.2258831113576889,
      "learning_rate": 0.00047171243135297054,
      "loss": 0.0017,
      "step": 5666
    },
    {
      "epoch": 2.8292561158262606,
      "grad_norm": 0.044868070632219315,
      "learning_rate": 0.0004717074388417374,
      "loss": 0.0009,
      "step": 5667
    },
    {
      "epoch": 2.8297553669495756,
      "grad_norm": 0.04764557629823685,
      "learning_rate": 0.00047170244633050424,
      "loss": 0.001,
      "step": 5668
    },
    {
      "epoch": 2.8302546180728907,
      "grad_norm": 0.01574872061610222,
      "learning_rate": 0.0004716974538192711,
      "loss": 0.0005,
      "step": 5669
    },
    {
      "epoch": 2.8307538691962058,
      "grad_norm": 0.008597977459430695,
      "learning_rate": 0.00047169246130803795,
      "loss": 0.0004,
      "step": 5670
    },
    {
      "epoch": 2.831253120319521,
      "grad_norm": 0.008691922761499882,
      "learning_rate": 0.0004716874687968048,
      "loss": 0.0004,
      "step": 5671
    },
    {
      "epoch": 2.831752371442836,
      "grad_norm": 0.008426109328866005,
      "learning_rate": 0.00047168247628557165,
      "loss": 0.0003,
      "step": 5672
    },
    {
      "epoch": 2.832251622566151,
      "grad_norm": 0.06687338650226593,
      "learning_rate": 0.0004716774837743385,
      "loss": 0.001,
      "step": 5673
    },
    {
      "epoch": 2.8327508736894655,
      "grad_norm": 0.010245511308312416,
      "learning_rate": 0.00047167249126310536,
      "loss": 0.0004,
      "step": 5674
    },
    {
      "epoch": 2.833250124812781,
      "grad_norm": 0.8150427341461182,
      "learning_rate": 0.0004716674987518722,
      "loss": 0.0042,
      "step": 5675
    },
    {
      "epoch": 2.8337493759360957,
      "grad_norm": 0.01219923049211502,
      "learning_rate": 0.00047166250624063906,
      "loss": 0.0004,
      "step": 5676
    },
    {
      "epoch": 2.8342486270594107,
      "grad_norm": 0.007310058455914259,
      "learning_rate": 0.0004716575137294059,
      "loss": 0.0004,
      "step": 5677
    },
    {
      "epoch": 2.8347478781827258,
      "grad_norm": 0.04363503307104111,
      "learning_rate": 0.00047165252121817276,
      "loss": 0.0007,
      "step": 5678
    },
    {
      "epoch": 2.835247129306041,
      "grad_norm": 0.046756040304899216,
      "learning_rate": 0.0004716475287069396,
      "loss": 0.0007,
      "step": 5679
    },
    {
      "epoch": 2.835746380429356,
      "grad_norm": 0.005893709138035774,
      "learning_rate": 0.00047164253619570647,
      "loss": 0.0004,
      "step": 5680
    },
    {
      "epoch": 2.836245631552671,
      "grad_norm": 0.01297758985310793,
      "learning_rate": 0.0004716375436844733,
      "loss": 0.0004,
      "step": 5681
    },
    {
      "epoch": 2.836744882675986,
      "grad_norm": 0.05306079611182213,
      "learning_rate": 0.0004716325511732402,
      "loss": 0.0007,
      "step": 5682
    },
    {
      "epoch": 2.837244133799301,
      "grad_norm": 0.341211199760437,
      "learning_rate": 0.000471627558662007,
      "loss": 0.0024,
      "step": 5683
    },
    {
      "epoch": 2.837743384922616,
      "grad_norm": 0.04409519582986832,
      "learning_rate": 0.0004716225661507739,
      "loss": 0.0006,
      "step": 5684
    },
    {
      "epoch": 2.838242636045931,
      "grad_norm": 0.1121985986828804,
      "learning_rate": 0.00047161757363954073,
      "loss": 0.001,
      "step": 5685
    },
    {
      "epoch": 2.8387418871692462,
      "grad_norm": 0.018401341512799263,
      "learning_rate": 0.0004716125811283076,
      "loss": 0.0005,
      "step": 5686
    },
    {
      "epoch": 2.8392411382925613,
      "grad_norm": 0.025537487119436264,
      "learning_rate": 0.00047160758861707443,
      "loss": 0.0007,
      "step": 5687
    },
    {
      "epoch": 2.8397403894158764,
      "grad_norm": 0.011997689493000507,
      "learning_rate": 0.0004716025961058413,
      "loss": 0.0004,
      "step": 5688
    },
    {
      "epoch": 2.840239640539191,
      "grad_norm": 0.41686776280403137,
      "learning_rate": 0.0004715976035946081,
      "loss": 0.0111,
      "step": 5689
    },
    {
      "epoch": 2.8407388916625065,
      "grad_norm": 1.5120753049850464,
      "learning_rate": 0.00047159261108337494,
      "loss": 0.0131,
      "step": 5690
    },
    {
      "epoch": 2.841238142785821,
      "grad_norm": 0.036148618906736374,
      "learning_rate": 0.0004715876185721418,
      "loss": 0.0007,
      "step": 5691
    },
    {
      "epoch": 2.841737393909136,
      "grad_norm": 0.046134963631629944,
      "learning_rate": 0.00047158262606090864,
      "loss": 0.0008,
      "step": 5692
    },
    {
      "epoch": 2.842236645032451,
      "grad_norm": 0.00986025296151638,
      "learning_rate": 0.0004715776335496755,
      "loss": 0.0005,
      "step": 5693
    },
    {
      "epoch": 2.8427358961557663,
      "grad_norm": 0.0655299574136734,
      "learning_rate": 0.00047157264103844234,
      "loss": 0.0005,
      "step": 5694
    },
    {
      "epoch": 2.8432351472790813,
      "grad_norm": 0.5039570331573486,
      "learning_rate": 0.0004715676485272092,
      "loss": 0.0165,
      "step": 5695
    },
    {
      "epoch": 2.8437343984023964,
      "grad_norm": 0.060994405299425125,
      "learning_rate": 0.00047156265601597605,
      "loss": 0.0011,
      "step": 5696
    },
    {
      "epoch": 2.8442336495257114,
      "grad_norm": 0.033462490886449814,
      "learning_rate": 0.0004715576635047429,
      "loss": 0.0006,
      "step": 5697
    },
    {
      "epoch": 2.8447329006490265,
      "grad_norm": 0.010775760747492313,
      "learning_rate": 0.00047155267099350975,
      "loss": 0.0005,
      "step": 5698
    },
    {
      "epoch": 2.8452321517723416,
      "grad_norm": 0.027649497613310814,
      "learning_rate": 0.0004715476784822766,
      "loss": 0.0006,
      "step": 5699
    },
    {
      "epoch": 2.8457314028956566,
      "grad_norm": 0.9730023145675659,
      "learning_rate": 0.00047154268597104346,
      "loss": 0.0195,
      "step": 5700
    },
    {
      "epoch": 2.8462306540189717,
      "grad_norm": 0.06989433616399765,
      "learning_rate": 0.0004715376934598103,
      "loss": 0.0009,
      "step": 5701
    },
    {
      "epoch": 2.8467299051422863,
      "grad_norm": 0.006120752077549696,
      "learning_rate": 0.00047153270094857716,
      "loss": 0.0004,
      "step": 5702
    },
    {
      "epoch": 2.847229156265602,
      "grad_norm": 0.9718095064163208,
      "learning_rate": 0.000471527708437344,
      "loss": 0.003,
      "step": 5703
    },
    {
      "epoch": 2.8477284073889164,
      "grad_norm": 0.5566727519035339,
      "learning_rate": 0.00047152271592611086,
      "loss": 0.003,
      "step": 5704
    },
    {
      "epoch": 2.848227658512232,
      "grad_norm": 0.02662300318479538,
      "learning_rate": 0.0004715177234148777,
      "loss": 0.0007,
      "step": 5705
    },
    {
      "epoch": 2.8487269096355465,
      "grad_norm": 0.22558824717998505,
      "learning_rate": 0.00047151273090364457,
      "loss": 0.0014,
      "step": 5706
    },
    {
      "epoch": 2.8492261607588616,
      "grad_norm": 0.2563435435295105,
      "learning_rate": 0.0004715077383924114,
      "loss": 0.0017,
      "step": 5707
    },
    {
      "epoch": 2.8497254118821767,
      "grad_norm": 0.04348110407590866,
      "learning_rate": 0.00047150274588117827,
      "loss": 0.0008,
      "step": 5708
    },
    {
      "epoch": 2.8502246630054917,
      "grad_norm": 0.15136323869228363,
      "learning_rate": 0.0004714977533699451,
      "loss": 0.0025,
      "step": 5709
    },
    {
      "epoch": 2.8507239141288068,
      "grad_norm": 0.005352523643523455,
      "learning_rate": 0.000471492760858712,
      "loss": 0.0004,
      "step": 5710
    },
    {
      "epoch": 2.851223165252122,
      "grad_norm": 0.06928481161594391,
      "learning_rate": 0.00047148776834747883,
      "loss": 0.0008,
      "step": 5711
    },
    {
      "epoch": 2.851722416375437,
      "grad_norm": 0.8023197650909424,
      "learning_rate": 0.0004714827758362457,
      "loss": 0.0031,
      "step": 5712
    },
    {
      "epoch": 2.852221667498752,
      "grad_norm": 0.39257484674453735,
      "learning_rate": 0.00047147778332501253,
      "loss": 0.0025,
      "step": 5713
    },
    {
      "epoch": 2.852720918622067,
      "grad_norm": 0.06483951956033707,
      "learning_rate": 0.0004714727908137794,
      "loss": 0.0008,
      "step": 5714
    },
    {
      "epoch": 2.853220169745382,
      "grad_norm": 0.07328486442565918,
      "learning_rate": 0.00047146779830254624,
      "loss": 0.0013,
      "step": 5715
    },
    {
      "epoch": 2.853719420868697,
      "grad_norm": 0.34995362162590027,
      "learning_rate": 0.0004714628057913131,
      "loss": 0.0036,
      "step": 5716
    },
    {
      "epoch": 2.8542186719920117,
      "grad_norm": 0.03243483975529671,
      "learning_rate": 0.00047145781328007994,
      "loss": 0.0009,
      "step": 5717
    },
    {
      "epoch": 2.8547179231153272,
      "grad_norm": 0.6758342981338501,
      "learning_rate": 0.00047145282076884674,
      "loss": 0.0035,
      "step": 5718
    },
    {
      "epoch": 2.855217174238642,
      "grad_norm": 0.38350069522857666,
      "learning_rate": 0.0004714478282576136,
      "loss": 0.0024,
      "step": 5719
    },
    {
      "epoch": 2.855716425361957,
      "grad_norm": 0.022172143682837486,
      "learning_rate": 0.00047144283574638044,
      "loss": 0.0007,
      "step": 5720
    },
    {
      "epoch": 2.856215676485272,
      "grad_norm": 0.42631182074546814,
      "learning_rate": 0.00047143784323514724,
      "loss": 0.0119,
      "step": 5721
    },
    {
      "epoch": 2.856714927608587,
      "grad_norm": 0.026596346870064735,
      "learning_rate": 0.0004714328507239141,
      "loss": 0.0006,
      "step": 5722
    },
    {
      "epoch": 2.857214178731902,
      "grad_norm": 2.300787925720215,
      "learning_rate": 0.00047142785821268095,
      "loss": 0.0035,
      "step": 5723
    },
    {
      "epoch": 2.857713429855217,
      "grad_norm": 0.011101221665740013,
      "learning_rate": 0.0004714228657014478,
      "loss": 0.0006,
      "step": 5724
    },
    {
      "epoch": 2.858212680978532,
      "grad_norm": 0.01751323975622654,
      "learning_rate": 0.00047141787319021465,
      "loss": 0.0006,
      "step": 5725
    },
    {
      "epoch": 2.8587119321018473,
      "grad_norm": 0.3653239607810974,
      "learning_rate": 0.0004714128806789815,
      "loss": 0.0574,
      "step": 5726
    },
    {
      "epoch": 2.8592111832251623,
      "grad_norm": 0.2832776606082916,
      "learning_rate": 0.00047140788816774835,
      "loss": 0.0207,
      "step": 5727
    },
    {
      "epoch": 2.8597104343484774,
      "grad_norm": 0.3269430696964264,
      "learning_rate": 0.0004714028956565152,
      "loss": 0.0024,
      "step": 5728
    },
    {
      "epoch": 2.8602096854717924,
      "grad_norm": 0.720043420791626,
      "learning_rate": 0.00047139790314528206,
      "loss": 0.0062,
      "step": 5729
    },
    {
      "epoch": 2.8607089365951075,
      "grad_norm": 0.2648354172706604,
      "learning_rate": 0.0004713929106340489,
      "loss": 0.0038,
      "step": 5730
    },
    {
      "epoch": 2.8612081877184226,
      "grad_norm": 0.06884264945983887,
      "learning_rate": 0.00047138791812281576,
      "loss": 0.0012,
      "step": 5731
    },
    {
      "epoch": 2.861707438841737,
      "grad_norm": 0.00873559806495905,
      "learning_rate": 0.0004713829256115826,
      "loss": 0.0005,
      "step": 5732
    },
    {
      "epoch": 2.8622066899650527,
      "grad_norm": 0.09837959706783295,
      "learning_rate": 0.00047137793310034947,
      "loss": 0.0019,
      "step": 5733
    },
    {
      "epoch": 2.8627059410883673,
      "grad_norm": 0.04390982538461685,
      "learning_rate": 0.0004713729405891163,
      "loss": 0.0009,
      "step": 5734
    },
    {
      "epoch": 2.8632051922116823,
      "grad_norm": 0.4544404447078705,
      "learning_rate": 0.00047136794807788317,
      "loss": 0.0038,
      "step": 5735
    },
    {
      "epoch": 2.8637044433349974,
      "grad_norm": 0.1332608461380005,
      "learning_rate": 0.00047136295556665,
      "loss": 0.0022,
      "step": 5736
    },
    {
      "epoch": 2.8642036944583125,
      "grad_norm": 1.8237992525100708,
      "learning_rate": 0.0004713579630554169,
      "loss": 0.0064,
      "step": 5737
    },
    {
      "epoch": 2.8647029455816275,
      "grad_norm": 0.2729731798171997,
      "learning_rate": 0.0004713529705441837,
      "loss": 0.0066,
      "step": 5738
    },
    {
      "epoch": 2.8652021967049426,
      "grad_norm": 0.049404650926589966,
      "learning_rate": 0.0004713479780329506,
      "loss": 0.0012,
      "step": 5739
    },
    {
      "epoch": 2.8657014478282576,
      "grad_norm": 0.049703311175107956,
      "learning_rate": 0.00047134298552171743,
      "loss": 0.001,
      "step": 5740
    },
    {
      "epoch": 2.8662006989515727,
      "grad_norm": 0.08423628658056259,
      "learning_rate": 0.0004713379930104843,
      "loss": 0.0015,
      "step": 5741
    },
    {
      "epoch": 2.8666999500748878,
      "grad_norm": 0.1538991928100586,
      "learning_rate": 0.00047133300049925113,
      "loss": 0.0026,
      "step": 5742
    },
    {
      "epoch": 2.867199201198203,
      "grad_norm": 0.6285594701766968,
      "learning_rate": 0.000471328007988018,
      "loss": 0.0051,
      "step": 5743
    },
    {
      "epoch": 2.867698452321518,
      "grad_norm": 0.025535020977258682,
      "learning_rate": 0.00047132301547678484,
      "loss": 0.0007,
      "step": 5744
    },
    {
      "epoch": 2.8681977034448325,
      "grad_norm": 0.33240076899528503,
      "learning_rate": 0.0004713180229655517,
      "loss": 0.0055,
      "step": 5745
    },
    {
      "epoch": 2.868696954568148,
      "grad_norm": 0.5312871336936951,
      "learning_rate": 0.00047131303045431854,
      "loss": 0.0044,
      "step": 5746
    },
    {
      "epoch": 2.8691962056914626,
      "grad_norm": 0.23095571994781494,
      "learning_rate": 0.00047130803794308534,
      "loss": 0.0032,
      "step": 5747
    },
    {
      "epoch": 2.8696954568147777,
      "grad_norm": 0.555152416229248,
      "learning_rate": 0.0004713030454318522,
      "loss": 0.0081,
      "step": 5748
    },
    {
      "epoch": 2.8701947079380927,
      "grad_norm": 0.024325449019670486,
      "learning_rate": 0.00047129805292061904,
      "loss": 0.0008,
      "step": 5749
    },
    {
      "epoch": 2.870693959061408,
      "grad_norm": 0.028273262083530426,
      "learning_rate": 0.0004712930604093859,
      "loss": 0.0008,
      "step": 5750
    },
    {
      "epoch": 2.871193210184723,
      "grad_norm": 0.33286309242248535,
      "learning_rate": 0.00047128806789815275,
      "loss": 0.0047,
      "step": 5751
    },
    {
      "epoch": 2.871692461308038,
      "grad_norm": 0.011526627466082573,
      "learning_rate": 0.0004712830753869196,
      "loss": 0.0006,
      "step": 5752
    },
    {
      "epoch": 2.872191712431353,
      "grad_norm": 0.030989373102784157,
      "learning_rate": 0.00047127808287568645,
      "loss": 0.0006,
      "step": 5753
    },
    {
      "epoch": 2.872690963554668,
      "grad_norm": 0.005557958967983723,
      "learning_rate": 0.0004712730903644533,
      "loss": 0.0005,
      "step": 5754
    },
    {
      "epoch": 2.873190214677983,
      "grad_norm": 0.513939380645752,
      "learning_rate": 0.00047126809785322016,
      "loss": 0.0221,
      "step": 5755
    },
    {
      "epoch": 2.873689465801298,
      "grad_norm": 0.37966087460517883,
      "learning_rate": 0.000471263105341987,
      "loss": 0.0042,
      "step": 5756
    },
    {
      "epoch": 2.874188716924613,
      "grad_norm": 0.09521983563899994,
      "learning_rate": 0.00047125811283075386,
      "loss": 0.0013,
      "step": 5757
    },
    {
      "epoch": 2.8746879680479283,
      "grad_norm": 0.361041396856308,
      "learning_rate": 0.0004712531203195207,
      "loss": 0.0061,
      "step": 5758
    },
    {
      "epoch": 2.8751872191712433,
      "grad_norm": 0.2522059679031372,
      "learning_rate": 0.00047124812780828757,
      "loss": 0.003,
      "step": 5759
    },
    {
      "epoch": 2.875686470294558,
      "grad_norm": 0.03116728365421295,
      "learning_rate": 0.0004712431352970544,
      "loss": 0.001,
      "step": 5760
    },
    {
      "epoch": 2.8761857214178734,
      "grad_norm": 0.32226940989494324,
      "learning_rate": 0.00047123814278582127,
      "loss": 0.0046,
      "step": 5761
    },
    {
      "epoch": 2.876684972541188,
      "grad_norm": 0.08710011094808578,
      "learning_rate": 0.0004712331502745881,
      "loss": 0.0023,
      "step": 5762
    },
    {
      "epoch": 2.877184223664503,
      "grad_norm": 0.016343222931027412,
      "learning_rate": 0.000471228157763355,
      "loss": 0.0006,
      "step": 5763
    },
    {
      "epoch": 2.877683474787818,
      "grad_norm": 1.1830439567565918,
      "learning_rate": 0.0004712231652521218,
      "loss": 0.0043,
      "step": 5764
    },
    {
      "epoch": 2.878182725911133,
      "grad_norm": 0.05335118621587753,
      "learning_rate": 0.0004712181727408887,
      "loss": 0.0015,
      "step": 5765
    },
    {
      "epoch": 2.8786819770344483,
      "grad_norm": 0.06932784616947174,
      "learning_rate": 0.00047121318022965553,
      "loss": 0.0014,
      "step": 5766
    },
    {
      "epoch": 2.8791812281577633,
      "grad_norm": 0.4956183135509491,
      "learning_rate": 0.0004712081877184224,
      "loss": 0.0142,
      "step": 5767
    },
    {
      "epoch": 2.8796804792810784,
      "grad_norm": 0.04539640620350838,
      "learning_rate": 0.00047120319520718923,
      "loss": 0.0012,
      "step": 5768
    },
    {
      "epoch": 2.8801797304043935,
      "grad_norm": 0.030498331412672997,
      "learning_rate": 0.0004711982026959561,
      "loss": 0.0008,
      "step": 5769
    },
    {
      "epoch": 2.8806789815277085,
      "grad_norm": 0.03966418281197548,
      "learning_rate": 0.00047119321018472294,
      "loss": 0.001,
      "step": 5770
    },
    {
      "epoch": 2.8811782326510236,
      "grad_norm": 0.01901707984507084,
      "learning_rate": 0.0004711882176734898,
      "loss": 0.0007,
      "step": 5771
    },
    {
      "epoch": 2.8816774837743386,
      "grad_norm": 0.9039703011512756,
      "learning_rate": 0.00047118322516225664,
      "loss": 0.0448,
      "step": 5772
    },
    {
      "epoch": 2.8821767348976532,
      "grad_norm": 0.04304680973291397,
      "learning_rate": 0.0004711782326510235,
      "loss": 0.001,
      "step": 5773
    },
    {
      "epoch": 2.8826759860209688,
      "grad_norm": 0.12884308397769928,
      "learning_rate": 0.00047117324013979035,
      "loss": 0.0016,
      "step": 5774
    },
    {
      "epoch": 2.8831752371442834,
      "grad_norm": 0.018038224428892136,
      "learning_rate": 0.0004711682476285572,
      "loss": 0.0005,
      "step": 5775
    },
    {
      "epoch": 2.883674488267599,
      "grad_norm": 1.3318170309066772,
      "learning_rate": 0.000471163255117324,
      "loss": 0.013,
      "step": 5776
    },
    {
      "epoch": 2.8841737393909135,
      "grad_norm": 0.09812907874584198,
      "learning_rate": 0.00047115826260609085,
      "loss": 0.0011,
      "step": 5777
    },
    {
      "epoch": 2.8846729905142285,
      "grad_norm": 0.2824959456920624,
      "learning_rate": 0.0004711532700948577,
      "loss": 0.0049,
      "step": 5778
    },
    {
      "epoch": 2.8851722416375436,
      "grad_norm": 0.04875947907567024,
      "learning_rate": 0.00047114827758362455,
      "loss": 0.0011,
      "step": 5779
    },
    {
      "epoch": 2.8856714927608587,
      "grad_norm": 0.5290235877037048,
      "learning_rate": 0.0004711432850723914,
      "loss": 0.0182,
      "step": 5780
    },
    {
      "epoch": 2.8861707438841737,
      "grad_norm": 0.03843962773680687,
      "learning_rate": 0.00047113829256115826,
      "loss": 0.0011,
      "step": 5781
    },
    {
      "epoch": 2.8866699950074888,
      "grad_norm": 0.06064567342400551,
      "learning_rate": 0.0004711333000499251,
      "loss": 0.0011,
      "step": 5782
    },
    {
      "epoch": 2.887169246130804,
      "grad_norm": 0.5859777331352234,
      "learning_rate": 0.00047112830753869196,
      "loss": 0.0072,
      "step": 5783
    },
    {
      "epoch": 2.887668497254119,
      "grad_norm": 0.5336358547210693,
      "learning_rate": 0.0004711233150274588,
      "loss": 0.0087,
      "step": 5784
    },
    {
      "epoch": 2.888167748377434,
      "grad_norm": 0.4220374524593353,
      "learning_rate": 0.00047111832251622567,
      "loss": 0.0027,
      "step": 5785
    },
    {
      "epoch": 2.888666999500749,
      "grad_norm": 0.013336715288460255,
      "learning_rate": 0.0004711133300049925,
      "loss": 0.0006,
      "step": 5786
    },
    {
      "epoch": 2.889166250624064,
      "grad_norm": 0.37118616700172424,
      "learning_rate": 0.00047110833749375937,
      "loss": 0.0035,
      "step": 5787
    },
    {
      "epoch": 2.8896655017473787,
      "grad_norm": 0.1379762589931488,
      "learning_rate": 0.0004711033449825262,
      "loss": 0.0025,
      "step": 5788
    },
    {
      "epoch": 2.890164752870694,
      "grad_norm": 0.5561847686767578,
      "learning_rate": 0.0004710983524712931,
      "loss": 0.0067,
      "step": 5789
    },
    {
      "epoch": 2.890664003994009,
      "grad_norm": 0.04891595244407654,
      "learning_rate": 0.0004710933599600599,
      "loss": 0.0011,
      "step": 5790
    },
    {
      "epoch": 2.891163255117324,
      "grad_norm": 0.23912487924098969,
      "learning_rate": 0.0004710883674488268,
      "loss": 0.0037,
      "step": 5791
    },
    {
      "epoch": 2.891662506240639,
      "grad_norm": 0.1318536400794983,
      "learning_rate": 0.00047108337493759363,
      "loss": 0.0017,
      "step": 5792
    },
    {
      "epoch": 2.892161757363954,
      "grad_norm": 0.5687801837921143,
      "learning_rate": 0.0004710783824263605,
      "loss": 0.0091,
      "step": 5793
    },
    {
      "epoch": 2.892661008487269,
      "grad_norm": 3.8644983768463135,
      "learning_rate": 0.00047107338991512733,
      "loss": 0.0282,
      "step": 5794
    },
    {
      "epoch": 2.893160259610584,
      "grad_norm": 0.01565825752913952,
      "learning_rate": 0.0004710683974038942,
      "loss": 0.0008,
      "step": 5795
    },
    {
      "epoch": 2.893659510733899,
      "grad_norm": 0.027470948174595833,
      "learning_rate": 0.00047106340489266104,
      "loss": 0.0009,
      "step": 5796
    },
    {
      "epoch": 2.894158761857214,
      "grad_norm": 0.0929204672574997,
      "learning_rate": 0.0004710584123814279,
      "loss": 0.0022,
      "step": 5797
    },
    {
      "epoch": 2.8946580129805293,
      "grad_norm": 0.5226007699966431,
      "learning_rate": 0.00047105341987019474,
      "loss": 0.0057,
      "step": 5798
    },
    {
      "epoch": 2.8951572641038443,
      "grad_norm": 0.039866313338279724,
      "learning_rate": 0.0004710484273589616,
      "loss": 0.001,
      "step": 5799
    },
    {
      "epoch": 2.8956565152271594,
      "grad_norm": 0.6160788536071777,
      "learning_rate": 0.00047104343484772845,
      "loss": 0.007,
      "step": 5800
    },
    {
      "epoch": 2.8961557663504744,
      "grad_norm": 0.08310312032699585,
      "learning_rate": 0.0004710384423364953,
      "loss": 0.0015,
      "step": 5801
    },
    {
      "epoch": 2.8966550174737895,
      "grad_norm": 0.5205775499343872,
      "learning_rate": 0.00047103344982526215,
      "loss": 0.0098,
      "step": 5802
    },
    {
      "epoch": 2.897154268597104,
      "grad_norm": 0.28269171714782715,
      "learning_rate": 0.000471028457314029,
      "loss": 0.0097,
      "step": 5803
    },
    {
      "epoch": 2.8976535197204196,
      "grad_norm": 0.1597309112548828,
      "learning_rate": 0.00047102346480279585,
      "loss": 0.0011,
      "step": 5804
    },
    {
      "epoch": 2.8981527708437342,
      "grad_norm": 0.014009044505655766,
      "learning_rate": 0.00047101847229156265,
      "loss": 0.0006,
      "step": 5805
    },
    {
      "epoch": 2.8986520219670493,
      "grad_norm": 0.18061627447605133,
      "learning_rate": 0.0004710134797803295,
      "loss": 0.0027,
      "step": 5806
    },
    {
      "epoch": 2.8991512730903644,
      "grad_norm": 0.33445391058921814,
      "learning_rate": 0.00047100848726909636,
      "loss": 0.0023,
      "step": 5807
    },
    {
      "epoch": 2.8996505242136794,
      "grad_norm": 1.9845011234283447,
      "learning_rate": 0.0004710034947578632,
      "loss": 0.0473,
      "step": 5808
    },
    {
      "epoch": 2.9001497753369945,
      "grad_norm": 0.6638806462287903,
      "learning_rate": 0.00047099850224663006,
      "loss": 0.0128,
      "step": 5809
    },
    {
      "epoch": 2.9006490264603095,
      "grad_norm": 0.01830359548330307,
      "learning_rate": 0.0004709935097353969,
      "loss": 0.0006,
      "step": 5810
    },
    {
      "epoch": 2.9011482775836246,
      "grad_norm": 0.4157451391220093,
      "learning_rate": 0.00047098851722416376,
      "loss": 0.0035,
      "step": 5811
    },
    {
      "epoch": 2.9016475287069396,
      "grad_norm": 0.38086429238319397,
      "learning_rate": 0.0004709835247129306,
      "loss": 0.0065,
      "step": 5812
    },
    {
      "epoch": 2.9021467798302547,
      "grad_norm": 0.04371319338679314,
      "learning_rate": 0.00047097853220169747,
      "loss": 0.0012,
      "step": 5813
    },
    {
      "epoch": 2.9026460309535698,
      "grad_norm": 0.39753273129463196,
      "learning_rate": 0.0004709735396904643,
      "loss": 0.0077,
      "step": 5814
    },
    {
      "epoch": 2.903145282076885,
      "grad_norm": 0.04519253969192505,
      "learning_rate": 0.00047096854717923117,
      "loss": 0.0011,
      "step": 5815
    },
    {
      "epoch": 2.9036445332001994,
      "grad_norm": 0.17829768359661102,
      "learning_rate": 0.000470963554667998,
      "loss": 0.002,
      "step": 5816
    },
    {
      "epoch": 2.904143784323515,
      "grad_norm": 0.17435285449028015,
      "learning_rate": 0.0004709585621567649,
      "loss": 0.0016,
      "step": 5817
    },
    {
      "epoch": 2.9046430354468296,
      "grad_norm": 0.17360234260559082,
      "learning_rate": 0.00047095356964553173,
      "loss": 0.0011,
      "step": 5818
    },
    {
      "epoch": 2.9051422865701446,
      "grad_norm": 1.2827098369598389,
      "learning_rate": 0.0004709485771342986,
      "loss": 0.0091,
      "step": 5819
    },
    {
      "epoch": 2.9056415376934597,
      "grad_norm": 0.320492684841156,
      "learning_rate": 0.00047094358462306543,
      "loss": 0.0063,
      "step": 5820
    },
    {
      "epoch": 2.9061407888167747,
      "grad_norm": 0.13289456069469452,
      "learning_rate": 0.0004709385921118323,
      "loss": 0.0017,
      "step": 5821
    },
    {
      "epoch": 2.90664003994009,
      "grad_norm": 1.1573148965835571,
      "learning_rate": 0.00047093359960059914,
      "loss": 0.0114,
      "step": 5822
    },
    {
      "epoch": 2.907139291063405,
      "grad_norm": 0.05350780114531517,
      "learning_rate": 0.000470928607089366,
      "loss": 0.0013,
      "step": 5823
    },
    {
      "epoch": 2.90763854218672,
      "grad_norm": 1.5626357793807983,
      "learning_rate": 0.00047092361457813284,
      "loss": 0.0256,
      "step": 5824
    },
    {
      "epoch": 2.908137793310035,
      "grad_norm": 0.13427136838436127,
      "learning_rate": 0.0004709186220668997,
      "loss": 0.0017,
      "step": 5825
    },
    {
      "epoch": 2.90863704443335,
      "grad_norm": 1.1133732795715332,
      "learning_rate": 0.00047091362955566655,
      "loss": 0.005,
      "step": 5826
    },
    {
      "epoch": 2.909136295556665,
      "grad_norm": 0.20026111602783203,
      "learning_rate": 0.0004709086370444334,
      "loss": 0.0016,
      "step": 5827
    },
    {
      "epoch": 2.90963554667998,
      "grad_norm": 0.03110935352742672,
      "learning_rate": 0.00047090364453320025,
      "loss": 0.0008,
      "step": 5828
    },
    {
      "epoch": 2.910134797803295,
      "grad_norm": 0.052176583558321,
      "learning_rate": 0.0004708986520219671,
      "loss": 0.0014,
      "step": 5829
    },
    {
      "epoch": 2.9106340489266103,
      "grad_norm": 0.03485942259430885,
      "learning_rate": 0.00047089365951073395,
      "loss": 0.0011,
      "step": 5830
    },
    {
      "epoch": 2.911133300049925,
      "grad_norm": 0.010275263339281082,
      "learning_rate": 0.0004708886669995008,
      "loss": 0.0007,
      "step": 5831
    },
    {
      "epoch": 2.9116325511732404,
      "grad_norm": 1.4134211540222168,
      "learning_rate": 0.00047088367448826766,
      "loss": 0.0137,
      "step": 5832
    },
    {
      "epoch": 2.912131802296555,
      "grad_norm": 0.08299940824508667,
      "learning_rate": 0.0004708786819770345,
      "loss": 0.0016,
      "step": 5833
    },
    {
      "epoch": 2.91263105341987,
      "grad_norm": 0.3166617453098297,
      "learning_rate": 0.00047087368946580125,
      "loss": 0.0101,
      "step": 5834
    },
    {
      "epoch": 2.913130304543185,
      "grad_norm": 0.39136889576911926,
      "learning_rate": 0.0004708686969545681,
      "loss": 0.0032,
      "step": 5835
    },
    {
      "epoch": 2.9136295556665,
      "grad_norm": 0.7797254323959351,
      "learning_rate": 0.00047086370444333496,
      "loss": 0.0427,
      "step": 5836
    },
    {
      "epoch": 2.9141288067898152,
      "grad_norm": 0.8176196813583374,
      "learning_rate": 0.0004708587119321018,
      "loss": 0.007,
      "step": 5837
    },
    {
      "epoch": 2.9146280579131303,
      "grad_norm": 0.17125003039836884,
      "learning_rate": 0.00047085371942086866,
      "loss": 0.0028,
      "step": 5838
    },
    {
      "epoch": 2.9151273090364453,
      "grad_norm": 0.15793363749980927,
      "learning_rate": 0.0004708487269096355,
      "loss": 0.0024,
      "step": 5839
    },
    {
      "epoch": 2.9156265601597604,
      "grad_norm": 0.5958551168441772,
      "learning_rate": 0.00047084373439840237,
      "loss": 0.0076,
      "step": 5840
    },
    {
      "epoch": 2.9161258112830755,
      "grad_norm": 0.04419263079762459,
      "learning_rate": 0.0004708387418871692,
      "loss": 0.0013,
      "step": 5841
    },
    {
      "epoch": 2.9166250624063905,
      "grad_norm": 0.039603427052497864,
      "learning_rate": 0.00047083374937593607,
      "loss": 0.0009,
      "step": 5842
    },
    {
      "epoch": 2.9171243135297056,
      "grad_norm": 0.3620787560939789,
      "learning_rate": 0.0004708287568647029,
      "loss": 0.0122,
      "step": 5843
    },
    {
      "epoch": 2.91762356465302,
      "grad_norm": 0.23119530081748962,
      "learning_rate": 0.0004708237643534698,
      "loss": 0.0293,
      "step": 5844
    },
    {
      "epoch": 2.9181228157763357,
      "grad_norm": 1.0813515186309814,
      "learning_rate": 0.0004708187718422366,
      "loss": 0.0099,
      "step": 5845
    },
    {
      "epoch": 2.9186220668996503,
      "grad_norm": 0.5547626614570618,
      "learning_rate": 0.0004708137793310035,
      "loss": 0.0045,
      "step": 5846
    },
    {
      "epoch": 2.919121318022966,
      "grad_norm": 0.21583129465579987,
      "learning_rate": 0.00047080878681977033,
      "loss": 0.0021,
      "step": 5847
    },
    {
      "epoch": 2.9196205691462804,
      "grad_norm": 3.5083811283111572,
      "learning_rate": 0.0004708037943085372,
      "loss": 0.0125,
      "step": 5848
    },
    {
      "epoch": 2.9201198202695955,
      "grad_norm": 1.0351147651672363,
      "learning_rate": 0.00047079880179730403,
      "loss": 0.0209,
      "step": 5849
    },
    {
      "epoch": 2.9206190713929105,
      "grad_norm": 0.7257218956947327,
      "learning_rate": 0.0004707938092860709,
      "loss": 0.0102,
      "step": 5850
    },
    {
      "epoch": 2.9211183225162256,
      "grad_norm": 0.11644797027111053,
      "learning_rate": 0.00047078881677483774,
      "loss": 0.0033,
      "step": 5851
    },
    {
      "epoch": 2.9216175736395407,
      "grad_norm": 0.29693466424942017,
      "learning_rate": 0.0004707838242636046,
      "loss": 0.0044,
      "step": 5852
    },
    {
      "epoch": 2.9221168247628557,
      "grad_norm": 0.1523476392030716,
      "learning_rate": 0.00047077883175237144,
      "loss": 0.003,
      "step": 5853
    },
    {
      "epoch": 2.922616075886171,
      "grad_norm": 0.1183939203619957,
      "learning_rate": 0.0004707738392411383,
      "loss": 0.0029,
      "step": 5854
    },
    {
      "epoch": 2.923115327009486,
      "grad_norm": 1.0974968671798706,
      "learning_rate": 0.00047076884672990515,
      "loss": 0.016,
      "step": 5855
    },
    {
      "epoch": 2.923614578132801,
      "grad_norm": 0.13924570381641388,
      "learning_rate": 0.000470763854218672,
      "loss": 0.0032,
      "step": 5856
    },
    {
      "epoch": 2.924113829256116,
      "grad_norm": 0.09367361664772034,
      "learning_rate": 0.00047075886170743885,
      "loss": 0.002,
      "step": 5857
    },
    {
      "epoch": 2.924613080379431,
      "grad_norm": 0.20173467695713043,
      "learning_rate": 0.0004707538691962057,
      "loss": 0.0039,
      "step": 5858
    },
    {
      "epoch": 2.9251123315027456,
      "grad_norm": 1.174065351486206,
      "learning_rate": 0.00047074887668497256,
      "loss": 0.0042,
      "step": 5859
    },
    {
      "epoch": 2.925611582626061,
      "grad_norm": 0.5076481103897095,
      "learning_rate": 0.0004707438841737394,
      "loss": 0.0066,
      "step": 5860
    },
    {
      "epoch": 2.9261108337493758,
      "grad_norm": 0.04989836737513542,
      "learning_rate": 0.00047073889166250626,
      "loss": 0.001,
      "step": 5861
    },
    {
      "epoch": 2.926610084872691,
      "grad_norm": 0.07981033623218536,
      "learning_rate": 0.0004707338991512731,
      "loss": 0.0011,
      "step": 5862
    },
    {
      "epoch": 2.927109335996006,
      "grad_norm": 0.25321847200393677,
      "learning_rate": 0.0004707289066400399,
      "loss": 0.003,
      "step": 5863
    },
    {
      "epoch": 2.927608587119321,
      "grad_norm": 0.6318966150283813,
      "learning_rate": 0.00047072391412880676,
      "loss": 0.0056,
      "step": 5864
    },
    {
      "epoch": 2.928107838242636,
      "grad_norm": 0.13286113739013672,
      "learning_rate": 0.0004707189216175736,
      "loss": 0.0015,
      "step": 5865
    },
    {
      "epoch": 2.928607089365951,
      "grad_norm": 0.7649407982826233,
      "learning_rate": 0.00047071392910634047,
      "loss": 0.016,
      "step": 5866
    },
    {
      "epoch": 2.929106340489266,
      "grad_norm": 0.11763792484998703,
      "learning_rate": 0.0004707089365951073,
      "loss": 0.0015,
      "step": 5867
    },
    {
      "epoch": 2.929605591612581,
      "grad_norm": 0.23501548171043396,
      "learning_rate": 0.00047070394408387417,
      "loss": 0.0031,
      "step": 5868
    },
    {
      "epoch": 2.930104842735896,
      "grad_norm": 0.18310734629631042,
      "learning_rate": 0.000470698951572641,
      "loss": 0.0025,
      "step": 5869
    },
    {
      "epoch": 2.9306040938592113,
      "grad_norm": 0.7380777597427368,
      "learning_rate": 0.0004706939590614079,
      "loss": 0.0097,
      "step": 5870
    },
    {
      "epoch": 2.9311033449825263,
      "grad_norm": 0.4550033211708069,
      "learning_rate": 0.0004706889665501747,
      "loss": 0.0062,
      "step": 5871
    },
    {
      "epoch": 2.9316025961058414,
      "grad_norm": 1.0139994621276855,
      "learning_rate": 0.0004706839740389416,
      "loss": 0.0254,
      "step": 5872
    },
    {
      "epoch": 2.9321018472291565,
      "grad_norm": 0.05568048730492592,
      "learning_rate": 0.00047067898152770843,
      "loss": 0.0009,
      "step": 5873
    },
    {
      "epoch": 2.932601098352471,
      "grad_norm": 0.5037097334861755,
      "learning_rate": 0.0004706739890164753,
      "loss": 0.0048,
      "step": 5874
    },
    {
      "epoch": 2.9331003494757866,
      "grad_norm": 1.381817102432251,
      "learning_rate": 0.00047066899650524213,
      "loss": 0.0157,
      "step": 5875
    },
    {
      "epoch": 2.933599600599101,
      "grad_norm": 0.08824203908443451,
      "learning_rate": 0.000470664003994009,
      "loss": 0.001,
      "step": 5876
    },
    {
      "epoch": 2.9340988517224162,
      "grad_norm": 0.4628528356552124,
      "learning_rate": 0.00047065901148277584,
      "loss": 0.0187,
      "step": 5877
    },
    {
      "epoch": 2.9345981028457313,
      "grad_norm": 0.5953323245048523,
      "learning_rate": 0.0004706540189715427,
      "loss": 0.005,
      "step": 5878
    },
    {
      "epoch": 2.9350973539690464,
      "grad_norm": 0.1652858406305313,
      "learning_rate": 0.00047064902646030954,
      "loss": 0.0026,
      "step": 5879
    },
    {
      "epoch": 2.9355966050923614,
      "grad_norm": 0.8296471238136292,
      "learning_rate": 0.0004706440339490764,
      "loss": 0.0235,
      "step": 5880
    },
    {
      "epoch": 2.9360958562156765,
      "grad_norm": 0.05717067793011665,
      "learning_rate": 0.00047063904143784325,
      "loss": 0.0015,
      "step": 5881
    },
    {
      "epoch": 2.9365951073389915,
      "grad_norm": 0.3554859757423401,
      "learning_rate": 0.0004706340489266101,
      "loss": 0.0061,
      "step": 5882
    },
    {
      "epoch": 2.9370943584623066,
      "grad_norm": 0.16168422996997833,
      "learning_rate": 0.00047062905641537695,
      "loss": 0.0027,
      "step": 5883
    },
    {
      "epoch": 2.9375936095856217,
      "grad_norm": 0.3454364538192749,
      "learning_rate": 0.0004706240639041438,
      "loss": 0.0069,
      "step": 5884
    },
    {
      "epoch": 2.9380928607089367,
      "grad_norm": 0.06397063285112381,
      "learning_rate": 0.00047061907139291065,
      "loss": 0.0016,
      "step": 5885
    },
    {
      "epoch": 2.9385921118322518,
      "grad_norm": 0.04208496958017349,
      "learning_rate": 0.0004706140788816775,
      "loss": 0.0013,
      "step": 5886
    },
    {
      "epoch": 2.9390913629555664,
      "grad_norm": 0.06755483150482178,
      "learning_rate": 0.00047060908637044436,
      "loss": 0.0017,
      "step": 5887
    },
    {
      "epoch": 2.939590614078882,
      "grad_norm": 0.037007689476013184,
      "learning_rate": 0.0004706040938592112,
      "loss": 0.0011,
      "step": 5888
    },
    {
      "epoch": 2.9400898652021965,
      "grad_norm": 0.3561263382434845,
      "learning_rate": 0.00047059910134797806,
      "loss": 0.0147,
      "step": 5889
    },
    {
      "epoch": 2.940589116325512,
      "grad_norm": 0.36622852087020874,
      "learning_rate": 0.0004705941088367449,
      "loss": 0.0061,
      "step": 5890
    },
    {
      "epoch": 2.9410883674488266,
      "grad_norm": 0.2501019537448883,
      "learning_rate": 0.00047058911632551177,
      "loss": 0.0037,
      "step": 5891
    },
    {
      "epoch": 2.9415876185721417,
      "grad_norm": 0.08065716922283173,
      "learning_rate": 0.0004705841238142786,
      "loss": 0.0016,
      "step": 5892
    },
    {
      "epoch": 2.9420868696954567,
      "grad_norm": 1.3356443643569946,
      "learning_rate": 0.0004705791313030454,
      "loss": 0.0315,
      "step": 5893
    },
    {
      "epoch": 2.942586120818772,
      "grad_norm": 0.4475037455558777,
      "learning_rate": 0.00047057413879181227,
      "loss": 0.0056,
      "step": 5894
    },
    {
      "epoch": 2.943085371942087,
      "grad_norm": 0.07957785576581955,
      "learning_rate": 0.0004705691462805791,
      "loss": 0.0011,
      "step": 5895
    },
    {
      "epoch": 2.943584623065402,
      "grad_norm": 0.1954234093427658,
      "learning_rate": 0.000470564153769346,
      "loss": 0.0021,
      "step": 5896
    },
    {
      "epoch": 2.944083874188717,
      "grad_norm": 0.10051769018173218,
      "learning_rate": 0.0004705591612581128,
      "loss": 0.0017,
      "step": 5897
    },
    {
      "epoch": 2.944583125312032,
      "grad_norm": 0.4303964078426361,
      "learning_rate": 0.0004705541687468797,
      "loss": 0.0058,
      "step": 5898
    },
    {
      "epoch": 2.945082376435347,
      "grad_norm": 0.6014154553413391,
      "learning_rate": 0.00047054917623564653,
      "loss": 0.0158,
      "step": 5899
    },
    {
      "epoch": 2.945581627558662,
      "grad_norm": 0.3242552876472473,
      "learning_rate": 0.0004705441837244134,
      "loss": 0.0051,
      "step": 5900
    },
    {
      "epoch": 2.946080878681977,
      "grad_norm": 0.1788700968027115,
      "learning_rate": 0.00047053919121318023,
      "loss": 0.0026,
      "step": 5901
    },
    {
      "epoch": 2.946580129805292,
      "grad_norm": 0.18728533387184143,
      "learning_rate": 0.0004705341987019471,
      "loss": 0.0032,
      "step": 5902
    },
    {
      "epoch": 2.9470793809286073,
      "grad_norm": 0.37117063999176025,
      "learning_rate": 0.00047052920619071394,
      "loss": 0.0053,
      "step": 5903
    },
    {
      "epoch": 2.947578632051922,
      "grad_norm": 0.0164945125579834,
      "learning_rate": 0.0004705242136794808,
      "loss": 0.0009,
      "step": 5904
    },
    {
      "epoch": 2.948077883175237,
      "grad_norm": 0.5028606057167053,
      "learning_rate": 0.00047051922116824764,
      "loss": 0.0047,
      "step": 5905
    },
    {
      "epoch": 2.948577134298552,
      "grad_norm": 0.07712100446224213,
      "learning_rate": 0.0004705142286570145,
      "loss": 0.0014,
      "step": 5906
    },
    {
      "epoch": 2.949076385421867,
      "grad_norm": 1.6295356750488281,
      "learning_rate": 0.00047050923614578135,
      "loss": 0.0131,
      "step": 5907
    },
    {
      "epoch": 2.949575636545182,
      "grad_norm": 0.21537013351917267,
      "learning_rate": 0.0004705042436345482,
      "loss": 0.003,
      "step": 5908
    },
    {
      "epoch": 2.9500748876684972,
      "grad_norm": 0.6416108012199402,
      "learning_rate": 0.00047049925112331505,
      "loss": 0.0071,
      "step": 5909
    },
    {
      "epoch": 2.9505741387918123,
      "grad_norm": 1.1268337965011597,
      "learning_rate": 0.0004704942586120819,
      "loss": 0.0127,
      "step": 5910
    },
    {
      "epoch": 2.9510733899151274,
      "grad_norm": 0.04256291314959526,
      "learning_rate": 0.00047048926610084875,
      "loss": 0.0011,
      "step": 5911
    },
    {
      "epoch": 2.9515726410384424,
      "grad_norm": 0.11608168482780457,
      "learning_rate": 0.0004704842735896156,
      "loss": 0.0016,
      "step": 5912
    },
    {
      "epoch": 2.9520718921617575,
      "grad_norm": 0.912440836429596,
      "learning_rate": 0.00047047928107838246,
      "loss": 0.005,
      "step": 5913
    },
    {
      "epoch": 2.9525711432850725,
      "grad_norm": 0.13993299007415771,
      "learning_rate": 0.0004704742885671493,
      "loss": 0.0019,
      "step": 5914
    },
    {
      "epoch": 2.953070394408387,
      "grad_norm": 0.4876365065574646,
      "learning_rate": 0.00047046929605591616,
      "loss": 0.0081,
      "step": 5915
    },
    {
      "epoch": 2.9535696455317026,
      "grad_norm": 0.06314577162265778,
      "learning_rate": 0.000470464303544683,
      "loss": 0.0009,
      "step": 5916
    },
    {
      "epoch": 2.9540688966550173,
      "grad_norm": 1.1783747673034668,
      "learning_rate": 0.00047045931103344987,
      "loss": 0.0368,
      "step": 5917
    },
    {
      "epoch": 2.9545681477783328,
      "grad_norm": 0.035480648279190063,
      "learning_rate": 0.0004704543185222167,
      "loss": 0.0009,
      "step": 5918
    },
    {
      "epoch": 2.9550673989016474,
      "grad_norm": 0.5560227036476135,
      "learning_rate": 0.00047044932601098357,
      "loss": 0.0103,
      "step": 5919
    },
    {
      "epoch": 2.9555666500249624,
      "grad_norm": 1.7360906600952148,
      "learning_rate": 0.0004704443334997504,
      "loss": 0.0105,
      "step": 5920
    },
    {
      "epoch": 2.9560659011482775,
      "grad_norm": 0.898788571357727,
      "learning_rate": 0.0004704393409885173,
      "loss": 0.0124,
      "step": 5921
    },
    {
      "epoch": 2.9565651522715926,
      "grad_norm": 0.44354525208473206,
      "learning_rate": 0.0004704343484772841,
      "loss": 0.0064,
      "step": 5922
    },
    {
      "epoch": 2.9570644033949076,
      "grad_norm": 0.15465465188026428,
      "learning_rate": 0.0004704293559660509,
      "loss": 0.0025,
      "step": 5923
    },
    {
      "epoch": 2.9575636545182227,
      "grad_norm": 0.14228548109531403,
      "learning_rate": 0.0004704243634548178,
      "loss": 0.0031,
      "step": 5924
    },
    {
      "epoch": 2.9580629056415377,
      "grad_norm": 0.26631972193717957,
      "learning_rate": 0.00047041937094358463,
      "loss": 0.004,
      "step": 5925
    },
    {
      "epoch": 2.958562156764853,
      "grad_norm": 0.395908921957016,
      "learning_rate": 0.0004704143784323515,
      "loss": 0.0084,
      "step": 5926
    },
    {
      "epoch": 2.959061407888168,
      "grad_norm": 0.41890159249305725,
      "learning_rate": 0.00047040938592111833,
      "loss": 0.0094,
      "step": 5927
    },
    {
      "epoch": 2.959560659011483,
      "grad_norm": 0.0466640405356884,
      "learning_rate": 0.0004704043934098852,
      "loss": 0.0014,
      "step": 5928
    },
    {
      "epoch": 2.960059910134798,
      "grad_norm": 0.6247161030769348,
      "learning_rate": 0.00047039940089865204,
      "loss": 0.003,
      "step": 5929
    },
    {
      "epoch": 2.9605591612581126,
      "grad_norm": 0.475495308637619,
      "learning_rate": 0.0004703944083874189,
      "loss": 0.0097,
      "step": 5930
    },
    {
      "epoch": 2.961058412381428,
      "grad_norm": 0.38117262721061707,
      "learning_rate": 0.00047038941587618574,
      "loss": 0.0161,
      "step": 5931
    },
    {
      "epoch": 2.9615576635047427,
      "grad_norm": 0.4244697690010071,
      "learning_rate": 0.0004703844233649526,
      "loss": 0.013,
      "step": 5932
    },
    {
      "epoch": 2.9620569146280578,
      "grad_norm": 0.4140138030052185,
      "learning_rate": 0.00047037943085371945,
      "loss": 0.0107,
      "step": 5933
    },
    {
      "epoch": 2.962556165751373,
      "grad_norm": 0.12654323875904083,
      "learning_rate": 0.0004703744383424863,
      "loss": 0.0017,
      "step": 5934
    },
    {
      "epoch": 2.963055416874688,
      "grad_norm": 0.6924580335617065,
      "learning_rate": 0.00047036944583125315,
      "loss": 0.0156,
      "step": 5935
    },
    {
      "epoch": 2.963554667998003,
      "grad_norm": 0.34383514523506165,
      "learning_rate": 0.00047036445332002,
      "loss": 0.0088,
      "step": 5936
    },
    {
      "epoch": 2.964053919121318,
      "grad_norm": 0.27926307916641235,
      "learning_rate": 0.00047035946080878685,
      "loss": 0.0052,
      "step": 5937
    },
    {
      "epoch": 2.964553170244633,
      "grad_norm": 0.17913483083248138,
      "learning_rate": 0.0004703544682975537,
      "loss": 0.0029,
      "step": 5938
    },
    {
      "epoch": 2.965052421367948,
      "grad_norm": 0.3491206169128418,
      "learning_rate": 0.00047034947578632056,
      "loss": 0.0083,
      "step": 5939
    },
    {
      "epoch": 2.965551672491263,
      "grad_norm": 0.9693323969841003,
      "learning_rate": 0.0004703444832750874,
      "loss": 0.013,
      "step": 5940
    },
    {
      "epoch": 2.9660509236145782,
      "grad_norm": 0.19521237909793854,
      "learning_rate": 0.00047033949076385426,
      "loss": 0.0041,
      "step": 5941
    },
    {
      "epoch": 2.9665501747378933,
      "grad_norm": 0.10805317014455795,
      "learning_rate": 0.0004703344982526211,
      "loss": 0.0024,
      "step": 5942
    },
    {
      "epoch": 2.9670494258612083,
      "grad_norm": 0.36405354738235474,
      "learning_rate": 0.00047032950574138797,
      "loss": 0.013,
      "step": 5943
    },
    {
      "epoch": 2.9675486769845234,
      "grad_norm": 0.07556869834661484,
      "learning_rate": 0.0004703245132301548,
      "loss": 0.0019,
      "step": 5944
    },
    {
      "epoch": 2.968047928107838,
      "grad_norm": 0.3878071904182434,
      "learning_rate": 0.00047031952071892167,
      "loss": 0.0091,
      "step": 5945
    },
    {
      "epoch": 2.9685471792311535,
      "grad_norm": 0.04959556832909584,
      "learning_rate": 0.0004703145282076885,
      "loss": 0.0011,
      "step": 5946
    },
    {
      "epoch": 2.969046430354468,
      "grad_norm": 0.11863678693771362,
      "learning_rate": 0.0004703095356964554,
      "loss": 0.0027,
      "step": 5947
    },
    {
      "epoch": 2.969545681477783,
      "grad_norm": 0.34716784954071045,
      "learning_rate": 0.0004703045431852222,
      "loss": 0.0074,
      "step": 5948
    },
    {
      "epoch": 2.9700449326010983,
      "grad_norm": 0.6350034475326538,
      "learning_rate": 0.0004702995506739891,
      "loss": 0.0077,
      "step": 5949
    },
    {
      "epoch": 2.9705441837244133,
      "grad_norm": 0.5302568078041077,
      "learning_rate": 0.0004702945581627559,
      "loss": 0.0268,
      "step": 5950
    },
    {
      "epoch": 2.9710434348477284,
      "grad_norm": 0.6951390504837036,
      "learning_rate": 0.0004702895656515227,
      "loss": 0.033,
      "step": 5951
    },
    {
      "epoch": 2.9715426859710434,
      "grad_norm": 0.08010320365428925,
      "learning_rate": 0.0004702845731402895,
      "loss": 0.0017,
      "step": 5952
    },
    {
      "epoch": 2.9720419370943585,
      "grad_norm": 0.103334441781044,
      "learning_rate": 0.0004702795806290564,
      "loss": 0.0025,
      "step": 5953
    },
    {
      "epoch": 2.9725411882176735,
      "grad_norm": 0.4353707730770111,
      "learning_rate": 0.00047027458811782323,
      "loss": 0.01,
      "step": 5954
    },
    {
      "epoch": 2.9730404393409886,
      "grad_norm": 0.3613610863685608,
      "learning_rate": 0.0004702695956065901,
      "loss": 0.0055,
      "step": 5955
    },
    {
      "epoch": 2.9735396904643037,
      "grad_norm": 0.04749374836683273,
      "learning_rate": 0.00047026460309535693,
      "loss": 0.0015,
      "step": 5956
    },
    {
      "epoch": 2.9740389415876187,
      "grad_norm": 0.06583798676729202,
      "learning_rate": 0.0004702596105841238,
      "loss": 0.0014,
      "step": 5957
    },
    {
      "epoch": 2.9745381927109333,
      "grad_norm": 0.5841403007507324,
      "learning_rate": 0.00047025461807289064,
      "loss": 0.0104,
      "step": 5958
    },
    {
      "epoch": 2.975037443834249,
      "grad_norm": 0.4216403067111969,
      "learning_rate": 0.0004702496255616575,
      "loss": 0.0045,
      "step": 5959
    },
    {
      "epoch": 2.9755366949575635,
      "grad_norm": 0.14426882565021515,
      "learning_rate": 0.00047024463305042434,
      "loss": 0.0018,
      "step": 5960
    },
    {
      "epoch": 2.976035946080879,
      "grad_norm": 0.1557663232088089,
      "learning_rate": 0.0004702396405391912,
      "loss": 0.0027,
      "step": 5961
    },
    {
      "epoch": 2.9765351972041936,
      "grad_norm": 0.21705488860607147,
      "learning_rate": 0.00047023464802795805,
      "loss": 0.0033,
      "step": 5962
    },
    {
      "epoch": 2.9770344483275086,
      "grad_norm": 0.05357712507247925,
      "learning_rate": 0.0004702296555167249,
      "loss": 0.0013,
      "step": 5963
    },
    {
      "epoch": 2.9775336994508237,
      "grad_norm": 0.4080718159675598,
      "learning_rate": 0.00047022466300549175,
      "loss": 0.019,
      "step": 5964
    },
    {
      "epoch": 2.9780329505741387,
      "grad_norm": 0.4724070429801941,
      "learning_rate": 0.0004702196704942586,
      "loss": 0.0038,
      "step": 5965
    },
    {
      "epoch": 2.978532201697454,
      "grad_norm": 0.08582839369773865,
      "learning_rate": 0.00047021467798302546,
      "loss": 0.0021,
      "step": 5966
    },
    {
      "epoch": 2.979031452820769,
      "grad_norm": 0.019290247932076454,
      "learning_rate": 0.0004702096854717923,
      "loss": 0.001,
      "step": 5967
    },
    {
      "epoch": 2.979530703944084,
      "grad_norm": 0.03455203399062157,
      "learning_rate": 0.00047020469296055916,
      "loss": 0.0012,
      "step": 5968
    },
    {
      "epoch": 2.980029955067399,
      "grad_norm": 0.2867748737335205,
      "learning_rate": 0.000470199700449326,
      "loss": 0.0041,
      "step": 5969
    },
    {
      "epoch": 2.980529206190714,
      "grad_norm": 2.0766713619232178,
      "learning_rate": 0.00047019470793809286,
      "loss": 0.0111,
      "step": 5970
    },
    {
      "epoch": 2.981028457314029,
      "grad_norm": 0.1377897411584854,
      "learning_rate": 0.0004701897154268597,
      "loss": 0.0021,
      "step": 5971
    },
    {
      "epoch": 2.981527708437344,
      "grad_norm": 0.03521627187728882,
      "learning_rate": 0.00047018472291562657,
      "loss": 0.0009,
      "step": 5972
    },
    {
      "epoch": 2.9820269595606588,
      "grad_norm": 0.21291744709014893,
      "learning_rate": 0.0004701797304043934,
      "loss": 0.0048,
      "step": 5973
    },
    {
      "epoch": 2.9825262106839743,
      "grad_norm": 0.07457935810089111,
      "learning_rate": 0.00047017473789316027,
      "loss": 0.0014,
      "step": 5974
    },
    {
      "epoch": 2.983025461807289,
      "grad_norm": 0.1395491510629654,
      "learning_rate": 0.0004701697453819271,
      "loss": 0.0023,
      "step": 5975
    },
    {
      "epoch": 2.983524712930604,
      "grad_norm": 0.21710513532161713,
      "learning_rate": 0.000470164752870694,
      "loss": 0.0027,
      "step": 5976
    },
    {
      "epoch": 2.984023964053919,
      "grad_norm": 0.42603445053100586,
      "learning_rate": 0.00047015976035946083,
      "loss": 0.0158,
      "step": 5977
    },
    {
      "epoch": 2.984523215177234,
      "grad_norm": 0.23848509788513184,
      "learning_rate": 0.0004701547678482277,
      "loss": 0.0031,
      "step": 5978
    },
    {
      "epoch": 2.985022466300549,
      "grad_norm": 1.5703526735305786,
      "learning_rate": 0.00047014977533699453,
      "loss": 0.017,
      "step": 5979
    },
    {
      "epoch": 2.985521717423864,
      "grad_norm": 0.07693304866552353,
      "learning_rate": 0.00047014478282576133,
      "loss": 0.0013,
      "step": 5980
    },
    {
      "epoch": 2.9860209685471792,
      "grad_norm": 1.6344997882843018,
      "learning_rate": 0.0004701397903145282,
      "loss": 0.0142,
      "step": 5981
    },
    {
      "epoch": 2.9865202196704943,
      "grad_norm": 0.512065052986145,
      "learning_rate": 0.00047013479780329503,
      "loss": 0.0429,
      "step": 5982
    },
    {
      "epoch": 2.9870194707938094,
      "grad_norm": 0.5748992562294006,
      "learning_rate": 0.0004701298052920619,
      "loss": 0.0041,
      "step": 5983
    },
    {
      "epoch": 2.9875187219171244,
      "grad_norm": 0.013561232946813107,
      "learning_rate": 0.00047012481278082874,
      "loss": 0.0007,
      "step": 5984
    },
    {
      "epoch": 2.9880179730404395,
      "grad_norm": 0.6317813396453857,
      "learning_rate": 0.0004701198202695956,
      "loss": 0.008,
      "step": 5985
    },
    {
      "epoch": 2.988517224163754,
      "grad_norm": 0.07245895266532898,
      "learning_rate": 0.00047011482775836244,
      "loss": 0.0014,
      "step": 5986
    },
    {
      "epoch": 2.9890164752870696,
      "grad_norm": 0.5203322172164917,
      "learning_rate": 0.0004701098352471293,
      "loss": 0.0096,
      "step": 5987
    },
    {
      "epoch": 2.989515726410384,
      "grad_norm": 0.11409741640090942,
      "learning_rate": 0.00047010484273589615,
      "loss": 0.0024,
      "step": 5988
    },
    {
      "epoch": 2.9900149775336997,
      "grad_norm": 0.15054050087928772,
      "learning_rate": 0.000470099850224663,
      "loss": 0.0026,
      "step": 5989
    },
    {
      "epoch": 2.9905142286570143,
      "grad_norm": 0.2279159128665924,
      "learning_rate": 0.00047009485771342985,
      "loss": 0.0053,
      "step": 5990
    },
    {
      "epoch": 2.9910134797803294,
      "grad_norm": 1.6021260023117065,
      "learning_rate": 0.0004700898652021967,
      "loss": 0.0112,
      "step": 5991
    },
    {
      "epoch": 2.9915127309036444,
      "grad_norm": 0.21085919439792633,
      "learning_rate": 0.00047008487269096356,
      "loss": 0.004,
      "step": 5992
    },
    {
      "epoch": 2.9920119820269595,
      "grad_norm": 0.7549158930778503,
      "learning_rate": 0.0004700798801797304,
      "loss": 0.0286,
      "step": 5993
    },
    {
      "epoch": 2.9925112331502746,
      "grad_norm": 0.036387037485837936,
      "learning_rate": 0.00047007488766849726,
      "loss": 0.0011,
      "step": 5994
    },
    {
      "epoch": 2.9930104842735896,
      "grad_norm": 1.6960515975952148,
      "learning_rate": 0.0004700698951572641,
      "loss": 0.0155,
      "step": 5995
    },
    {
      "epoch": 2.9935097353969047,
      "grad_norm": 0.16990438103675842,
      "learning_rate": 0.00047006490264603096,
      "loss": 0.0021,
      "step": 5996
    },
    {
      "epoch": 2.9940089865202197,
      "grad_norm": 0.6125544905662537,
      "learning_rate": 0.0004700599101347978,
      "loss": 0.0085,
      "step": 5997
    },
    {
      "epoch": 2.994508237643535,
      "grad_norm": 0.035884834825992584,
      "learning_rate": 0.00047005491762356467,
      "loss": 0.0013,
      "step": 5998
    },
    {
      "epoch": 2.99500748876685,
      "grad_norm": 2.0476183891296387,
      "learning_rate": 0.0004700499251123315,
      "loss": 0.0305,
      "step": 5999
    },
    {
      "epoch": 2.995506739890165,
      "grad_norm": 0.5243074297904968,
      "learning_rate": 0.00047004493260109837,
      "loss": 0.0089,
      "step": 6000
    },
    {
      "epoch": 2.9960059910134795,
      "grad_norm": 0.49770066142082214,
      "learning_rate": 0.0004700399400898652,
      "loss": 0.0043,
      "step": 6001
    },
    {
      "epoch": 2.996505242136795,
      "grad_norm": 0.01767979934811592,
      "learning_rate": 0.0004700349475786321,
      "loss": 0.0008,
      "step": 6002
    },
    {
      "epoch": 2.9970044932601096,
      "grad_norm": 0.887638509273529,
      "learning_rate": 0.00047002995506739893,
      "loss": 0.0224,
      "step": 6003
    },
    {
      "epoch": 2.9975037443834247,
      "grad_norm": 0.049294453114271164,
      "learning_rate": 0.0004700249625561658,
      "loss": 0.0013,
      "step": 6004
    },
    {
      "epoch": 2.9980029955067398,
      "grad_norm": 0.48729655146598816,
      "learning_rate": 0.00047001997004493263,
      "loss": 0.0107,
      "step": 6005
    },
    {
      "epoch": 2.998502246630055,
      "grad_norm": 0.7369515895843506,
      "learning_rate": 0.0004700149775336995,
      "loss": 0.0335,
      "step": 6006
    },
    {
      "epoch": 2.99900149775337,
      "grad_norm": 0.3937693238258362,
      "learning_rate": 0.00047000998502246634,
      "loss": 0.0099,
      "step": 6007
    },
    {
      "epoch": 2.999500748876685,
      "grad_norm": 0.19913357496261597,
      "learning_rate": 0.0004700049925112332,
      "loss": 0.0033,
      "step": 6008
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.744547963142395,
      "learning_rate": 0.00047,
      "loss": 0.0087,
      "step": 6009
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.1428188532590866,
      "eval_runtime": 2070.5321,
      "eval_samples_per_second": 2.831,
      "eval_steps_per_second": 0.236,
      "step": 6009
    }
  ],
  "logging_steps": 1,
  "max_steps": 100150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.117414100344832e+19,
  "train_batch_size": 12,
  "trial_name": null,
  "trial_params": null
}
